# Spark

æœ¬æ–‡ç›®å½•ï¼š

ä¸€ã€Spark åŸºç¡€
äºŒã€Spark Core
ä¸‰ã€Spark SQL
å››ã€Spark Streaming
äº”ã€Structured Streaming
å…­ã€Spark ä¸¤ç§æ ¸å¿ƒ Shuffle
ä¸ƒã€Spark åº•å±‚æ‰§è¡ŒåŽŸç†
å…«ã€Spark æ•°æ®å€¾æ–œ
ä¹ã€Spark æ€§èƒ½è°ƒä¼˜
åã€Spark æ•…éšœæŽ’é™¤
åä¸€ã€Sparkå¤§åŽ‚é¢è¯•çœŸé¢˜

**Spark****æ¶‰åŠçš„çŸ¥è¯†ç‚¹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæœ¬æ–‡å°†é€ä¸€è®²è§£ï¼š**



![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BCl9wk9pRNl69bTKpm34PfNJibicsHQTm8qm7Jib3Ow1ibZibobiclPMYGQicibYg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



**æœ¬æ–‡å‚è€ƒäº†å…³äºŽ Spark çš„ä¼—å¤šèµ„æ–™æ•´ç†è€Œæˆï¼Œä¸ºäº†æ•´æ´çš„æŽ’ç‰ˆåŠèˆ’é€‚çš„é˜…è¯»ï¼Œå¯¹äºŽæ¨¡ç³Šä¸æ¸…æ™°çš„å›¾ç‰‡åŠé»‘ç™½å›¾ç‰‡è¿›è¡Œé‡æ–°ç»˜åˆ¶æˆäº†é«˜æ¸…å½©å›¾**ã€‚

## ä¸€ã€Spark åŸºç¡€

### 1. æ¿€åŠ¨äººå¿ƒçš„ Spark å‘å±•å²

å¤§æ•°æ®ã€äººå·¥æ™ºèƒ½( Artificial Intelligence )åƒå½“å¹´çš„çŸ³æ²¹ã€ç”µåŠ›ä¸€æ ·ï¼Œ æ­£ä»¥å‰æ‰€æœªæœ‰çš„å¹¿åº¦å’Œæ·±åº¦å½±å“æ‰€æœ‰çš„è¡Œä¸šï¼Œ çŽ°åœ¨åŠæœªæ¥å…¬å¸çš„æ ¸å¿ƒå£åž’æ˜¯æ•°æ®ï¼Œ æ ¸å¿ƒç«žäº‰åŠ›æ¥è‡ªåŸºäºŽå¤§æ•°æ®çš„äººå·¥æ™ºèƒ½çš„ç«žäº‰ã€‚

Spark æ˜¯å½“ä»Šå¤§æ•°æ®é¢†åŸŸæœ€æ´»è·ƒã€æœ€çƒ­é—¨ã€æœ€é«˜æ•ˆçš„å¤§æ•°æ®é€šç”¨è®¡ç®—å¹³å°ä¹‹ä¸€ã€‚

**2009 å¹´è¯žç”ŸäºŽç¾Žå›½åŠ å·žå¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡ AMP å®žéªŒå®¤**ï¼›
2010 å¹´é€šè¿‡ BSD è®¸å¯åè®®å¼€æºå‘å¸ƒï¼›
2013 å¹´æèµ ç»™ Apache è½¯ä»¶åŸºé‡‘ä¼šå¹¶åˆ‡æ¢å¼€æºåè®®åˆ°åˆ‡æ¢è®¸å¯åè®®è‡³ Apache2.0ï¼›
**2014 å¹´ 2 æœˆï¼ŒSpark æˆä¸º Apache çš„é¡¶çº§é¡¹ç›®**ï¼›
2014 å¹´ 11 æœˆ, Spark çš„æ¯å…¬å¸ Databricks å›¢é˜Ÿä½¿ç”¨ Spark åˆ·æ–°æ•°æ®æŽ’åºä¸–ç•Œè®°å½•ã€‚

Spark æˆåŠŸæž„å»ºèµ·äº†ä¸€ä½“åŒ–ã€å¤šå…ƒåŒ–çš„å¤§æ•°æ®å¤„ç†ä½“ç³»ã€‚åœ¨ä»»ä½•è§„æ¨¡çš„æ•°æ®è®¡ç®—ä¸­ï¼Œ Spark åœ¨æ€§èƒ½å’Œæ‰©å±•æ€§ä¸Šéƒ½æ›´å…·ä¼˜åŠ¿ã€‚

1. Hadoop ä¹‹çˆ¶ Doug Cutting æŒ‡å‡ºï¼šUse of MapReduce engine for Big Data projects will decline, replaced by Apache Spark (å¤§æ•°æ®é¡¹ç›®çš„ MapReduce å¼•æ“Žçš„ä½¿ç”¨å°†ä¸‹é™ï¼Œç”± Apache Spark å–ä»£)ã€‚
2. Hadoop å•†ä¸šå‘è¡Œç‰ˆæœ¬çš„å¸‚åœºé¢†å¯¼è€… Cloudera ã€HortonWorks ã€MapR çº·çº·è½¬æŠ• Spark,å¹¶æŠŠ Spark ä½œä¸ºå¤§æ•°æ®è§£å†³æ–¹æ¡ˆçš„é¦–é€‰å’Œæ ¸å¿ƒè®¡ç®—å¼•æ“Žã€‚

2014 å¹´çš„ Benchmark æµ‹è¯•ä¸­ï¼Œ Spark ç§’æ€ Hadoop ï¼Œåœ¨ä½¿ç”¨ååˆ†ä¹‹ä¸€è®¡ç®—èµ„æºçš„æƒ…å†µä¸‹ï¼Œç›¸åŒæ•°æ®çš„æŽ’åºä¸Šï¼Œ Spark æ¯” MapReduce å¿« 3 å€ï¼åœ¨æ²¡æœ‰å®˜æ–¹ PB æŽ’åºå¯¹æ¯”çš„æƒ…å†µä¸‹ï¼Œé¦–æ¬¡å°† Spark æŽ¨åˆ°äº† IPB æ•°æ®(åä¸‡äº¿æ¡è®°å½•) çš„æŽ’åºï¼Œåœ¨ä½¿ç”¨ 190 ä¸ªèŠ‚ç‚¹çš„æƒ…å†µä¸‹ï¼Œå·¥ä½œè´Ÿè½½åœ¨ 4 å°æ—¶å†…å®Œæˆï¼Œ åŒæ ·è¿œè¶…é›…è™Žä¹‹å‰ä½¿ç”¨ 3800 å°ä¸»æœºè€—æ—¶ 16 ä¸ªå°æ—¶çš„è®°å½•ã€‚

åœ¨ FullStack ç†æƒ³çš„æŒ‡å¼•ä¸‹ï¼Œ**Spark ä¸­çš„ Spark SQL ã€SparkStreaming ã€MLLib ã€GraphX ã€R äº”å¤§å­æ¡†æž¶å’Œåº“ä¹‹é—´å¯ä»¥æ— ç¼åœ°å…±äº«æ•°æ®å’Œæ“ä½œ**ï¼Œ è¿™ä¸ä»…æ‰“é€ äº† Spark åœ¨å½“ä»Šå¤§æ•°æ®è®¡ç®—é¢†åŸŸå…¶ä»–è®¡ç®—æ¡†æž¶éƒ½æ— å¯åŒ¹æ•Œçš„ä¼˜åŠ¿ï¼Œ è€Œä¸”ä½¿å¾— Spark æ­£åœ¨åŠ é€Ÿæˆä¸ºå¤§æ•°æ®å¤„ç†ä¸­å¿ƒé¦–é€‰é€šç”¨è®¡ç®—å¹³å°ã€‚

### 2. Spark ä¸ºä»€ä¹ˆä¼šæµè¡Œ

- åŽŸå›  1ï¼š**ä¼˜ç§€çš„æ•°æ®æ¨¡åž‹å’Œä¸°å¯Œè®¡ç®—æŠ½è±¡**

Spark äº§ç”Ÿä¹‹å‰ï¼Œå·²ç»æœ‰ MapReduce è¿™ç±»éžå¸¸æˆç†Ÿçš„è®¡ç®—ç³»ç»Ÿå­˜åœ¨äº†ï¼Œå¹¶æä¾›äº†é«˜å±‚æ¬¡çš„ API(map/reduce)ï¼ŒæŠŠè®¡ç®—è¿è¡Œåœ¨é›†ç¾¤ä¸­å¹¶æä¾›å®¹é”™èƒ½åŠ›ï¼Œä»Žè€Œå®žçŽ°åˆ†å¸ƒå¼è®¡ç®—ã€‚

è™½ç„¶ MapReduce æä¾›äº†å¯¹æ•°æ®è®¿é—®å’Œè®¡ç®—çš„æŠ½è±¡ï¼Œä½†æ˜¯å¯¹äºŽæ•°æ®çš„å¤ç”¨å°±æ˜¯ç®€å•çš„å°†ä¸­é—´æ•°æ®å†™åˆ°ä¸€ä¸ªç¨³å®šçš„æ–‡ä»¶ç³»ç»Ÿä¸­(ä¾‹å¦‚ HDFS)ï¼Œæ‰€ä»¥ä¼šäº§ç”Ÿæ•°æ®çš„å¤åˆ¶å¤‡ä»½ï¼Œç£ç›˜çš„ I/O ä»¥åŠæ•°æ®çš„åºåˆ—åŒ–ï¼Œæ‰€ä»¥åœ¨é‡åˆ°éœ€è¦åœ¨å¤šä¸ªè®¡ç®—ä¹‹é—´å¤ç”¨ä¸­é—´ç»“æžœçš„æ“ä½œæ—¶æ•ˆçŽ‡å°±ä¼šéžå¸¸çš„ä½Žã€‚è€Œè¿™ç±»æ“ä½œæ˜¯éžå¸¸å¸¸è§çš„ï¼Œä¾‹å¦‚è¿­ä»£å¼è®¡ç®—ï¼Œäº¤äº’å¼æ•°æ®æŒ–æŽ˜ï¼Œå›¾è®¡ç®—ç­‰ã€‚

è®¤è¯†åˆ°è¿™ä¸ªé—®é¢˜åŽï¼Œå­¦æœ¯ç•Œçš„ AMPLab æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¨¡åž‹ï¼Œå«åš RDDã€‚RDD æ˜¯ä¸€ä¸ªå¯ä»¥å®¹é”™ä¸”å¹¶è¡Œçš„æ•°æ®ç»“æž„(å…¶å®žå¯ä»¥ç†è§£æˆåˆ†å¸ƒå¼çš„é›†åˆï¼Œæ“ä½œèµ·æ¥å’Œæ“ä½œæœ¬åœ°é›†åˆä¸€æ ·ç®€å•)ï¼Œå®ƒå¯ä»¥è®©ç”¨æˆ·æ˜¾å¼çš„å°†ä¸­é—´ç»“æžœæ•°æ®é›†ä¿å­˜åœ¨å†…å­˜ä¸­ï¼Œå¹¶ä¸”é€šè¿‡æŽ§åˆ¶æ•°æ®é›†çš„åˆ†åŒºæ¥è¾¾åˆ°æ•°æ®å­˜æ”¾å¤„ç†æœ€ä¼˜åŒ–.åŒæ—¶ RDD ä¹Ÿæä¾›äº†ä¸°å¯Œçš„ API (mapã€reduceã€filterã€foreachã€redeceByKey...)æ¥æ“ä½œæ•°æ®é›†ã€‚åŽæ¥ RDD è¢« AMPLab åœ¨ä¸€ä¸ªå«åš Spark çš„æ¡†æž¶ä¸­æä¾›å¹¶å¼€æºã€‚

ç®€è€Œè¨€ä¹‹ï¼ŒSpark å€Ÿé‰´äº† MapReduce æ€æƒ³å‘å±•è€Œæ¥ï¼Œä¿ç•™äº†å…¶åˆ†å¸ƒå¼å¹¶è¡Œè®¡ç®—çš„ä¼˜ç‚¹å¹¶æ”¹è¿›äº†å…¶æ˜Žæ˜¾çš„ç¼ºé™·ã€‚è®©ä¸­é—´æ•°æ®å­˜å‚¨åœ¨å†…å­˜ä¸­æé«˜äº†è¿è¡Œé€Ÿåº¦ã€å¹¶æä¾›ä¸°å¯Œçš„æ“ä½œæ•°æ®çš„ API æé«˜äº†å¼€å‘é€Ÿåº¦ã€‚

- åŽŸå›  2ï¼š**å®Œå–„çš„ç”Ÿæ€åœˆ-fullstack**![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClZ35ruRTUPdyXuM47thBiaXDBuj5p8apCDibOVWRJRcB2mO2ua9iaE3yZw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)ç›®å‰ï¼ŒSpark å·²ç»å‘å±•æˆä¸ºä¸€ä¸ªåŒ…å«å¤šä¸ªå­é¡¹ç›®çš„é›†åˆï¼Œå…¶ä¸­åŒ…å« SparkSQLã€Spark Streamingã€GraphXã€MLlib ç­‰å­é¡¹ç›®ã€‚

**Spark Core**ï¼šå®žçŽ°äº† Spark çš„åŸºæœ¬åŠŸèƒ½ï¼ŒåŒ…å« RDDã€ä»»åŠ¡è°ƒåº¦ã€å†…å­˜ç®¡ç†ã€é”™è¯¯æ¢å¤ã€ä¸Žå­˜å‚¨ç³»ç»Ÿäº¤äº’ç­‰æ¨¡å—ã€‚

**Spark SQL**ï¼šSpark ç”¨æ¥æ“ä½œç»“æž„åŒ–æ•°æ®çš„ç¨‹åºåŒ…ã€‚é€šè¿‡ Spark SQLï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ SQL æ“ä½œæ•°æ®ã€‚

**Spark Streaming**ï¼šSpark æä¾›çš„å¯¹å®žæ—¶æ•°æ®è¿›è¡Œæµå¼è®¡ç®—çš„ç»„ä»¶ã€‚æä¾›äº†ç”¨æ¥æ“ä½œæ•°æ®æµçš„ APIã€‚

**Spark MLlib**ï¼šæä¾›å¸¸è§çš„æœºå™¨å­¦ä¹ (ML)åŠŸèƒ½çš„ç¨‹åºåº“ã€‚åŒ…æ‹¬åˆ†ç±»ã€å›žå½’ã€èšç±»ã€ååŒè¿‡æ»¤ç­‰ï¼Œè¿˜æä¾›äº†æ¨¡åž‹è¯„ä¼°ã€æ•°æ®å¯¼å…¥ç­‰é¢å¤–çš„æ”¯æŒåŠŸèƒ½ã€‚

**GraphX(å›¾è®¡ç®—)**ï¼šSpark ä¸­ç”¨äºŽå›¾è®¡ç®—çš„ APIï¼Œæ€§èƒ½è‰¯å¥½ï¼Œæ‹¥æœ‰ä¸°å¯Œçš„åŠŸèƒ½å’Œè¿ç®—ç¬¦ï¼Œèƒ½åœ¨æµ·é‡æ•°æ®ä¸Šè‡ªå¦‚åœ°è¿è¡Œå¤æ‚çš„å›¾ç®—æ³•ã€‚

**é›†ç¾¤ç®¡ç†å™¨**ï¼šSpark è®¾è®¡ä¸ºå¯ä»¥é«˜æ•ˆåœ°åœ¨ä¸€ä¸ªè®¡ç®—èŠ‚ç‚¹åˆ°æ•°åƒä¸ªè®¡ç®—èŠ‚ç‚¹ä¹‹é—´ä¼¸ç¼©è®¡ç®—ã€‚

**Structured Streaming**ï¼šå¤„ç†ç»“æž„åŒ–æµ,ç»Ÿä¸€äº†ç¦»çº¿å’Œå®žæ—¶çš„ APIã€‚

### 3. Spark VS Hadoop

|              |                     Hadoop                     |                      Spark                      |
| :----------: | :--------------------------------------------: | :---------------------------------------------: |
|     ç±»åž‹     |      åˆ†å¸ƒå¼åŸºç¡€å¹³å°, åŒ…å«è®¡ç®—, å­˜å‚¨, è°ƒåº¦      |                 åˆ†å¸ƒå¼è®¡ç®—å·¥å…·                  |
|     åœºæ™¯     |             å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„æ‰¹å¤„ç†             |          è¿­ä»£è®¡ç®—, äº¤äº’å¼è®¡ç®—, æµè®¡ç®—           |
|     ä»·æ ¼     |               å¯¹æœºå™¨è¦æ±‚ä½Ž, ä¾¿å®œ               |             å¯¹å†…å­˜æœ‰è¦æ±‚, ç›¸å¯¹è¾ƒè´µ              |
|   ç¼–ç¨‹èŒƒå¼   |     Map+Reduce, API è¾ƒä¸ºåº•å±‚, ç®—æ³•é€‚åº”æ€§å·®     | RDD ç»„æˆ DAG æœ‰å‘æ— çŽ¯å›¾, API è¾ƒä¸ºé¡¶å±‚, æ–¹ä¾¿ä½¿ç”¨ |
| æ•°æ®å­˜å‚¨ç»“æž„ | MapReduce ä¸­é—´è®¡ç®—ç»“æžœå­˜åœ¨ HDFS ç£ç›˜ä¸Š, å»¶è¿Ÿå¤§ |       RDD ä¸­é—´è¿ç®—ç»“æžœå­˜åœ¨å†…å­˜ä¸­ , å»¶è¿Ÿå°       |
|   è¿è¡Œæ–¹å¼   |        Task ä»¥è¿›ç¨‹æ–¹å¼ç»´æŠ¤, ä»»åŠ¡å¯åŠ¨æ…¢         |         Task ä»¥çº¿ç¨‹æ–¹å¼ç»´æŠ¤, ä»»åŠ¡å¯åŠ¨å¿«         |

> ðŸ’– æ³¨æ„ï¼š
> å°½ç®¡ Spark ç›¸å¯¹äºŽ Hadoop è€Œè¨€å…·æœ‰è¾ƒå¤§ä¼˜åŠ¿ï¼Œä½† Spark å¹¶ä¸èƒ½å®Œå…¨æ›¿ä»£ Hadoopï¼ŒSpark ä¸»è¦ç”¨äºŽæ›¿ä»£ Hadoop ä¸­çš„ MapReduce è®¡ç®—æ¨¡åž‹ã€‚å­˜å‚¨ä¾ç„¶å¯ä»¥ä½¿ç”¨ HDFSï¼Œä½†æ˜¯ä¸­é—´ç»“æžœå¯ä»¥å­˜æ”¾åœ¨å†…å­˜ä¸­ï¼›è°ƒåº¦å¯ä»¥ä½¿ç”¨ Spark å†…ç½®çš„ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨æ›´æˆç†Ÿçš„è°ƒåº¦ç³»ç»Ÿ YARN ç­‰ã€‚
> å®žé™…ä¸Šï¼ŒSpark å·²ç»å¾ˆå¥½åœ°èžå…¥äº† Hadoop ç”Ÿæ€åœˆï¼Œå¹¶æˆä¸ºå…¶ä¸­çš„é‡è¦ä¸€å‘˜ï¼Œå®ƒå¯ä»¥å€ŸåŠ©äºŽ YARN å®žçŽ°èµ„æºè°ƒåº¦ç®¡ç†ï¼Œå€ŸåŠ©äºŽ HDFS å®žçŽ°åˆ†å¸ƒå¼å­˜å‚¨ã€‚
> æ­¤å¤–ï¼ŒHadoop å¯ä»¥ä½¿ç”¨å»‰ä»·çš„ã€å¼‚æž„çš„æœºå™¨æ¥åšåˆ†å¸ƒå¼å­˜å‚¨ä¸Žè®¡ç®—ï¼Œä½†æ˜¯ï¼ŒSpark å¯¹ç¡¬ä»¶çš„è¦æ±‚ç¨é«˜ä¸€äº›ï¼Œå¯¹å†…å­˜ä¸Ž CPU æœ‰ä¸€å®šçš„è¦æ±‚ã€‚

### 3. Spark ç‰¹ç‚¹

- å¿«

ä¸Ž Hadoop çš„ MapReduce ç›¸æ¯”ï¼Œ**Spark åŸºäºŽå†…å­˜çš„è¿ç®—è¦å¿« 100 å€ä»¥ä¸Šï¼ŒåŸºäºŽç¡¬ç›˜çš„è¿ç®—ä¹Ÿè¦å¿« 10 å€ä»¥ä¸Š**ã€‚Spark å®žçŽ°äº†é«˜æ•ˆçš„ DAG æ‰§è¡Œå¼•æ“Žï¼Œå¯ä»¥é€šè¿‡åŸºäºŽå†…å­˜æ¥é«˜æ•ˆå¤„ç†æ•°æ®æµã€‚

- æ˜“ç”¨

**Spark æ”¯æŒ Javaã€Pythonã€R å’Œ Scala çš„ APIï¼Œè¿˜æ”¯æŒè¶…è¿‡ 80 ç§é«˜çº§ç®—æ³•ï¼Œä½¿ç”¨æˆ·å¯ä»¥å¿«é€Ÿæž„å»ºä¸åŒçš„åº”ç”¨**ã€‚è€Œä¸” Spark æ”¯æŒäº¤äº’å¼çš„ Python å’Œ Scala çš„ shellï¼Œå¯ä»¥éžå¸¸æ–¹ä¾¿åœ°åœ¨è¿™äº› shell ä¸­ä½¿ç”¨ Spark é›†ç¾¤æ¥éªŒè¯è§£å†³é—®é¢˜çš„æ–¹æ³•ã€‚

- é€šç”¨

Spark æä¾›äº†ç»Ÿä¸€çš„è§£å†³æ–¹æ¡ˆã€‚**Spark å¯ä»¥ç”¨äºŽæ‰¹å¤„ç†ã€äº¤äº’å¼æŸ¥è¯¢(Spark SQL)ã€å®žæ—¶æµå¤„ç†(Spark Streaming)ã€æœºå™¨å­¦ä¹ (Spark MLlib)å’Œå›¾è®¡ç®—(GraphX)**ã€‚è¿™äº›ä¸åŒç±»åž‹çš„å¤„ç†éƒ½å¯ä»¥åœ¨åŒä¸€ä¸ªåº”ç”¨ä¸­æ— ç¼ä½¿ç”¨ã€‚Spark ç»Ÿä¸€çš„è§£å†³æ–¹æ¡ˆéžå¸¸å…·æœ‰å¸å¼•åŠ›ï¼Œæ¯•ç«Ÿä»»ä½•å…¬å¸éƒ½æƒ³ç”¨ç»Ÿä¸€çš„å¹³å°åŽ»å¤„ç†é‡åˆ°çš„é—®é¢˜ï¼Œå‡å°‘å¼€å‘å’Œç»´æŠ¤çš„äººåŠ›æˆæœ¬å’Œéƒ¨ç½²å¹³å°çš„ç‰©åŠ›æˆæœ¬ã€‚

- å…¼å®¹æ€§

**Spark å¯ä»¥éžå¸¸æ–¹ä¾¿åœ°ä¸Žå…¶ä»–çš„å¼€æºäº§å“è¿›è¡Œèžåˆ**ã€‚æ¯”å¦‚ï¼ŒSpark å¯ä»¥ä½¿ç”¨ Hadoop çš„ YARN å’Œ Apache Mesos ä½œä¸ºå®ƒçš„èµ„æºç®¡ç†å’Œè°ƒåº¦å™¨ï¼Œå¹¶ä¸”å¯ä»¥å¤„ç†æ‰€æœ‰ Hadoop æ”¯æŒçš„æ•°æ®ï¼ŒåŒ…æ‹¬ HDFSã€HBase å’Œ Cassandra ç­‰ã€‚è¿™å¯¹äºŽå·²ç»éƒ¨ç½² Hadoop é›†ç¾¤çš„ç”¨æˆ·ç‰¹åˆ«é‡è¦ï¼Œå› ä¸ºä¸éœ€è¦åšä»»ä½•æ•°æ®è¿ç§»å°±å¯ä»¥ä½¿ç”¨ Spark çš„å¼ºå¤§å¤„ç†èƒ½åŠ›ã€‚Spark ä¹Ÿå¯ä»¥ä¸ä¾èµ–äºŽç¬¬ä¸‰æ–¹çš„èµ„æºç®¡ç†å’Œè°ƒåº¦å™¨ï¼Œå®ƒå®žçŽ°äº† Standalone ä½œä¸ºå…¶å†…ç½®çš„èµ„æºç®¡ç†å’Œè°ƒåº¦æ¡†æž¶ï¼Œè¿™æ ·è¿›ä¸€æ­¥é™ä½Žäº† Spark çš„ä½¿ç”¨é—¨æ§›ï¼Œä½¿å¾—æ‰€æœ‰äººéƒ½å¯ä»¥éžå¸¸å®¹æ˜“åœ°éƒ¨ç½²å’Œä½¿ç”¨ Sparkã€‚æ­¤å¤–ï¼ŒSpark è¿˜æä¾›äº†åœ¨ EC2 ä¸Šéƒ¨ç½² Standalone çš„ Spark é›†ç¾¤çš„å·¥å…·ã€‚

### 4. Spark è¿è¡Œæ¨¡å¼

1. local æœ¬åœ°æ¨¡å¼(å•æœº)--å­¦ä¹ æµ‹è¯•ä½¿ç”¨

   åˆ†ä¸º local å•çº¿ç¨‹å’Œ local-cluster å¤šçº¿ç¨‹ã€‚

2. standalone ç‹¬ç«‹é›†ç¾¤æ¨¡å¼--å­¦ä¹ æµ‹è¯•ä½¿ç”¨

   å…¸åž‹çš„ Mater/slave æ¨¡å¼ã€‚

3. standalone-HA é«˜å¯ç”¨æ¨¡å¼--ç”Ÿäº§çŽ¯å¢ƒä½¿ç”¨

   åŸºäºŽ standalone æ¨¡å¼ï¼Œä½¿ç”¨ zk æ­å»ºé«˜å¯ç”¨ï¼Œé¿å… Master æ˜¯æœ‰å•ç‚¹æ•…éšœçš„ã€‚

4. **on yarn é›†ç¾¤æ¨¡å¼--ç”Ÿäº§çŽ¯å¢ƒä½¿ç”¨**

   è¿è¡Œåœ¨ yarn é›†ç¾¤ä¹‹ä¸Šï¼Œç”± yarn è´Ÿè´£èµ„æºç®¡ç†ï¼ŒSpark è´Ÿè´£ä»»åŠ¡è°ƒåº¦å’Œè®¡ç®—ã€‚

   å¥½å¤„ï¼šè®¡ç®—èµ„æºæŒ‰éœ€ä¼¸ç¼©ï¼Œé›†ç¾¤åˆ©ç”¨çŽ‡é«˜ï¼Œå…±äº«åº•å±‚å­˜å‚¨ï¼Œé¿å…æ•°æ®è·¨é›†ç¾¤è¿ç§»ã€‚

5. on mesos é›†ç¾¤æ¨¡å¼--å›½å†…ä½¿ç”¨è¾ƒå°‘

   è¿è¡Œåœ¨ mesos èµ„æºç®¡ç†å™¨æ¡†æž¶ä¹‹ä¸Šï¼Œç”± mesos è´Ÿè´£èµ„æºç®¡ç†ï¼ŒSpark è´Ÿè´£ä»»åŠ¡è°ƒåº¦å’Œè®¡ç®—ã€‚

6. on cloud é›†ç¾¤æ¨¡å¼--ä¸­å°å…¬å¸æœªæ¥ä¼šæ›´å¤šçš„ä½¿ç”¨äº‘æœåŠ¡

   æ¯”å¦‚ AWS çš„ EC2ï¼Œä½¿ç”¨è¿™ä¸ªæ¨¡å¼èƒ½å¾ˆæ–¹ä¾¿çš„è®¿é—® Amazon çš„ S3ã€‚

## äºŒã€Spark Core

### 1. RDD è¯¦è§£

#### 1) ä¸ºä»€ä¹ˆè¦æœ‰ RDD?

åœ¨è®¸å¤šè¿­ä»£å¼ç®—æ³•(æ¯”å¦‚æœºå™¨å­¦ä¹ ã€å›¾ç®—æ³•ç­‰)å’Œäº¤äº’å¼æ•°æ®æŒ–æŽ˜ä¸­ï¼Œä¸åŒè®¡ç®—é˜¶æ®µä¹‹é—´ä¼šé‡ç”¨ä¸­é—´ç»“æžœï¼Œå³ä¸€ä¸ªé˜¶æ®µçš„è¾“å‡ºç»“æžœä¼šä½œä¸ºä¸‹ä¸€ä¸ªé˜¶æ®µçš„è¾“å…¥ã€‚ä½†æ˜¯ï¼Œä¹‹å‰çš„ MapReduce æ¡†æž¶é‡‡ç”¨éžå¾ªçŽ¯å¼çš„æ•°æ®æµæ¨¡åž‹ï¼ŒæŠŠä¸­é—´ç»“æžœå†™å…¥åˆ° HDFS ä¸­ï¼Œå¸¦æ¥äº†å¤§é‡çš„æ•°æ®å¤åˆ¶ã€ç£ç›˜ IO å’Œåºåˆ—åŒ–å¼€é”€ã€‚ä¸”è¿™äº›æ¡†æž¶åªèƒ½æ”¯æŒä¸€äº›ç‰¹å®šçš„è®¡ç®—æ¨¡å¼(map/reduce)ï¼Œå¹¶æ²¡æœ‰æä¾›ä¸€ç§é€šç”¨çš„æ•°æ®æŠ½è±¡ã€‚

AMP å®žéªŒå®¤å‘è¡¨çš„ä¸€ç¯‡å…³äºŽ RDD çš„è®ºæ–‡:ã€ŠResilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computingã€‹å°±æ˜¯ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜çš„ã€‚

RDD æä¾›äº†ä¸€ä¸ªæŠ½è±¡çš„æ•°æ®æ¨¡åž‹ï¼Œè®©æˆ‘ä»¬ä¸å¿…æ‹…å¿ƒåº•å±‚æ•°æ®çš„åˆ†å¸ƒå¼ç‰¹æ€§ï¼Œåªéœ€å°†å…·ä½“çš„åº”ç”¨é€»è¾‘è¡¨è¾¾ä¸ºä¸€ç³»åˆ—è½¬æ¢æ“ä½œ(å‡½æ•°)ï¼Œä¸åŒ RDD ä¹‹é—´çš„è½¬æ¢æ“ä½œä¹‹é—´è¿˜å¯ä»¥å½¢æˆä¾èµ–å…³ç³»ï¼Œè¿›è€Œå®žçŽ°ç®¡é“åŒ–ï¼Œä»Žè€Œé¿å…äº†ä¸­é—´ç»“æžœçš„å­˜å‚¨ï¼Œå¤§å¤§é™ä½Žäº†æ•°æ®å¤åˆ¶ã€ç£ç›˜ IO å’Œåºåˆ—åŒ–å¼€é”€ï¼Œå¹¶ä¸”è¿˜æä¾›äº†æ›´å¤šçš„ API(map/reduec/filter/groupBy...)ã€‚

#### 2) RDD æ˜¯ä»€ä¹ˆ?

RDD(Resilient Distributed Dataset)å«åšå¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†ï¼Œæ˜¯ Spark ä¸­æœ€åŸºæœ¬çš„æ•°æ®æŠ½è±¡ï¼Œä»£è¡¨ä¸€ä¸ªä¸å¯å˜ã€å¯åˆ†åŒºã€é‡Œé¢çš„å…ƒç´ å¯å¹¶è¡Œè®¡ç®—çš„é›†åˆã€‚å•è¯æ‹†è§£ï¼š

- Resilient ï¼šå®ƒæ˜¯å¼¹æ€§çš„ï¼ŒRDD é‡Œé¢çš„ä¸­çš„æ•°æ®å¯ä»¥ä¿å­˜åœ¨å†…å­˜ä¸­æˆ–è€…ç£ç›˜é‡Œé¢ï¼›
- Distributed ï¼šå®ƒé‡Œé¢çš„å…ƒç´ æ˜¯åˆ†å¸ƒå¼å­˜å‚¨çš„ï¼Œå¯ä»¥ç”¨äºŽåˆ†å¸ƒå¼è®¡ç®—ï¼›
- Dataset: å®ƒæ˜¯ä¸€ä¸ªé›†åˆï¼Œå¯ä»¥å­˜æ”¾å¾ˆå¤šå…ƒç´ ã€‚

#### 3) RDD ä¸»è¦å±žæ€§

è¿›å…¥ RDD çš„æºç ä¸­çœ‹ä¸‹ï¼š

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BCl6NMlgeGVowML26CggWDeVVpnzhGVybr4Lm2wM45nYbQAVzwep36Mew/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)RDDæºç 

åœ¨æºç ä¸­å¯ä»¥çœ‹åˆ°æœ‰å¯¹ RDD ä»‹ç»çš„æ³¨é‡Šï¼Œæˆ‘ä»¬æ¥ç¿»è¯‘ä¸‹ï¼š

1. A list of partitions ï¼šä¸€ç»„åˆ†ç‰‡(Partition)/ä¸€ä¸ªåˆ†åŒº(Partition)åˆ—è¡¨ï¼Œå³æ•°æ®é›†çš„åŸºæœ¬ç»„æˆå•ä½ã€‚å¯¹äºŽ RDD æ¥è¯´ï¼Œæ¯ä¸ªåˆ†ç‰‡éƒ½ä¼šè¢«ä¸€ä¸ªè®¡ç®—ä»»åŠ¡å¤„ç†ï¼Œåˆ†ç‰‡æ•°å†³å®šå¹¶è¡Œåº¦ã€‚ç”¨æˆ·å¯ä»¥åœ¨åˆ›å»º RDD æ—¶æŒ‡å®š RDD çš„åˆ†ç‰‡ä¸ªæ•°ï¼Œå¦‚æžœæ²¡æœ‰æŒ‡å®šï¼Œé‚£ä¹ˆå°±ä¼šé‡‡ç”¨é»˜è®¤å€¼ã€‚
2. A function for computing each split ï¼šä¸€ä¸ªå‡½æ•°ä¼šè¢«ä½œç”¨åœ¨æ¯ä¸€ä¸ªåˆ†åŒºã€‚Spark ä¸­ RDD çš„è®¡ç®—æ˜¯ä»¥åˆ†ç‰‡ä¸ºå•ä½çš„ï¼Œcompute å‡½æ•°ä¼šè¢«ä½œç”¨åˆ°æ¯ä¸ªåˆ†åŒºä¸Šã€‚
3. A list of dependencies on other RDDs ï¼šä¸€ä¸ª RDD ä¼šä¾èµ–äºŽå…¶ä»–å¤šä¸ª RDDã€‚RDD çš„æ¯æ¬¡è½¬æ¢éƒ½ä¼šç”Ÿæˆä¸€ä¸ªæ–°çš„ RDDï¼Œæ‰€ä»¥ RDD ä¹‹é—´å°±ä¼šå½¢æˆç±»ä¼¼äºŽæµæ°´çº¿ä¸€æ ·çš„å‰åŽä¾èµ–å…³ç³»ã€‚åœ¨éƒ¨åˆ†åˆ†åŒºæ•°æ®ä¸¢å¤±æ—¶ï¼ŒSpark å¯ä»¥é€šè¿‡è¿™ä¸ªä¾èµ–å…³ç³»é‡æ–°è®¡ç®—ä¸¢å¤±çš„åˆ†åŒºæ•°æ®ï¼Œè€Œä¸æ˜¯å¯¹ RDD çš„æ‰€æœ‰åˆ†åŒºè¿›è¡Œé‡æ–°è®¡ç®—ã€‚(Spark çš„å®¹é”™æœºåˆ¶)
4. Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)ï¼šå¯é€‰é¡¹ï¼Œå¯¹äºŽ KV ç±»åž‹çš„ RDD ä¼šæœ‰ä¸€ä¸ª Partitionerï¼Œå³ RDD çš„åˆ†åŒºå‡½æ•°ï¼Œé»˜è®¤ä¸º HashPartitionerã€‚
5. Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)ï¼šå¯é€‰é¡¹,ä¸€ä¸ªåˆ—è¡¨ï¼Œå­˜å‚¨å­˜å–æ¯ä¸ª Partition çš„ä¼˜å…ˆä½ç½®(preferred location)ã€‚å¯¹äºŽä¸€ä¸ª HDFS æ–‡ä»¶æ¥è¯´ï¼Œè¿™ä¸ªåˆ—è¡¨ä¿å­˜çš„å°±æ˜¯æ¯ä¸ª Partition æ‰€åœ¨çš„å—çš„ä½ç½®ã€‚æŒ‰ç…§"ç§»åŠ¨æ•°æ®ä¸å¦‚ç§»åŠ¨è®¡ç®—"çš„ç†å¿µï¼ŒSpark åœ¨è¿›è¡Œä»»åŠ¡è°ƒåº¦çš„æ—¶å€™ï¼Œä¼šå°½å¯èƒ½é€‰æ‹©é‚£äº›å­˜æœ‰æ•°æ®çš„ worker èŠ‚ç‚¹æ¥è¿›è¡Œä»»åŠ¡è®¡ç®—ã€‚

**æ€»ç»“**

RDD æ˜¯ä¸€ä¸ªæ•°æ®é›†çš„è¡¨ç¤ºï¼Œä¸ä»…è¡¨ç¤ºäº†æ•°æ®é›†ï¼Œè¿˜è¡¨ç¤ºäº†è¿™ä¸ªæ•°æ®é›†ä»Žå“ªæ¥ï¼Œå¦‚ä½•è®¡ç®—ï¼Œä¸»è¦å±žæ€§åŒ…æ‹¬ï¼š

1. åˆ†åŒºåˆ—è¡¨
2. è®¡ç®—å‡½æ•°
3. ä¾èµ–å…³ç³»
4. åˆ†åŒºå‡½æ•°(é»˜è®¤æ˜¯ hash)
5. æœ€ä½³ä½ç½®

åˆ†åŒºåˆ—è¡¨ã€åˆ†åŒºå‡½æ•°ã€æœ€ä½³ä½ç½®ï¼Œè¿™ä¸‰ä¸ªå±žæ€§å…¶å®žè¯´çš„å°±æ˜¯æ•°æ®é›†åœ¨å“ªï¼Œåœ¨å“ªè®¡ç®—æ›´åˆé€‚ï¼Œå¦‚ä½•åˆ†åŒºï¼›
è®¡ç®—å‡½æ•°ã€ä¾èµ–å…³ç³»ï¼Œè¿™ä¸¤ä¸ªå±žæ€§å…¶å®žè¯´çš„æ˜¯æ•°æ®é›†æ€Žä¹ˆæ¥çš„ã€‚

### 2. RDD-API

#### 1) RDD çš„åˆ›å»ºæ–¹å¼

1. ç”±å¤–éƒ¨å­˜å‚¨ç³»ç»Ÿçš„æ•°æ®é›†åˆ›å»ºï¼ŒåŒ…æ‹¬æœ¬åœ°çš„æ–‡ä»¶ç³»ç»Ÿï¼Œè¿˜æœ‰æ‰€æœ‰ Hadoop æ”¯æŒçš„æ•°æ®é›†ï¼Œæ¯”å¦‚ HDFSã€Cassandraã€HBase ç­‰ï¼š
   `val rdd1 = sc.textFile("hdfs://node1:8020/wordcount/input/words.txt")`
2. é€šè¿‡å·²æœ‰çš„ RDD ç»è¿‡ç®—å­è½¬æ¢ç”Ÿæˆæ–°çš„ RDDï¼š
   `val rdd2=rdd1.flatMap(_.split(" "))`
3. ç”±ä¸€ä¸ªå·²ç»å­˜åœ¨çš„ Scala é›†åˆåˆ›å»ºï¼š
   `val rdd3 = sc.parallelize(Array(1,2,3,4,5,6,7,8))`æˆ–è€…
   `val rdd4 = sc.makeRDD(List(1,2,3,4,5,6,7,8))`

makeRDD æ–¹æ³•åº•å±‚è°ƒç”¨äº† parallelize æ–¹æ³•ï¼š

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClojiakN5JicPAcWRE7NPMTibS6x6EjE8jK43lpXNSaeEoHEibJJjU2NibMSQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)RDDæºç 

#### 2) RDD çš„ç®—å­åˆ†ç±»

RDD çš„ç®—å­åˆ†ä¸ºä¸¤ç±»:

1. Transformationè½¬æ¢æ“ä½œ:**è¿”å›žä¸€ä¸ªæ–°çš„ RDD**
2. ActionåŠ¨ä½œæ“ä½œ:**è¿”å›žå€¼ä¸æ˜¯ RDD(æ— è¿”å›žå€¼æˆ–è¿”å›žå…¶ä»–çš„)**

> â£ï¸ æ³¨æ„:
> 1ã€RDD ä¸å®žé™…å­˜å‚¨çœŸæ­£è¦è®¡ç®—çš„æ•°æ®ï¼Œè€Œæ˜¯è®°å½•äº†æ•°æ®çš„ä½ç½®åœ¨å“ªé‡Œï¼Œæ•°æ®çš„è½¬æ¢å…³ç³»(è°ƒç”¨äº†ä»€ä¹ˆæ–¹æ³•ï¼Œä¼ å…¥ä»€ä¹ˆå‡½æ•°)ã€‚
> 2ã€RDD ä¸­çš„æ‰€æœ‰è½¬æ¢éƒ½æ˜¯æƒ°æ€§æ±‚å€¼/å»¶è¿Ÿæ‰§è¡Œçš„ï¼Œä¹Ÿå°±æ˜¯è¯´å¹¶ä¸ä¼šç›´æŽ¥è®¡ç®—ã€‚åªæœ‰å½“å‘ç”Ÿä¸€ä¸ªè¦æ±‚è¿”å›žç»“æžœç»™ Driver çš„ Action åŠ¨ä½œæ—¶ï¼Œè¿™äº›è½¬æ¢æ‰ä¼šçœŸæ­£è¿è¡Œã€‚
> 3ã€ä¹‹æ‰€ä»¥ä½¿ç”¨æƒ°æ€§æ±‚å€¼/å»¶è¿Ÿæ‰§è¡Œï¼Œæ˜¯å› ä¸ºè¿™æ ·å¯ä»¥åœ¨ Action æ—¶å¯¹ RDD æ“ä½œå½¢æˆ DAG æœ‰å‘æ— çŽ¯å›¾è¿›è¡Œ Stage çš„åˆ’åˆ†å’Œå¹¶è¡Œä¼˜åŒ–ï¼Œè¿™ç§è®¾è®¡è®© Spark æ›´åŠ æœ‰æ•ˆçŽ‡åœ°è¿è¡Œã€‚

#### 3) Transformation è½¬æ¢ç®—å­

|                       è½¬æ¢ç®—å­                       |                             å«ä¹‰                             |
| :--------------------------------------------------: | :----------------------------------------------------------: |
|                    **map**(func)                     | è¿”å›žä¸€ä¸ªæ–°çš„ RDDï¼Œè¯¥ RDD ç”±æ¯ä¸€ä¸ªè¾“å…¥å…ƒç´ ç»è¿‡ func å‡½æ•°è½¬æ¢åŽç»„æˆ |
|                   **filter**(func)                   | è¿”å›žä¸€ä¸ªæ–°çš„ RDDï¼Œè¯¥ RDD ç”±ç»è¿‡ func å‡½æ•°è®¡ç®—åŽè¿”å›žå€¼ä¸º true çš„è¾“å…¥å…ƒç´ ç»„æˆ |
|                  **flatMap**(func)                   | ç±»ä¼¼äºŽ mapï¼Œä½†æ˜¯æ¯ä¸€ä¸ªè¾“å…¥å…ƒç´ å¯ä»¥è¢«æ˜ å°„ä¸º 0 æˆ–å¤šä¸ªè¾“å‡ºå…ƒç´ (æ‰€ä»¥ func åº”è¯¥è¿”å›žä¸€ä¸ªåºåˆ—ï¼Œè€Œä¸æ˜¯å•ä¸€å…ƒç´ ) |
|               **mapPartitions**(func)                | ç±»ä¼¼äºŽ mapï¼Œä½†ç‹¬ç«‹åœ°åœ¨ RDD çš„æ¯ä¸€ä¸ªåˆ†ç‰‡ä¸Šè¿è¡Œï¼Œå› æ­¤åœ¨ç±»åž‹ä¸º T çš„ RDD ä¸Šè¿è¡Œæ—¶ï¼Œfunc çš„å‡½æ•°ç±»åž‹å¿…é¡»æ˜¯ Iterator[T] => Iterator[U] |
|           **mapPartitionsWithIndex**(func)           | ç±»ä¼¼äºŽ mapPartitionsï¼Œä½† func å¸¦æœ‰ä¸€ä¸ªæ•´æ•°å‚æ•°è¡¨ç¤ºåˆ†ç‰‡çš„ç´¢å¼•å€¼ï¼Œå› æ­¤åœ¨ç±»åž‹ä¸º T çš„ RDD ä¸Šè¿è¡Œæ—¶ï¼Œfunc çš„å‡½æ•°ç±»åž‹å¿…é¡»æ˜¯(Int, Interator[T]) => Iterator[U] |
|       sample(withReplacement, fraction, seed)        | æ ¹æ® fraction æŒ‡å®šçš„æ¯”ä¾‹å¯¹æ•°æ®è¿›è¡Œé‡‡æ ·ï¼Œå¯ä»¥é€‰æ‹©æ˜¯å¦ä½¿ç”¨éšæœºæ•°è¿›è¡Œæ›¿æ¢ï¼Œseed ç”¨äºŽæŒ‡å®šéšæœºæ•°ç”Ÿæˆå™¨ç§å­ |
|               **union**(otherDataset)                |         å¯¹æº RDD å’Œå‚æ•° RDD æ±‚å¹¶é›†åŽè¿”å›žä¸€ä¸ªæ–°çš„ RDD         |
|              intersection(otherDataset)              |         å¯¹æº RDD å’Œå‚æ•° RDD æ±‚äº¤é›†åŽè¿”å›žä¸€ä¸ªæ–°çš„ RDD         |
|              **distinct**([numTasks]))               |             å¯¹æº RDD è¿›è¡ŒåŽ»é‡åŽè¿”å›žä¸€ä¸ªæ–°çš„ RDD              |
|              **groupByKey**([numTasks])              |   åœ¨ä¸€ä¸ª(K,V)çš„ RDD ä¸Šè°ƒç”¨ï¼Œè¿”å›žä¸€ä¸ª(K, Iterator[V])çš„ RDD   |
|          **reduceByKey**(func, [numTasks])           | åœ¨ä¸€ä¸ª(K,V)çš„ RDD ä¸Šè°ƒç”¨ï¼Œè¿”å›žä¸€ä¸ª(K,V)çš„ RDDï¼Œä½¿ç”¨æŒ‡å®šçš„ reduce å‡½æ•°ï¼Œå°†ç›¸åŒ key çš„å€¼èšåˆåˆ°ä¸€èµ·ï¼Œä¸Ž groupByKey ç±»ä¼¼ï¼Œreduce ä»»åŠ¡çš„ä¸ªæ•°å¯ä»¥é€šè¿‡ç¬¬äºŒä¸ªå¯é€‰çš„å‚æ•°æ¥è®¾ç½® |
| aggregateByKey(zeroValue)(seqOp, combOp, [numTasks]) | å¯¹ PairRDD ä¸­ç›¸åŒçš„ Key å€¼è¿›è¡Œèšåˆæ“ä½œï¼Œåœ¨èšåˆè¿‡ç¨‹ä¸­åŒæ ·ä½¿ç”¨äº†ä¸€ä¸ªä¸­ç«‹çš„åˆå§‹å€¼ã€‚å’Œ aggregate å‡½æ•°ç±»ä¼¼ï¼ŒaggregateByKey è¿”å›žå€¼çš„ç±»åž‹ä¸éœ€è¦å’Œ RDD ä¸­ value çš„ç±»åž‹ä¸€è‡´ |
|        **sortByKey**([ascending], [numTasks])        | åœ¨ä¸€ä¸ª(K,V)çš„ RDD ä¸Šè°ƒç”¨ï¼ŒK å¿…é¡»å®žçŽ° Ordered æŽ¥å£ï¼Œè¿”å›žä¸€ä¸ªæŒ‰ç…§ key è¿›è¡ŒæŽ’åºçš„(K,V)çš„ RDD |
|         sortBy(func,[ascending], [numTasks])         |                ä¸Ž sortByKey ç±»ä¼¼ï¼Œä½†æ˜¯æ›´çµæ´»                 |
|          **join**(otherDataset, [numTasks])          | åœ¨ç±»åž‹ä¸º(K,V)å’Œ(K,W)çš„ RDD ä¸Šè°ƒç”¨ï¼Œè¿”å›žä¸€ä¸ªç›¸åŒ key å¯¹åº”çš„æ‰€æœ‰å…ƒç´ å¯¹åœ¨ä¸€èµ·çš„(K,(V,W))çš„ RDD |
|          cogroup(otherDataset, [numTasks])           | åœ¨ç±»åž‹ä¸º(K,V)å’Œ(K,W)çš„ RDD ä¸Šè°ƒç”¨ï¼Œè¿”å›žä¸€ä¸ª(K,(Iterable,Iterable))ç±»åž‹çš„ RDD |
|               cartesian(otherDataset)                |                           ç¬›å¡å°”ç§¯                           |
|               pipe(command, [envVars])               |                     å¯¹ rdd è¿›è¡Œç®¡é“æ“ä½œ                      |
|             **coalesce**(numPartitions)              | å‡å°‘ RDD çš„åˆ†åŒºæ•°åˆ°æŒ‡å®šå€¼ã€‚åœ¨è¿‡æ»¤å¤§é‡æ•°æ®ä¹‹åŽï¼Œå¯ä»¥æ‰§è¡Œæ­¤æ“ä½œ |
|            **repartition**(numPartitions)            |                       é‡æ–°ç»™ RDD åˆ†åŒº                        |

#### 4) Action åŠ¨ä½œç®—å­

|                åŠ¨ä½œç®—å­                 |                             å«ä¹‰                             |
| :-------------------------------------: | :----------------------------------------------------------: |
|              reduce(func)               | é€šè¿‡ func å‡½æ•°èšé›† RDD ä¸­çš„æ‰€æœ‰å…ƒç´ ï¼Œè¿™ä¸ªåŠŸèƒ½å¿…é¡»æ˜¯å¯äº¤æ¢ä¸”å¯å¹¶è”çš„ |
|                collect()                |        åœ¨é©±åŠ¨ç¨‹åºä¸­ï¼Œä»¥æ•°ç»„çš„å½¢å¼è¿”å›žæ•°æ®é›†çš„æ‰€æœ‰å…ƒç´         |
|                 count()                 |                     è¿”å›ž RDD çš„å…ƒç´ ä¸ªæ•°                      |
|                 first()                 |            è¿”å›ž RDD çš„ç¬¬ä¸€ä¸ªå…ƒç´ (ç±»ä¼¼äºŽ take(1))             |
|                 take(n)                 |           è¿”å›žä¸€ä¸ªç”±æ•°æ®é›†çš„å‰ n ä¸ªå…ƒç´ ç»„æˆçš„æ•°ç»„            |
| takeSample(withReplacement,num, [seed]) | è¿”å›žä¸€ä¸ªæ•°ç»„ï¼Œè¯¥æ•°ç»„ç”±ä»Žæ•°æ®é›†ä¸­éšæœºé‡‡æ ·çš„ num ä¸ªå…ƒç´ ç»„æˆï¼Œå¯ä»¥é€‰æ‹©æ˜¯å¦ç”¨éšæœºæ•°æ›¿æ¢ä¸è¶³çš„éƒ¨åˆ†ï¼Œseed ç”¨äºŽæŒ‡å®šéšæœºæ•°ç”Ÿæˆå™¨ç§å­ |
|       takeOrdered(n, [ordering])        |           è¿”å›žè‡ªç„¶é¡ºåºæˆ–è€…è‡ªå®šä¹‰é¡ºåºçš„å‰ n ä¸ªå…ƒç´             |
|        **saveAsTextFile**(path)         | å°†æ•°æ®é›†çš„å…ƒç´ ä»¥ textfile çš„å½¢å¼ä¿å­˜åˆ° HDFS æ–‡ä»¶ç³»ç»Ÿæˆ–è€…å…¶ä»–æ”¯æŒçš„æ–‡ä»¶ç³»ç»Ÿï¼Œå¯¹äºŽæ¯ä¸ªå…ƒç´ ï¼ŒSpark å°†ä¼šè°ƒç”¨ toString æ–¹æ³•ï¼Œå°†å®ƒè£…æ¢ä¸ºæ–‡ä»¶ä¸­çš„æ–‡æœ¬ |
|      **saveAsSequenceFile**(path)       | å°†æ•°æ®é›†ä¸­çš„å…ƒç´ ä»¥ Hadoop sequencefile çš„æ ¼å¼ä¿å­˜åˆ°æŒ‡å®šçš„ç›®å½•ä¸‹ï¼Œå¯ä»¥ä½¿ HDFS æˆ–è€…å…¶ä»– Hadoop æ”¯æŒçš„æ–‡ä»¶ç³»ç»Ÿ |
|         saveAsObjectFile(path)          |    å°†æ•°æ®é›†çš„å…ƒç´ ï¼Œä»¥ Java åºåˆ—åŒ–çš„æ–¹å¼ä¿å­˜åˆ°æŒ‡å®šçš„ç›®å½•ä¸‹    |
|            **countByKey**()             | é’ˆå¯¹(K,V)ç±»åž‹çš„ RDDï¼Œè¿”å›žä¸€ä¸ª(K,Int)çš„ mapï¼Œè¡¨ç¤ºæ¯ä¸€ä¸ª key å¯¹åº”çš„å…ƒç´ ä¸ªæ•° |
|              foreach(func)              |        åœ¨æ•°æ®é›†çš„æ¯ä¸€ä¸ªå…ƒç´ ä¸Šï¼Œè¿è¡Œå‡½æ•° func è¿›è¡Œæ›´æ–°        |
|       **foreachPartition**(func)        |            åœ¨æ•°æ®é›†çš„æ¯ä¸€ä¸ªåˆ†åŒºä¸Šï¼Œè¿è¡Œå‡½æ•° func             |

**ç»Ÿè®¡æ“ä½œï¼š**

|      ç®—å­      |           å«ä¹‰            |
| :------------: | :-----------------------: |
|     count      |           ä¸ªæ•°            |
|      mean      |           å‡å€¼            |
|      sum       |           æ±‚å’Œ            |
|      max       |          æœ€å¤§å€¼           |
|      min       |          æœ€å°å€¼           |
|    variance    |           æ–¹å·®            |
| sampleVariance |     ä»Žé‡‡æ ·ä¸­è®¡ç®—æ–¹å·®      |
|     stdev      | æ ‡å‡†å·®:è¡¡é‡æ•°æ®çš„ç¦»æ•£ç¨‹åº¦ |
|  sampleStdev   |       é‡‡æ ·çš„æ ‡å‡†å·®        |
|     stats      |       æŸ¥çœ‹ç»Ÿè®¡ç»“æžœ        |

#### 4) RDD ç®—å­ç»ƒä¹ 

- **éœ€æ±‚**ï¼š

ç»™å®šä¸€ä¸ªé”®å€¼å¯¹ RDDï¼š

```
val rdd = sc.parallelize(Array(("spark",2),("hadoop",6),("hadoop",4),("spark",6)))
```

key è¡¨ç¤ºå›¾ä¹¦åç§°ï¼Œvalue è¡¨ç¤ºæŸå¤©å›¾ä¹¦é”€é‡

è¯·è®¡ç®—æ¯ä¸ªé”®å¯¹åº”çš„å¹³å‡å€¼ï¼Œä¹Ÿå°±æ˜¯è®¡ç®—æ¯ç§å›¾ä¹¦çš„æ¯å¤©å¹³å‡é”€é‡ã€‚

æœ€ç»ˆç»“æžœ:("spark",4),("hadoop",5)ã€‚

- **ç­”æ¡ˆ 1**ï¼š

```
val rdd = sc.parallelize(Array(("spark",2),("hadoop",6),("hadoop",4),("spark",6)))
val rdd2 = rdd.groupByKey()
rdd2.collect
//Array[(String, Iterable[Int])] = Array((spark,CompactBuffer(2, 6)), (hadoop,CompactBuffer(6, 4)))
rdd2.mapValues(v=>v.sum/v.size).collect
Array[(String, Int)] = Array((spark,4), (hadoop,5))
```

- **ç­”æ¡ˆ 2**ï¼š

```
val rdd = sc.parallelize(Array(("spark",2),("hadoop",6),("hadoop",4),("spark",6)))
val rdd2 = rdd.groupByKey()
rdd2.collect
//Array[(String, Iterable[Int])] = Array((spark,CompactBuffer(2, 6)), (hadoop,CompactBuffer(6, 4)))

val rdd3 = rdd2.map(t=>(t._1,t._2.sum /t._2.size))
rdd3.collect
//Array[(String, Int)] = Array((spark,4), (hadoop,5))
```

### 3. RDD çš„æŒä¹…åŒ–/ç¼“å­˜

åœ¨å®žé™…å¼€å‘ä¸­æŸäº› RDD çš„è®¡ç®—æˆ–è½¬æ¢å¯èƒ½ä¼šæ¯”è¾ƒè€—è´¹æ—¶é—´ï¼Œå¦‚æžœè¿™äº› RDD åŽç»­è¿˜ä¼šé¢‘ç¹çš„è¢«ä½¿ç”¨åˆ°ï¼Œé‚£ä¹ˆå¯ä»¥å°†è¿™äº› RDD è¿›è¡ŒæŒä¹…åŒ–/ç¼“å­˜ï¼Œè¿™æ ·ä¸‹æ¬¡å†ä½¿ç”¨åˆ°çš„æ—¶å€™å°±ä¸ç”¨å†é‡æ–°è®¡ç®—äº†ï¼Œæé«˜äº†ç¨‹åºè¿è¡Œçš„æ•ˆçŽ‡ã€‚

```
val rdd1 = sc.textFile("hdfs://node01:8020/words.txt")
val rdd2 = rdd1.flatMap(x=>x.split(" ")).map((_,1)).reduceByKey(_+_)
rdd2.cache //ç¼“å­˜/æŒä¹…åŒ–
rdd2.sortBy(_._2,false).collect//è§¦å‘action,ä¼šåŽ»è¯»å–HDFSçš„æ–‡ä»¶,rdd2ä¼šçœŸæ­£æ‰§è¡ŒæŒä¹…åŒ–
rdd2.sortBy(_._2,false).collect//è§¦å‘action,ä¼šåŽ»è¯»ç¼“å­˜ä¸­çš„æ•°æ®,æ‰§è¡Œé€Ÿåº¦ä¼šæ¯”ä¹‹å‰å¿«,å› ä¸ºrdd2å·²ç»æŒä¹…åŒ–åˆ°å†…å­˜ä¸­äº†
```

#### æŒä¹…åŒ–/ç¼“å­˜ API è¯¦è§£

- ersist æ–¹æ³•å’Œ cache æ–¹æ³•

RDD é€šè¿‡ persist æˆ– cache æ–¹æ³•å¯ä»¥å°†å‰é¢çš„è®¡ç®—ç»“æžœç¼“å­˜ï¼Œä½†æ˜¯å¹¶ä¸æ˜¯è¿™ä¸¤ä¸ªæ–¹æ³•è¢«è°ƒç”¨æ—¶ç«‹å³ç¼“å­˜ï¼Œè€Œæ˜¯è§¦å‘åŽé¢çš„ action æ—¶ï¼Œè¯¥ RDD å°†ä¼šè¢«ç¼“å­˜åœ¨è®¡ç®—èŠ‚ç‚¹çš„å†…å­˜ä¸­ï¼Œå¹¶ä¾›åŽé¢é‡ç”¨ã€‚
é€šè¿‡æŸ¥çœ‹ RDD çš„æºç å‘çŽ° cache æœ€ç»ˆä¹Ÿæ˜¯è°ƒç”¨äº† persist æ— å‚æ–¹æ³•(é»˜è®¤å­˜å‚¨åªå­˜åœ¨å†…å­˜ä¸­)ï¼š

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClY05wsJFvT5qL6N4fs3cVWkCp7RBwPjKOuBPSQEnI2QVjzGw6DGzChw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)RDDæºç 

- å­˜å‚¨çº§åˆ«

é»˜è®¤çš„å­˜å‚¨çº§åˆ«éƒ½æ˜¯ä»…åœ¨å†…å­˜å­˜å‚¨ä¸€ä»½ï¼ŒSpark çš„å­˜å‚¨çº§åˆ«è¿˜æœ‰å¥½å¤šç§ï¼Œå­˜å‚¨çº§åˆ«åœ¨ object StorageLevel ä¸­å®šä¹‰çš„ã€‚

|              æŒä¹…åŒ–çº§åˆ«               |                             è¯´æ˜Ž                             |
| :-----------------------------------: | :----------------------------------------------------------: |
|          **MORY_ONLY(é»˜è®¤)**          | å°† RDD ä»¥éžåºåˆ—åŒ–çš„ Java å¯¹è±¡å­˜å‚¨åœ¨ JVM ä¸­ã€‚å¦‚æžœæ²¡æœ‰è¶³å¤Ÿçš„å†…å­˜å­˜å‚¨ RDDï¼Œåˆ™æŸäº›åˆ†åŒºå°†ä¸ä¼šè¢«ç¼“å­˜ï¼Œæ¯æ¬¡éœ€è¦æ—¶éƒ½ä¼šé‡æ–°è®¡ç®—ã€‚è¿™æ˜¯é»˜è®¤çº§åˆ« |
| **MORY_AND_DISK(å¼€å‘ä¸­å¯ä»¥ä½¿ç”¨è¿™ä¸ª)** | å°† RDD ä»¥éžåºåˆ—åŒ–çš„ Java å¯¹è±¡å­˜å‚¨åœ¨ JVM ä¸­ã€‚å¦‚æžœæ•°æ®åœ¨å†…å­˜ä¸­æ”¾ä¸ä¸‹ï¼Œåˆ™æº¢å†™åˆ°ç£ç›˜ä¸Šï¼Žéœ€è¦æ—¶åˆ™ä¼šä»Žç£ç›˜ä¸Šè¯»å– |
|   MEMORY_ONLY_SER (Java and Scala)    | å°† RDD ä»¥åºåˆ—åŒ–çš„ Java å¯¹è±¡(æ¯ä¸ªåˆ†åŒºä¸€ä¸ªå­—èŠ‚æ•°ç»„)çš„æ–¹å¼å­˜å‚¨ï¼Žè¿™é€šå¸¸æ¯”éžåºåˆ—åŒ–å¯¹è±¡(deserialized objects)æ›´å…·ç©ºé—´æ•ˆçŽ‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½¿ç”¨å¿«é€Ÿåºåˆ—åŒ–çš„æƒ…å†µä¸‹ï¼Œä½†æ˜¯è¿™ç§æ–¹å¼è¯»å–æ•°æ®ä¼šæ¶ˆè€—æ›´å¤šçš„ CPU |
| MEMORY_AND_DISK_SER (Java and Scala)  | ä¸Ž MEMORY_ONLY_SER ç±»ä¼¼ï¼Œä½†å¦‚æžœæ•°æ®åœ¨å†…å­˜ä¸­æ”¾ä¸ä¸‹ï¼Œåˆ™æº¢å†™åˆ°ç£ç›˜ä¸Šï¼Œè€Œä¸æ˜¯æ¯æ¬¡éœ€è¦é‡æ–°è®¡ç®—å®ƒä»¬ |
|               DISK_ONLY               |                   å°† RDD åˆ†åŒºå­˜å‚¨åœ¨ç£ç›˜ä¸Š                    |
|  MEMORY_ONLY_2, MEMORY_AND_DISK_2 ç­‰  | ä¸Žä¸Šé¢çš„å‚¨å­˜çº§åˆ«ç›¸åŒï¼Œåªä¸è¿‡å°†æŒä¹…åŒ–æ•°æ®å­˜ä¸ºä¸¤ä»½ï¼Œå¤‡ä»½æ¯ä¸ªåˆ†åŒºå­˜å‚¨åœ¨ä¸¤ä¸ªé›†ç¾¤èŠ‚ç‚¹ä¸Š |
|           OFF_HEAP(å®žéªŒä¸­)            | ä¸Ž MEMORY_ONLY_SER ç±»ä¼¼ï¼Œä½†å°†æ•°æ®å­˜å‚¨åœ¨å †å¤–å†…å­˜ä¸­ã€‚(å³ä¸æ˜¯ç›´æŽ¥å­˜å‚¨åœ¨ JVM å†…å­˜ä¸­) |

**æ€»ç»“ï¼š**

1. RDD æŒä¹…åŒ–/ç¼“å­˜çš„ç›®çš„æ˜¯ä¸ºäº†æé«˜åŽç»­æ“ä½œçš„é€Ÿåº¦
2. ç¼“å­˜çš„çº§åˆ«æœ‰å¾ˆå¤šï¼Œé»˜è®¤åªå­˜åœ¨å†…å­˜ä¸­,å¼€å‘ä¸­ä½¿ç”¨ memory_and_disk
3. åªæœ‰æ‰§è¡Œ action æ“ä½œçš„æ—¶å€™æ‰ä¼šçœŸæ­£å°† RDD æ•°æ®è¿›è¡ŒæŒä¹…åŒ–/ç¼“å­˜
4. å®žé™…å¼€å‘ä¸­å¦‚æžœæŸä¸€ä¸ª RDD åŽç»­ä¼šè¢«é¢‘ç¹çš„ä½¿ç”¨ï¼Œå¯ä»¥å°†è¯¥ RDD è¿›è¡ŒæŒä¹…åŒ–/ç¼“å­˜

### 4. RDD å®¹é”™æœºåˆ¶ Checkpoint

- **æŒä¹…åŒ–çš„å±€é™ï¼š**

æŒä¹…åŒ–/ç¼“å­˜å¯ä»¥æŠŠæ•°æ®æ”¾åœ¨å†…å­˜ä¸­ï¼Œè™½ç„¶æ˜¯å¿«é€Ÿçš„ï¼Œä½†æ˜¯ä¹Ÿæ˜¯æœ€ä¸å¯é çš„ï¼›ä¹Ÿå¯ä»¥æŠŠæ•°æ®æ”¾åœ¨ç£ç›˜ä¸Šï¼Œä¹Ÿä¸æ˜¯å®Œå…¨å¯é çš„ï¼ä¾‹å¦‚ç£ç›˜ä¼šæŸåç­‰ã€‚

- **é—®é¢˜è§£å†³ï¼š**

Checkpoint çš„äº§ç”Ÿå°±æ˜¯ä¸ºäº†æ›´åŠ å¯é çš„æ•°æ®æŒä¹…åŒ–ï¼Œåœ¨ Checkpoint çš„æ—¶å€™ä¸€èˆ¬æŠŠæ•°æ®æ”¾åœ¨åœ¨ HDFS ä¸Šï¼Œè¿™å°±å¤©ç„¶çš„å€ŸåŠ©äº† HDFS å¤©ç”Ÿçš„é«˜å®¹é”™ã€é«˜å¯é æ¥å®žçŽ°æ•°æ®æœ€å¤§ç¨‹åº¦ä¸Šçš„å®‰å…¨ï¼Œå®žçŽ°äº† RDD çš„å®¹é”™å’Œé«˜å¯ç”¨ã€‚

ç”¨æ³•ï¼š

```
SparkContext.setCheckpointDir("ç›®å½•") //HDFSçš„ç›®å½•

RDD.checkpoint
```

- **æ€»ç»“ï¼š**
- å¼€å‘ä¸­å¦‚ä½•ä¿è¯æ•°æ®çš„å®‰å…¨æ€§æ€§åŠè¯»å–æ•ˆçŽ‡ï¼šå¯ä»¥å¯¹é¢‘ç¹ä½¿ç”¨ä¸”é‡è¦çš„æ•°æ®ï¼Œå…ˆåšç¼“å­˜/æŒä¹…åŒ–ï¼Œå†åš checkpint æ“ä½œã€‚
- æŒä¹…åŒ–å’Œ Checkpoint çš„åŒºåˆ«ï¼š

1. ä½ç½®ï¼šPersist å’Œ Cache åªèƒ½ä¿å­˜åœ¨æœ¬åœ°çš„ç£ç›˜å’Œå†…å­˜ä¸­(æˆ–è€…å †å¤–å†…å­˜--å®žéªŒä¸­) Checkpoint å¯ä»¥ä¿å­˜æ•°æ®åˆ° HDFS è¿™ç±»å¯é çš„å­˜å‚¨ä¸Šã€‚
2. ç”Ÿå‘½å‘¨æœŸï¼šCache å’Œ Persist çš„ RDD ä¼šåœ¨ç¨‹åºç»“æŸåŽä¼šè¢«æ¸…é™¤æˆ–è€…æ‰‹åŠ¨è°ƒç”¨ unpersist æ–¹æ³• Checkpoint çš„ RDD åœ¨ç¨‹åºç»“æŸåŽä¾ç„¶å­˜åœ¨ï¼Œä¸ä¼šè¢«åˆ é™¤ã€‚

### 5. RDD ä¾èµ–å…³ç³»

#### 1) å®½çª„ä¾èµ–

- ä¸¤ç§ä¾èµ–å…³ç³»ç±»åž‹ï¼šRDD å’Œå®ƒä¾èµ–çš„çˆ¶ RDD çš„å…³ç³»æœ‰ä¸¤ç§ä¸åŒçš„ç±»åž‹ï¼Œå³**å®½ä¾èµ–**(wide dependency/shuffle dependency)**çª„ä¾èµ–**(narrow dependency)

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BCl5j7ByHibBxOyX6ZbNpe2u6ziad51JUt6cZW0FbbRrJjSrYDW2qDh7D3Q/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

- å›¾è§£ï¼š

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BCl1Phib8uA6rk5oVoRr50j8fHLaYbsDMfzeice66uHxpiaQADP5PcmSCGGQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)å®½çª„ä¾èµ–

- å¦‚ä½•åŒºåˆ†å®½çª„ä¾èµ–ï¼š

çª„ä¾èµ–:çˆ¶ RDD çš„ä¸€ä¸ªåˆ†åŒºåªä¼šè¢«å­ RDD çš„ä¸€ä¸ªåˆ†åŒºä¾èµ–ï¼›
å®½ä¾èµ–:çˆ¶ RDD çš„ä¸€ä¸ªåˆ†åŒºä¼šè¢«å­ RDD çš„å¤šä¸ªåˆ†åŒºä¾èµ–(æ¶‰åŠåˆ° shuffle)ã€‚

#### 2) ä¸ºä»€ä¹ˆè¦è®¾è®¡å®½çª„ä¾èµ–

1. å¯¹äºŽçª„ä¾èµ–ï¼š

çª„ä¾èµ–çš„å¤šä¸ªåˆ†åŒºå¯ä»¥å¹¶è¡Œè®¡ç®—ï¼›
çª„ä¾èµ–çš„ä¸€ä¸ªåˆ†åŒºçš„æ•°æ®å¦‚æžœä¸¢å¤±åªéœ€è¦é‡æ–°è®¡ç®—å¯¹åº”çš„åˆ†åŒºçš„æ•°æ®å°±å¯ä»¥äº†ã€‚

1. å¯¹äºŽå®½ä¾èµ–ï¼š

åˆ’åˆ† Stage(é˜¶æ®µ)çš„ä¾æ®:å¯¹äºŽå®½ä¾èµ–,å¿…é¡»ç­‰åˆ°ä¸Šä¸€é˜¶æ®µè®¡ç®—å®Œæˆæ‰èƒ½è®¡ç®—ä¸‹ä¸€é˜¶æ®µã€‚

### 6. DAG çš„ç”Ÿæˆå’Œåˆ’åˆ† Stage

#### 1) DAG ä»‹ç»

- DAG æ˜¯ä»€ä¹ˆï¼š

DAG(Directed Acyclic Graph æœ‰å‘æ— çŽ¯å›¾)æŒ‡çš„æ˜¯æ•°æ®è½¬æ¢æ‰§è¡Œçš„è¿‡ç¨‹ï¼Œæœ‰æ–¹å‘ï¼Œæ— é—­çŽ¯(å…¶å®žå°±æ˜¯ RDD æ‰§è¡Œçš„æµç¨‹)ï¼›
åŽŸå§‹çš„ RDD é€šè¿‡ä¸€ç³»åˆ—çš„è½¬æ¢æ“ä½œå°±å½¢æˆäº† DAG æœ‰å‘æ— çŽ¯å›¾ï¼Œä»»åŠ¡æ‰§è¡Œæ—¶ï¼Œå¯ä»¥æŒ‰ç…§ DAG çš„æè¿°ï¼Œæ‰§è¡ŒçœŸæ­£çš„è®¡ç®—(æ•°æ®è¢«æ“ä½œçš„ä¸€ä¸ªè¿‡ç¨‹)ã€‚

- DAG çš„è¾¹ç•Œ

å¼€å§‹:é€šè¿‡ SparkContext åˆ›å»ºçš„ RDDï¼›
ç»“æŸ:è§¦å‘ Actionï¼Œä¸€æ—¦è§¦å‘ Action å°±å½¢æˆäº†ä¸€ä¸ªå®Œæ•´çš„ DAGã€‚

#### 2) DAG åˆ’åˆ† Stage

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClUh9icmu8GUhXHpptjuVvudsRJOWKUCmQaO892YS24tUZ4QbiaibyY3psw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)DAGåˆ’åˆ†Stage

**ä¸€ä¸ª Spark ç¨‹åºå¯ä»¥æœ‰å¤šä¸ª DAG(æœ‰å‡ ä¸ª Actionï¼Œå°±æœ‰å‡ ä¸ª DAGï¼Œä¸Šå›¾æœ€åŽåªæœ‰ä¸€ä¸ª Actionï¼ˆå›¾ä¸­æœªè¡¨çŽ°ï¼‰,é‚£ä¹ˆå°±æ˜¯ä¸€ä¸ª DAG)**ã€‚

ä¸€ä¸ª DAG å¯ä»¥æœ‰å¤šä¸ª Stage(æ ¹æ®å®½ä¾èµ–/shuffle è¿›è¡Œåˆ’åˆ†)ã€‚

**åŒä¸€ä¸ª Stage å¯ä»¥æœ‰å¤šä¸ª Task å¹¶è¡Œæ‰§è¡Œ**(**task æ•°=åˆ†åŒºæ•°**ï¼Œå¦‚ä¸Šå›¾ï¼ŒStage1 ä¸­æœ‰ä¸‰ä¸ªåˆ†åŒº P1ã€P2ã€P3ï¼Œå¯¹åº”çš„ä¹Ÿæœ‰ä¸‰ä¸ª Task)ã€‚

å¯ä»¥çœ‹åˆ°è¿™ä¸ª DAG ä¸­åª reduceByKey æ“ä½œæ˜¯ä¸€ä¸ªå®½ä¾èµ–ï¼ŒSpark å†…æ ¸ä¼šä»¥æ­¤ä¸ºè¾¹ç•Œå°†å…¶å‰åŽåˆ’åˆ†æˆä¸åŒçš„ Stageã€‚

åŒæ—¶æˆ‘ä»¬å¯ä»¥æ³¨æ„åˆ°ï¼Œåœ¨å›¾ä¸­ Stage1 ä¸­ï¼Œ**ä»Ž textFile åˆ° flatMap åˆ° map éƒ½æ˜¯çª„ä¾èµ–ï¼Œè¿™å‡ æ­¥æ“ä½œå¯ä»¥å½¢æˆä¸€ä¸ªæµæ°´çº¿æ“ä½œï¼Œé€šè¿‡ flatMap æ“ä½œç”Ÿæˆçš„ partition å¯ä»¥ä¸ç”¨ç­‰å¾…æ•´ä¸ª RDD è®¡ç®—ç»“æŸï¼Œè€Œæ˜¯ç»§ç»­è¿›è¡Œ map æ“ä½œï¼Œè¿™æ ·å¤§å¤§æé«˜äº†è®¡ç®—çš„æ•ˆçŽ‡**ã€‚

- ä¸ºä»€ä¹ˆè¦åˆ’åˆ† Stage? --å¹¶è¡Œè®¡ç®—

ä¸€ä¸ªå¤æ‚çš„ä¸šåŠ¡é€»è¾‘å¦‚æžœæœ‰ shuffleï¼Œé‚£ä¹ˆå°±æ„å‘³ç€å‰é¢é˜¶æ®µäº§ç”Ÿç»“æžœåŽï¼Œæ‰èƒ½æ‰§è¡Œä¸‹ä¸€ä¸ªé˜¶æ®µï¼Œå³ä¸‹ä¸€ä¸ªé˜¶æ®µçš„è®¡ç®—è¦ä¾èµ–ä¸Šä¸€ä¸ªé˜¶æ®µçš„æ•°æ®ã€‚é‚£ä¹ˆæˆ‘ä»¬æŒ‰ç…§ shuffle è¿›è¡Œåˆ’åˆ†(ä¹Ÿå°±æ˜¯æŒ‰ç…§å®½ä¾èµ–å°±è¡Œåˆ’åˆ†)ï¼Œå°±å¯ä»¥å°†ä¸€ä¸ª DAG åˆ’åˆ†æˆå¤šä¸ª Stage/é˜¶æ®µï¼Œåœ¨åŒä¸€ä¸ª Stage ä¸­ï¼Œä¼šæœ‰å¤šä¸ªç®—å­æ“ä½œï¼Œå¯ä»¥å½¢æˆä¸€ä¸ª pipeline æµæ°´çº¿ï¼Œæµæ°´çº¿å†…çš„å¤šä¸ªå¹³è¡Œçš„åˆ†åŒºå¯ä»¥å¹¶è¡Œæ‰§è¡Œã€‚

- å¦‚ä½•åˆ’åˆ† DAG çš„ stageï¼Ÿ

å¯¹äºŽçª„ä¾èµ–ï¼Œpartition çš„è½¬æ¢å¤„ç†åœ¨ stage ä¸­å®Œæˆè®¡ç®—ï¼Œä¸åˆ’åˆ†(å°†çª„ä¾èµ–å°½é‡æ”¾åœ¨åœ¨åŒä¸€ä¸ª stage ä¸­ï¼Œå¯ä»¥å®žçŽ°æµæ°´çº¿è®¡ç®—)ã€‚

å¯¹äºŽå®½ä¾èµ–ï¼Œç”±äºŽæœ‰ shuffle çš„å­˜åœ¨ï¼Œåªèƒ½åœ¨çˆ¶ RDD å¤„ç†å®ŒæˆåŽï¼Œæ‰èƒ½å¼€å§‹æŽ¥ä¸‹æ¥çš„è®¡ç®—ï¼Œä¹Ÿå°±æ˜¯è¯´éœ€è¦è¦åˆ’åˆ† stageã€‚

**æ€»ç»“ï¼š**

Spark ä¼šæ ¹æ® shuffle/å®½ä¾èµ–ä½¿ç”¨å›žæº¯ç®—æ³•æ¥å¯¹ DAG è¿›è¡Œ Stage åˆ’åˆ†ï¼Œä»ŽåŽå¾€å‰ï¼Œé‡åˆ°å®½ä¾èµ–å°±æ–­å¼€ï¼Œé‡åˆ°çª„ä¾èµ–å°±æŠŠå½“å‰çš„ RDD åŠ å…¥åˆ°å½“å‰çš„ stage/é˜¶æ®µä¸­

å…·ä½“çš„åˆ’åˆ†ç®—æ³•è¯·å‚è§ AMP å®žéªŒå®¤å‘è¡¨çš„è®ºæ–‡ï¼šã€ŠResilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computingã€‹
`http://xueshu.baidu.com/usercenter/paper/show?paperid=b33564e60f0a7e7a1889a9da10963461&site=xueshu_se`

### 7. RDD ç´¯åŠ å™¨å’Œå¹¿æ’­å˜é‡

åœ¨é»˜è®¤æƒ…å†µä¸‹ï¼Œå½“ Spark åœ¨é›†ç¾¤çš„å¤šä¸ªä¸åŒèŠ‚ç‚¹çš„å¤šä¸ªä»»åŠ¡ä¸Šå¹¶è¡Œè¿è¡Œä¸€ä¸ªå‡½æ•°æ—¶ï¼Œå®ƒä¼šæŠŠå‡½æ•°ä¸­æ¶‰åŠåˆ°çš„æ¯ä¸ªå˜é‡ï¼Œåœ¨æ¯ä¸ªä»»åŠ¡ä¸Šéƒ½ç”Ÿæˆä¸€ä¸ªå‰¯æœ¬ã€‚ä½†æ˜¯ï¼Œæœ‰æ—¶å€™éœ€è¦åœ¨å¤šä¸ªä»»åŠ¡ä¹‹é—´å…±äº«å˜é‡ï¼Œæˆ–è€…åœ¨ä»»åŠ¡(Task)å’Œä»»åŠ¡æŽ§åˆ¶èŠ‚ç‚¹(Driver Program)ä¹‹é—´å…±äº«å˜é‡ã€‚

ä¸ºäº†æ»¡è¶³è¿™ç§éœ€æ±‚ï¼ŒSpark æä¾›äº†ä¸¤ç§ç±»åž‹çš„å˜é‡ï¼š

1. **ç´¯åŠ å™¨ accumulators**ï¼šç´¯åŠ å™¨æ”¯æŒåœ¨æ‰€æœ‰ä¸åŒèŠ‚ç‚¹ä¹‹é—´è¿›è¡Œç´¯åŠ è®¡ç®—(æ¯”å¦‚è®¡æ•°æˆ–è€…æ±‚å’Œ)ã€‚
2. **å¹¿æ’­å˜é‡ broadcast variables**ï¼šå¹¿æ’­å˜é‡ç”¨æ¥æŠŠå˜é‡åœ¨æ‰€æœ‰èŠ‚ç‚¹çš„å†…å­˜ä¹‹é—´è¿›è¡Œå…±äº«ï¼Œåœ¨æ¯ä¸ªæœºå™¨ä¸Šç¼“å­˜ä¸€ä¸ªåªè¯»çš„å˜é‡ï¼Œè€Œä¸æ˜¯ä¸ºæœºå™¨ä¸Šçš„æ¯ä¸ªä»»åŠ¡éƒ½ç”Ÿæˆä¸€ä¸ªå‰¯æœ¬ã€‚

#### 1) ç´¯åŠ å™¨

**1. ä¸ä½¿ç”¨ç´¯åŠ å™¨**

```
var counter = 0
val data = Seq(1, 2, 3)
data.foreach(x => counter += x)
println("Counter value: "+ counter)
```

è¿è¡Œç»“æžœï¼š

```
Counter value: 6
```

å¦‚æžœæˆ‘ä»¬å°† data è½¬æ¢æˆ RDDï¼Œå†æ¥é‡æ–°è®¡ç®—ï¼š

```
var counter = 0
val data = Seq(1, 2, 3)
var rdd = sc.parallelize(data)
rdd.foreach(x => counter += x)
println("Counter value: "+ counter)
```

è¿è¡Œç»“æžœï¼š

```
Counter value: 0
```

**2. ä½¿ç”¨ç´¯åŠ å™¨**

é€šå¸¸åœ¨å‘ Spark ä¼ é€’å‡½æ•°æ—¶ï¼Œæ¯”å¦‚ä½¿ç”¨ map() å‡½æ•°æˆ–è€…ç”¨ filter() ä¼ æ¡ä»¶æ—¶ï¼Œå¯ä»¥ä½¿ç”¨é©±åŠ¨å™¨ç¨‹åºä¸­å®šä¹‰çš„å˜é‡ï¼Œä½†æ˜¯é›†ç¾¤ä¸­è¿è¡Œçš„æ¯ä¸ªä»»åŠ¡éƒ½ä¼šå¾—åˆ°è¿™äº›å˜é‡çš„ä¸€ä»½æ–°çš„å‰¯æœ¬ï¼Œæ›´æ–°è¿™äº›å‰¯æœ¬çš„å€¼ä¹Ÿä¸ä¼šå½±å“é©±åŠ¨å™¨ä¸­çš„å¯¹åº”å˜é‡ã€‚è¿™æ—¶ä½¿ç”¨ç´¯åŠ å™¨å°±å¯ä»¥å®žçŽ°æˆ‘ä»¬æƒ³è¦çš„æ•ˆæžœ:

**val xx: Accumulator[Int] = sc.accumulator(0)**

**3. ä»£ç ç¤ºä¾‹**ï¼š

```
import org.apache.spark.rdd.RDD
import org.apache.spark.{Accumulator, SparkConf, SparkContext}

object AccumulatorTest {
  def main(args: Array[String]): Unit = {
    val conf: SparkConf = new SparkConf().setAppName("wc").setMaster("local[*]")
    val sc: SparkContext = new SparkContext(conf)
    sc.setLogLevel("WARN")

    //ä½¿ç”¨scalaé›†åˆå®Œæˆç´¯åŠ 
    var counter1: Int = 0;
    var data = Seq(1,2,3)
    data.foreach(x => counter1 += x )
    println(counter1)//6

    println("+++++++++++++++++++++++++")

    //ä½¿ç”¨RDDè¿›è¡Œç´¯åŠ 
    var counter2: Int = 0;
    val dataRDD: RDD[Int] = sc.parallelize(data) //åˆ†å¸ƒå¼é›†åˆçš„[1,2,3]
    dataRDD.foreach(x => counter2 += x)
    println(counter2)//0
    //æ³¨æ„ï¼šä¸Šé¢çš„RDDæ“ä½œè¿è¡Œç»“æžœæ˜¯0
    //å› ä¸ºforeachä¸­çš„å‡½æ•°æ˜¯ä¼ é€’ç»™Workerä¸­çš„Executoræ‰§è¡Œ,ç”¨åˆ°äº†counter2å˜é‡
    //è€Œcounter2å˜é‡åœ¨Driverç«¯å®šä¹‰çš„,åœ¨ä¼ é€’ç»™Executorçš„æ—¶å€™,å„ä¸ªExecutoréƒ½æœ‰äº†ä¸€ä»½counter2
    //æœ€åŽå„ä¸ªExecutorå°†å„è‡ªä¸ªxåŠ åˆ°è‡ªå·±çš„counter2ä¸Šé¢äº†,å’ŒDriverç«¯çš„counter2æ²¡æœ‰å…³ç³»

    //é‚£è¿™ä¸ªé—®é¢˜å¾—è§£å†³å•Š!ä¸èƒ½å› ä¸ºä½¿ç”¨äº†Sparkè¿žç´¯åŠ éƒ½åšä¸äº†äº†å•Š!
    //å¦‚æžœè§£å†³?---ä½¿ç”¨ç´¯åŠ å™¨
    val counter3: Accumulator[Int] = sc.accumulator(0)
    dataRDD.foreach(x => counter3 += x)
    println(counter3)//6
  }
}
```

#### 2) å¹¿æ’­å˜é‡

**1. ä¸ä½¿ç”¨å¹¿æ’­å˜é‡**

**2. ä½¿ç”¨å¹¿æ’­å˜é‡**

**3. ä»£ç ç¤ºä¾‹**ï¼š

å…³é”®è¯ï¼š**sc.broadcast()**

```
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

object BroadcastVariablesTest {
  def main(args: Array[String]): Unit = {
    val conf: SparkConf = new SparkConf().setAppName("wc").setMaster("local[*]")
    val sc: SparkContext = new SparkContext(conf)
    sc.setLogLevel("WARN")

    //ä¸ä½¿ç”¨å¹¿æ’­å˜é‡
    val kvFruit: RDD[(Int, String)] = sc.parallelize(List((1,"apple"),(2,"orange"),(3,"banana"),(4,"grape")))
    val fruitMap: collection.Map[Int, String] =kvFruit.collectAsMap
    //scala.collection.Map[Int,String] = Map(2 -> orange, 4 -> grape, 1 -> apple, 3 -> banana)
    val fruitIds: RDD[Int] = sc.parallelize(List(2,4,1,3))
    //æ ¹æ®æ°´æžœç¼–å·å–æ°´æžœåç§°
    val fruitNames: RDD[String] = fruitIds.map(x=>fruitMap(x))
    fruitNames.foreach(println)
    //æ³¨æ„:ä»¥ä¸Šä»£ç çœ‹ä¼¼ä¸€ç‚¹é—®é¢˜æ²¡æœ‰,ä½†æ˜¯è€ƒè™‘åˆ°æ•°æ®é‡å¦‚æžœè¾ƒå¤§,ä¸”Taskæ•°è¾ƒå¤š,
    //é‚£ä¹ˆä¼šå¯¼è‡´,è¢«å„ä¸ªTaskå…±ç”¨åˆ°çš„fruitMapä¼šè¢«å¤šæ¬¡ä¼ è¾“
    //åº”è¯¥è¦å‡å°‘fruitMapçš„ä¼ è¾“,ä¸€å°æœºå™¨ä¸Šä¸€ä¸ª,è¢«è¯¥å°æœºå™¨ä¸­çš„Taskå…±ç”¨å³å¯
    //å¦‚ä½•åšåˆ°?---ä½¿ç”¨å¹¿æ’­å˜é‡
    //æ³¨æ„:å¹¿æ’­å˜é‡çš„å€¼ä¸èƒ½è¢«ä¿®æ”¹,å¦‚éœ€ä¿®æ”¹å¯ä»¥å°†æ•°æ®å­˜åˆ°å¤–éƒ¨æ•°æ®æº,å¦‚MySQLã€Redis
    println("=====================")
    val BroadcastFruitMap: Broadcast[collection.Map[Int, String]] = sc.broadcast(fruitMap)
    val fruitNames2: RDD[String] = fruitIds.map(x=>BroadcastFruitMap.value(x))
    fruitNames2.foreach(println)

  }
}
```

## ä¸‰ã€Spark SQL

### 1. æ•°æ®åˆ†æžæ–¹å¼

#### 1) å‘½ä»¤å¼

åœ¨å‰é¢çš„ RDD éƒ¨åˆ†, éžå¸¸æ˜Žæ˜¾å¯ä»¥æ„Ÿè§‰çš„åˆ°æ˜¯å‘½ä»¤å¼çš„, ä¸»è¦ç‰¹å¾æ˜¯é€šè¿‡ä¸€ä¸ªç®—å­, å¯ä»¥å¾—åˆ°ä¸€ä¸ªç»“æžœ, é€šè¿‡ç»“æžœå†è¿›è¡ŒåŽç»­è®¡ç®—ã€‚

```
sc.textFile("...")
  .flatMap(_.split(" "))
  .map((_, 1))
  .reduceByKey(_ + _)
  .collect()
```

1. å‘½ä»¤å¼çš„ä¼˜ç‚¹

- æ“ä½œç²’åº¦æ›´ç»†ï¼Œèƒ½å¤ŸæŽ§åˆ¶æ•°æ®çš„æ¯ä¸€ä¸ªå¤„ç†çŽ¯èŠ‚ï¼›
- æ“ä½œæ›´æ˜Žç¡®ï¼Œæ­¥éª¤æ›´æ¸…æ™°ï¼Œå®¹æ˜“ç»´æŠ¤ï¼›
- æ”¯æŒåŠ/éžç»“æž„åŒ–æ•°æ®çš„æ“ä½œã€‚

1. å‘½ä»¤å¼çš„ç¼ºç‚¹

- éœ€è¦ä¸€å®šçš„ä»£ç åŠŸåº•ï¼›
- å†™èµ·æ¥æ¯”è¾ƒéº»çƒ¦ã€‚

#### 2) SQL

å¯¹äºŽä¸€äº›æ•°æ®ç§‘å­¦å®¶/æ•°æ®åº“ç®¡ç†å‘˜/DBA, è¦æ±‚ä»–ä»¬ä¸ºäº†åšä¸€ä¸ªéžå¸¸ç®€å•çš„æŸ¥è¯¢, å†™ä¸€å¤§å †ä»£ç , æ˜Žæ˜¾æ˜¯ä¸€ä»¶éžå¸¸æ®‹å¿çš„äº‹æƒ…, æ‰€ä»¥ SQL on Hadoop æ˜¯ä¸€ä¸ªéžå¸¸é‡è¦çš„æ–¹å‘ã€‚

```
SELECT
   name,
   age,
   school
FROM students
WHERE age > 10
```

1. SQL çš„ä¼˜ç‚¹

è¡¨è¾¾éžå¸¸æ¸…æ™°, æ¯”å¦‚è¯´è¿™æ®µ SQL æ˜Žæ˜¾å°±æ˜¯ä¸ºäº†æŸ¥è¯¢ä¸‰ä¸ªå­—æ®µï¼Œæ¡ä»¶æ˜¯æŸ¥è¯¢å¹´é¾„å¤§äºŽ 10 å²çš„ã€‚

1. SQL çš„ç¼ºç‚¹

- è¯•æƒ³ä¸€ä¸‹ 3 å±‚åµŒå¥—çš„ SQL ç»´æŠ¤èµ·æ¥åº”è¯¥æŒºåŠ›ä¸ä»Žå¿ƒçš„å§ï¼›
- è¯•æƒ³ä¸€ä¸‹å¦‚æžœä½¿ç”¨ SQL æ¥å®žçŽ°æœºå™¨å­¦ä¹ ç®—æ³•ä¹ŸæŒºä¸ºéš¾çš„å§ã€‚

#### 3) æ€»ç»“

SQL æ“…é•¿æ•°æ®åˆ†æžå’Œé€šè¿‡ç®€å•çš„è¯­æ³•è¡¨ç¤ºæŸ¥è¯¢ï¼Œå‘½ä»¤å¼æ“ä½œé€‚åˆè¿‡ç¨‹å¼å¤„ç†å’Œç®—æ³•æ€§çš„å¤„ç†ã€‚

åœ¨ Spark å‡ºçŽ°ä¹‹å‰ï¼Œå¯¹äºŽç»“æž„åŒ–æ•°æ®çš„æŸ¥è¯¢å’Œå¤„ç†ï¼Œ ä¸€ä¸ªå·¥å…·ä¸€å‘åªèƒ½æ”¯æŒ SQL æˆ–è€…å‘½ä»¤å¼ï¼Œä½¿ç”¨è€…è¢«è¿«è¦ä½¿ç”¨å¤šä¸ªå·¥å…·æ¥é€‚åº”ä¸¤ç§åœºæ™¯ï¼Œå¹¶ä¸”å¤šä¸ªå·¥å…·é…åˆèµ·æ¥æ¯”è¾ƒè´¹åŠ²ã€‚

è€Œ Spark å‡ºçŽ°äº†ä»¥åŽï¼Œç»Ÿä¸€äº†ä¸¤ç§æ•°æ®å¤„ç†èŒƒå¼æ˜¯ä¸€ç§é©æ–°æ€§çš„è¿›æ­¥ã€‚

### 2. SparkSQL å‰ä¸–ä»Šç”Ÿ

SQL æ˜¯æ•°æ®åˆ†æžé¢†åŸŸä¸€ä¸ªéžå¸¸é‡è¦çš„èŒƒå¼ï¼Œæ‰€ä»¥ Spark ä¸€ç›´æƒ³è¦æ”¯æŒè¿™ç§èŒƒå¼ï¼Œè€Œä¼´éšç€ä¸€äº›å†³ç­–å¤±è¯¯ï¼Œè¿™ä¸ªè¿‡ç¨‹å…¶å®žè¿˜æ˜¯éžå¸¸æ›²æŠ˜çš„ã€‚

#### 1) å‘å±•åŽ†å²

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClAHtANNQia2KnMZgLdct1xefvT3fwm7pamHib01gXcR1Gibg1xKBgy8XnQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

- **Hive**

è§£å†³çš„é—®é¢˜:

Hive å®žçŽ°äº† SQL on Hadoopï¼Œä½¿ç”¨ MapReduce æ‰§è¡Œä»»åŠ¡ ç®€åŒ–äº† MapReduce ä»»åŠ¡ã€‚

æ–°çš„é—®é¢˜:

Hive çš„æŸ¥è¯¢å»¶è¿Ÿæ¯”è¾ƒé«˜ï¼ŒåŽŸå› æ˜¯ä½¿ç”¨ MapReduce åšè®¡ç®—ã€‚

- **Shark**

è§£å†³çš„é—®é¢˜ï¼š

Shark æ”¹å†™ Hive çš„ç‰©ç†æ‰§è¡Œè®¡åˆ’ï¼Œ ä½¿ç”¨ Spark ä»£æ›¿ MapReduce ç‰©ç†å¼•æ“Ž ä½¿ç”¨åˆ—å¼å†…å­˜å­˜å‚¨ã€‚ä»¥ä¸Šä¸¤ç‚¹ä½¿å¾— Shark çš„æŸ¥è¯¢æ•ˆçŽ‡å¾ˆé«˜ã€‚

æ–°çš„é—®é¢˜ï¼š

Shark æ‰§è¡Œè®¡åˆ’çš„ç”Ÿæˆä¸¥é‡ä¾èµ– Hiveï¼Œæƒ³è¦å¢žåŠ æ–°çš„ä¼˜åŒ–éžå¸¸å›°éš¾ï¼›

Hive æ˜¯è¿›ç¨‹çº§åˆ«çš„å¹¶è¡Œï¼ŒSpark æ˜¯çº¿ç¨‹çº§åˆ«çš„å¹¶è¡Œï¼Œæ‰€ä»¥ Hive ä¸­å¾ˆå¤šçº¿ç¨‹ä¸å®‰å…¨çš„ä»£ç ä¸é€‚ç”¨äºŽ Sparkï¼›

ç”±äºŽä»¥ä¸Šé—®é¢˜ï¼ŒShark ç»´æŠ¤äº† Hive çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå¹¶ä¸”æ— æ³•åˆå¹¶è¿›ä¸»çº¿ï¼Œéš¾ä»¥ä¸ºç»§ï¼›

åœ¨ 2014 å¹´ 7 æœˆ 1 æ—¥çš„ Spark Summit ä¸Šï¼ŒDatabricks å®£å¸ƒç»ˆæ­¢å¯¹ Shark çš„å¼€å‘ï¼Œå°†é‡ç‚¹æ”¾åˆ° Spark SQL ä¸Šã€‚

- **SparkSQL-DataFrame**

è§£å†³çš„é—®é¢˜ï¼š

Spark SQL æ‰§è¡Œè®¡åˆ’å’Œä¼˜åŒ–äº¤ç»™ä¼˜åŒ–å™¨ Catalystï¼›

å†…å»ºäº†ä¸€å¥—ç®€å•çš„ SQL è§£æžå™¨ï¼Œå¯ä»¥ä¸ä½¿ç”¨ HQLï¼›

è¿˜å¼•å…¥å’Œ DataFrame è¿™æ ·çš„ DSL APIï¼Œå®Œå…¨å¯ä»¥ä¸ä¾èµ–ä»»ä½• Hive çš„ç»„ä»¶

æ–°çš„é—®é¢˜ï¼š

å¯¹äºŽåˆæœŸç‰ˆæœ¬çš„ SparkSQLï¼Œä¾ç„¶æœ‰æŒºå¤šé—®é¢˜ï¼Œä¾‹å¦‚åªèƒ½æ”¯æŒ SQL çš„ä½¿ç”¨ï¼Œä¸èƒ½å¾ˆå¥½çš„å…¼å®¹å‘½ä»¤å¼ï¼Œå…¥å£ä¸å¤Ÿç»Ÿä¸€ç­‰ã€‚

- **SparkSQL-Dataset**

SparkSQL åœ¨ 1.6 æ—¶ä»£ï¼Œå¢žåŠ äº†ä¸€ä¸ªæ–°çš„ APIï¼Œå«åš Datasetï¼ŒDataset ç»Ÿä¸€å’Œç»“åˆäº† SQL çš„è®¿é—®å’Œå‘½ä»¤å¼ API çš„ä½¿ç”¨ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ’æ—¶ä»£çš„è¿›æ­¥ã€‚

åœ¨ Dataset ä¸­å¯ä»¥è½»æ˜“çš„åšåˆ°ä½¿ç”¨ SQL æŸ¥è¯¢å¹¶ä¸”ç­›é€‰æ•°æ®ï¼Œç„¶åŽä½¿ç”¨å‘½ä»¤å¼ API è¿›è¡ŒæŽ¢ç´¢å¼åˆ†æžã€‚

### 3. Hive å’Œ SparkSQL

Hive æ˜¯å°† SQL è½¬ä¸º MapReduceã€‚

SparkSQL å¯ä»¥ç†è§£æˆæ˜¯å°† SQL è§£æžæˆï¼šâ€œRDD + ä¼˜åŒ–â€ å†æ‰§è¡Œã€‚

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClGLiczIlHwy6lxKTGQAWrE0KQdDibN1OF5Cqo1N4X1OF606bFicuQnpqZQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

### 4. æ•°æ®åˆ†ç±»å’Œ SparkSQL é€‚ç”¨åœºæ™¯

#### 1) ç»“æž„åŒ–æ•°æ®

ä¸€èˆ¬æŒ‡æ•°æ®æœ‰å›ºå®šçš„ **Schema**(çº¦æŸ)ï¼Œä¾‹å¦‚åœ¨ç”¨æˆ·è¡¨ä¸­ï¼Œname å­—æ®µæ˜¯ String åž‹ï¼Œé‚£ä¹ˆæ¯ä¸€æ¡æ•°æ®çš„ name å­—æ®µå€¼éƒ½å¯ä»¥å½“ä½œ String æ¥ä½¿ç”¨ï¼š

|  id  |   name   |            url            | alexa | country |
| :--: | :------: | :-----------------------: | :---: | :-----: |
|  1   |  Google  |  https://www.google.cm/   |   1   |   USA   |
|  2   |   æ·˜å®   |  https://www.taobao.com/  |  13   |   CN    |
|  3   | èœé¸Ÿæ•™ç¨‹ |  https://www.runoob.com/  | 4689  |   CN    |
|  4   |   å¾®åš   |     http://weibo.com/     |  20   |   CN    |
|  5   | Facebook | https://www.facebook.com/ |   3   |   USA   |

#### 2) åŠç»“æž„åŒ–æ•°æ®

èˆ¬æŒ‡çš„æ˜¯æ•°æ®æ²¡æœ‰å›ºå®šçš„ Schemaï¼Œä½†æ˜¯æ•°æ®æœ¬èº«æ˜¯æœ‰ç»“æž„çš„ã€‚

- æ²¡æœ‰å›ºå®š Schema

æŒ‡çš„æ˜¯åŠç»“æž„åŒ–æ•°æ®æ˜¯æ²¡æœ‰å›ºå®šçš„ Schema çš„ï¼Œå¯ä»¥ç†è§£ä¸ºæ²¡æœ‰æ˜¾å¼æŒ‡å®š Schemaã€‚

æ¯”å¦‚è¯´ä¸€ä¸ªç”¨æˆ·ä¿¡æ¯çš„ JSON æ–‡ä»¶ï¼Œ
ç¬¬ 1 æ¡æ•°æ®çš„ phone_num æœ‰å¯èƒ½æ˜¯æ•°å­—ï¼Œ
ç¬¬ 2 æ¡æ•°æ®çš„ phone_num è™½è¯´åº”è¯¥ä¹Ÿæ˜¯æ•°å­—ï¼Œä½†æ˜¯å¦‚æžœæŒ‡å®šä¸º Stringï¼Œä¹Ÿæ˜¯å¯ä»¥çš„ï¼Œ
å› ä¸ºæ²¡æœ‰æŒ‡å®š Schemaï¼Œæ²¡æœ‰æ˜¾å¼çš„å¼ºåˆ¶çš„çº¦æŸã€‚

- æœ‰ç»“æž„

è™½è¯´åŠç»“æž„åŒ–æ•°æ®æ˜¯æ²¡æœ‰æ˜¾å¼æŒ‡å®š Schema çš„ï¼Œä¹Ÿæ²¡æœ‰çº¦æŸï¼Œä½†æ˜¯åŠç»“æž„åŒ–æ•°æ®æœ¬èº«æ˜¯æœ‰æœ‰éšå¼çš„ç»“æž„çš„ï¼Œä¹Ÿå°±æ˜¯æ•°æ®è‡ªèº«å¯ä»¥æè¿°è‡ªèº«ã€‚

ä¾‹å¦‚ JSON æ–‡ä»¶ï¼Œå…¶ä¸­çš„æŸä¸€æ¡æ•°æ®æ˜¯æœ‰å­—æ®µè¿™ä¸ªæ¦‚å¿µçš„ï¼Œæ¯ä¸ªå­—æ®µä¹Ÿæœ‰ç±»åž‹çš„æ¦‚å¿µï¼Œæ‰€ä»¥è¯´ JSON æ˜¯å¯ä»¥æè¿°è‡ªèº«çš„ï¼Œä¹Ÿå°±æ˜¯æ•°æ®æœ¬èº«æºå¸¦æœ‰å…ƒä¿¡æ¯ã€‚

#### 3) æ€»ç»“

- **æ•°æ®åˆ†ç±»æ€»ç»“**ï¼š

|              |             å®šä¹‰              |                       ç‰¹ç‚¹                        |               ä¸¾ä¾‹                |
| :----------: | :---------------------------: | :-----------------------------------------------: | :-------------------------------: |
|  ç»“æž„åŒ–æ•°æ®  |        æœ‰å›ºå®šçš„ Schema        |                 æœ‰é¢„å®šä¹‰çš„ Schema                 |         å…³ç³»åž‹æ•°æ®åº“çš„è¡¨          |
| åŠç»“æž„åŒ–æ•°æ® | æ²¡æœ‰å›ºå®šçš„ Schemaï¼Œä½†æ˜¯æœ‰ç»“æž„ | æ²¡æœ‰å›ºå®šçš„ Schemaï¼Œæœ‰ç»“æž„ä¿¡æ¯ï¼Œæ•°æ®ä¸€èˆ¬æ˜¯è‡ªæè¿°çš„ | æŒ‡ä¸€äº›æœ‰ç»“æž„çš„æ–‡ä»¶æ ¼å¼ï¼Œä¾‹å¦‚ JSON |
| éžç»“æž„åŒ–æ•°æ® |  æ²¡æœ‰å›ºå®š Schemaï¼Œä¹Ÿæ²¡æœ‰ç»“æž„  |            æ²¡æœ‰å›ºå®š Schemaï¼Œä¹Ÿæ²¡æœ‰ç»“æž„            |       æŒ‡å›¾ç‰‡/éŸ³é¢‘ä¹‹ç±»çš„æ ¼å¼       |

- **Spark å¤„ç†ä»€ä¹ˆæ ·çš„æ•°æ®**ï¼Ÿ

RDD ä¸»è¦ç”¨äºŽå¤„ç†éžç»“æž„åŒ–æ•°æ® ã€åŠç»“æž„åŒ–æ•°æ®ã€ç»“æž„åŒ–ï¼›

SparkSQL ä¸»è¦ç”¨äºŽå¤„ç†ç»“æž„åŒ–æ•°æ®(è¾ƒä¸ºè§„èŒƒçš„åŠç»“æž„åŒ–æ•°æ®ä¹Ÿå¯ä»¥å¤„ç†)ã€‚

- **æ€»ç»“**ï¼š

SparkSQL æ˜¯ä¸€ä¸ªæ—¢æ”¯æŒ SQL åˆæ”¯æŒå‘½ä»¤å¼æ•°æ®å¤„ç†çš„å·¥å…·ï¼›

SparkSQL çš„ä¸»è¦é€‚ç”¨åœºæ™¯æ˜¯å¤„ç†ç»“æž„åŒ–æ•°æ®(è¾ƒä¸ºè§„èŒƒçš„åŠç»“æž„åŒ–æ•°æ®ä¹Ÿå¯ä»¥å¤„ç†)ã€‚

### 5. Spark SQL æ•°æ®æŠ½è±¡

#### 1) DataFrame

- **ä»€ä¹ˆæ˜¯ DataFrame**

DataFrame çš„å‰èº«æ˜¯ SchemaRDDï¼Œä»Ž Spark 1.3.0 å¼€å§‹ SchemaRDD æ›´åä¸º DataFrameã€‚å¹¶ä¸å†ç›´æŽ¥ç»§æ‰¿è‡ª RDDï¼Œè€Œæ˜¯è‡ªå·±å®žçŽ°äº† RDD çš„ç»å¤§å¤šæ•°åŠŸèƒ½ã€‚

DataFrame æ˜¯ä¸€ç§ä»¥ RDD ä¸ºåŸºç¡€çš„åˆ†å¸ƒå¼æ•°æ®é›†ï¼Œç±»ä¼¼äºŽä¼ ç»Ÿæ•°æ®åº“çš„äºŒç»´è¡¨æ ¼ï¼Œå¸¦æœ‰ Schema å…ƒä¿¡æ¯(å¯ä»¥ç†è§£ä¸ºæ•°æ®åº“çš„åˆ—åå’Œç±»åž‹)ã€‚

- **æ€»ç»“**:

**DataFrame å°±æ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼çš„è¡¨**ï¼›

**DataFrame = RDD - æ³›åž‹ + SQL çš„æ“ä½œ + ä¼˜åŒ–**ã€‚

#### 2) DataSet

- **DataSet**ï¼š

DataSet æ˜¯åœ¨ Spark1.6 ä¸­æ·»åŠ çš„æ–°çš„æŽ¥å£ã€‚

ä¸Ž RDD ç›¸æ¯”ï¼Œä¿å­˜äº†æ›´å¤šçš„æè¿°ä¿¡æ¯ï¼Œæ¦‚å¿µä¸Šç­‰åŒäºŽå…³ç³»åž‹æ•°æ®åº“ä¸­çš„äºŒç»´è¡¨ã€‚

ä¸Ž DataFrame ç›¸æ¯”ï¼Œä¿å­˜äº†ç±»åž‹ä¿¡æ¯ï¼Œæ˜¯å¼ºç±»åž‹çš„ï¼Œæä¾›äº†ç¼–è¯‘æ—¶ç±»åž‹æ£€æŸ¥ã€‚

è°ƒç”¨ Dataset çš„æ–¹æ³•å…ˆä¼šç”Ÿæˆé€»è¾‘è®¡åˆ’ï¼Œç„¶åŽè¢« spark çš„ä¼˜åŒ–å™¨è¿›è¡Œä¼˜åŒ–ï¼Œæœ€ç»ˆç”Ÿæˆç‰©ç†è®¡åˆ’ï¼Œç„¶åŽæäº¤åˆ°é›†ç¾¤ä¸­è¿è¡Œï¼

DataSet åŒ…å«äº† DataFrame çš„åŠŸèƒ½ã€‚

**Spark2.0 ä¸­ä¸¤è€…ç»Ÿä¸€ï¼ŒDataFrame è¡¨ç¤ºä¸º DataSet[Row]ï¼Œå³ DataSet çš„å­é›†ã€‚**

**DataFrame å…¶å®žå°±æ˜¯ Dateset[Row]**ï¼š

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BCld2HvoMSHGy8DOeKWDOlnCk6xiaPTOGNJ4YibcAhcGibib5Kk43fc7ibICgQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

#### 3) RDDã€DataFrameã€DataSet çš„åŒºåˆ«

1. **ç»“æž„å›¾è§£**ï¼š

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClBDI1pkiafGE50MQrCia2AI1BbnmX6kC1RQkhyZayp64PdgkDsv6pLhxw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

- RDD[Person]ï¼š

  ä»¥ Person ä¸ºç±»åž‹å‚æ•°ï¼Œä½†ä¸äº†è§£ å…¶å†…éƒ¨ç»“æž„ã€‚

- DataFrameï¼š

  æä¾›äº†è¯¦ç»†çš„ç»“æž„ä¿¡æ¯ schema åˆ—çš„åç§°å’Œç±»åž‹ã€‚è¿™æ ·çœ‹èµ·æ¥å°±åƒä¸€å¼ è¡¨äº†ã€‚

- DataSet[Person]

  ä¸å…‰æœ‰ schema ä¿¡æ¯ï¼Œè¿˜æœ‰ç±»åž‹ä¿¡æ¯ã€‚

1. **æ•°æ®å›¾è§£**ï¼š

- å‡è®¾ RDD ä¸­çš„ä¸¤è¡Œæ•°æ®é•¿è¿™æ ·ï¼š

  ```
   RDD[Person]ï¼š
  ```

  ![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BCld42ALv6JzaPzbpCZA7KHu01Npp6hXEPfa1RZ89Va5hicQs3Hg9ib8tYQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

- é‚£ä¹ˆ DataFrame ä¸­çš„æ•°æ®é•¿è¿™æ ·ï¼š

  DataFrame = RDD[Person] - æ³›åž‹ + Schema + SQL æ“ä½œ + ä¼˜åŒ–ï¼š

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClzz9qRPlc36YWQ8ZuNaJkAIrBYQf2O0iaKGDWyRvJrRSCWyOUn1QF4cQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

- é‚£ä¹ˆ Dataset ä¸­çš„æ•°æ®é•¿è¿™æ ·ï¼š

  Dataset[Person] = DataFrame + æ³›åž‹ï¼š

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClPIzrhK4NTuTZEJ90wbPjlmIjgZRtUTflpnZskKdgogmGvHITibmdGpA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

- Dataset ä¹Ÿå¯èƒ½é•¿è¿™æ ·:Dataset[Row]ï¼š

  å³ DataFrame = DataSet[Row]ï¼š

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClxN9TTQPC14TlzdUqpm7b65kKAn4B28XXBbkcO1odCU7icYN5e10Vt3Q/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

#### 4) æ€»ç»“

**DataFrame = RDD - æ³›åž‹ + Schema + SQL + ä¼˜åŒ–**

**DataSet = DataFrame + æ³›åž‹**

**DataSet = RDD + Schema + SQL + ä¼˜åŒ–**

### 6. Spark SQL åº”ç”¨

- åœ¨ spark2.0 ç‰ˆæœ¬ä¹‹å‰

  SQLContext æ˜¯åˆ›å»º DataFrame å’Œæ‰§è¡Œ SQL çš„å…¥å£ã€‚

  HiveContext é€šè¿‡ hive sql è¯­å¥æ“ä½œ hive è¡¨æ•°æ®ï¼Œå…¼å®¹ hive æ“ä½œï¼ŒhiveContext ç»§æ‰¿è‡ª SQLContextã€‚

- åœ¨ spark2.0 ä¹‹åŽ

  è¿™äº›éƒ½ç»Ÿä¸€äºŽ SparkSessionï¼ŒSparkSession å°è£…äº† SqlContext åŠ HiveContextï¼›

  å®žçŽ°äº† SQLContext åŠ HiveContext æ‰€æœ‰åŠŸèƒ½ï¼›

  é€šè¿‡ SparkSession è¿˜å¯ä»¥èŽ·å–åˆ° SparkConetxtã€‚

#### 1) åˆ›å»º DataFrame/DataSet

- **è¯»å–æ–‡æœ¬æ–‡ä»¶**ï¼š

1. åœ¨æœ¬åœ°åˆ›å»ºä¸€ä¸ªæ–‡ä»¶ï¼Œæœ‰ idã€nameã€age ä¸‰åˆ—ï¼Œç”¨ç©ºæ ¼åˆ†éš”ï¼Œç„¶åŽä¸Šä¼ åˆ° hdfs ä¸Šã€‚

vim /root/person.txt

```
1 zhangsan 20
2 lisi 29
3 wangwu 25
4 zhaoliu 30
5 tianqi 35
6 kobe 40
```

1. æ‰“å¼€ spark-shell

```
spark/bin/spark-shell
```

åˆ›å»º RDD

```
val lineRDD= sc.textFile("hdfs://node1:8020/person.txt").map(_.split(" ")) //RDD[Array[String]]
```

1. å®šä¹‰ case class(ç›¸å½“äºŽè¡¨çš„ schema)

```
case class Person(id:Int, name:String, age:Int)
```

1. å°† RDD å’Œ case class å…³è” val personRDD = lineRDD.map(x => Person(x(0).toInt, x(1), x(2).toInt)) //RDD[Person]
2. å°† RDD è½¬æ¢æˆ DataFrame

```
val personDF = personRDD.toDF //DataFrame
```

1. æŸ¥çœ‹æ•°æ®å’Œ schema

personDF.show

```
+---+--------+---+
| id|    name|age|
+---+--------+---+
|  1|zhangsan| 20|
|  2|    lisi| 29|
|  3|  wangwu| 25|
|  4| zhaoliu| 30|
|  5|  tianqi| 35|
|  6|    kobe| 40|
+---+--------+---+
```

personDF.printSchema

1. æ³¨å†Œè¡¨

```
personDF.createOrReplaceTempView("t_person")
```

1. æ‰§è¡Œ SQL

```
spark.sql("select id,name from t_person where id > 3").show
```

1. ä¹Ÿå¯ä»¥é€šè¿‡ SparkSession æž„å»º DataFrame

```
val dataFrame=spark.read.text("hdfs://node1:8020/person.txt")
dataFrame.show //æ³¨æ„ï¼šç›´æŽ¥è¯»å–çš„æ–‡æœ¬æ–‡ä»¶æ²¡æœ‰å®Œæ•´schemaä¿¡æ¯
dataFrame.printSchema
```

- **è¯»å– json æ–‡ä»¶**:

```
val jsonDF= spark.read.json("file:///resources/people.json")
```

æŽ¥ä¸‹æ¥å°±å¯ä»¥ä½¿ç”¨ DataFrame çš„å‡½æ•°æ“ä½œ

```
jsonDF.show
```

> æ³¨æ„ï¼šç›´æŽ¥è¯»å– json æ–‡ä»¶æœ‰ schema ä¿¡æ¯ï¼Œå› ä¸º json æ–‡ä»¶æœ¬èº«å«æœ‰ Schema ä¿¡æ¯ï¼ŒSparkSQL å¯ä»¥è‡ªåŠ¨è§£æžã€‚

- **è¯»å– parquet æ–‡ä»¶**ï¼š

```
val parquetDF=spark.read.parquet("file:///resources/users.parquet")
```

æŽ¥ä¸‹æ¥å°±å¯ä»¥ä½¿ç”¨ DataFrame çš„å‡½æ•°æ“ä½œ

```
parquetDF.show
```

> æ³¨æ„ï¼šç›´æŽ¥è¯»å– parquet æ–‡ä»¶æœ‰ schema ä¿¡æ¯ï¼Œå› ä¸º parquet æ–‡ä»¶ä¸­ä¿å­˜äº†åˆ—çš„ä¿¡æ¯ã€‚

#### 2) ä¸¤ç§æŸ¥è¯¢é£Žæ ¼ï¼šDSL å’Œ SQL

- å‡†å¤‡å·¥ä½œï¼š

å…ˆè¯»å–æ–‡ä»¶å¹¶è½¬æ¢ä¸º DataFrame æˆ– DataSetï¼š

```
val lineRDD= sc.textFile("hdfs://node1:8020/person.txt").map(_.split(" "))
case class Person(id:Int, name:String, age:Int)
val personRDD = lineRDD.map(x => Person(x(0).toInt, x(1), x(2).toInt))
val personDF = personRDD.toDF
personDF.show
//val personDS = personRDD.toDS
//personDS.show
```

- **DSL é£Žæ ¼**:

SparkSQL æä¾›äº†ä¸€ä¸ªé¢†åŸŸç‰¹å®šè¯­è¨€(DSL)ä»¥æ–¹ä¾¿æ“ä½œç»“æž„åŒ–æ•°æ®

1. æŸ¥çœ‹ name å­—æ®µçš„æ•°æ®

```
personDF.select(personDF.col("name")).show
personDF.select(personDF("name")).show
personDF.select(col("name")).show
personDF.select("name").show
```

1. æŸ¥çœ‹ name å’Œ age å­—æ®µæ•°æ®

```
personDF.select("name", "age").show
```

1. æŸ¥è¯¢æ‰€æœ‰çš„ name å’Œ ageï¼Œå¹¶å°† age+1

```
personDF.select(personDF.col("name"), personDF.col("age") + 1).show
personDF.select(personDF("name"), personDF("age") + 1).show
personDF.select(col("name"), col("age") + 1).show
personDF.select("name","age").show
//personDF.select("name", "age"+1).show
personDF.select($"name",$"age",$"age"+1).show
```

1. è¿‡æ»¤ age å¤§äºŽç­‰äºŽ 25 çš„ï¼Œä½¿ç”¨ filter æ–¹æ³•è¿‡æ»¤

```
personDF.filter(col("age") >= 25).show
personDF.filter($"age" >25).show
```

1. ç»Ÿè®¡å¹´é¾„å¤§äºŽ 30 çš„äººæ•°

```
personDF.filter(col("age")>30).count()
personDF.filter($"age" >30).count()
```

1. æŒ‰å¹´é¾„è¿›è¡Œåˆ†ç»„å¹¶ç»Ÿè®¡ç›¸åŒå¹´é¾„çš„äººæ•°

```
personDF.groupBy("age").count().show
```

- **SQL é£Žæ ¼**:

DataFrame çš„ä¸€ä¸ªå¼ºå¤§ä¹‹å¤„å°±æ˜¯æˆ‘ä»¬å¯ä»¥å°†å®ƒçœ‹ä½œæ˜¯ä¸€ä¸ªå…³ç³»åž‹æ•°æ®è¡¨ï¼Œç„¶åŽå¯ä»¥é€šè¿‡åœ¨ç¨‹åºä¸­ä½¿ç”¨ spark.sql() æ¥æ‰§è¡Œ SQL æŸ¥è¯¢ï¼Œç»“æžœå°†ä½œä¸ºä¸€ä¸ª DataFrame è¿”å›žã€‚

å¦‚æžœæƒ³ä½¿ç”¨ SQL é£Žæ ¼çš„è¯­æ³•ï¼Œéœ€è¦å°† DataFrame æ³¨å†Œæˆè¡¨,é‡‡ç”¨å¦‚ä¸‹çš„æ–¹å¼ï¼š

```
personDF.createOrReplaceTempView("t_person")
spark.sql("select * from t_person").show
```

1. æ˜¾ç¤ºè¡¨çš„æè¿°ä¿¡æ¯

```
spark.sql("desc t_person").show
```

1. æŸ¥è¯¢å¹´é¾„æœ€å¤§çš„å‰ä¸¤å

```
spark.sql("select * from t_person order by age desc limit 2").show
```

1. æŸ¥è¯¢å¹´é¾„å¤§äºŽ 30 çš„äººçš„ä¿¡æ¯

```
spark.sql("select * from t_person where age > 30 ").show
```

1. ä½¿ç”¨ SQL é£Žæ ¼å®Œæˆ DSL ä¸­çš„éœ€æ±‚

```
spark.sql("select name, age + 1 from t_person").show
spark.sql("select name, age from t_person where age > 25").show
spark.sql("select count(age) from t_person where age > 30").show
spark.sql("select age, count(age) from t_person group by age").show
```

- **æ€»ç»“**ï¼š

1. **DataFrame å’Œ DataSet éƒ½å¯ä»¥é€šè¿‡ RDD æ¥è¿›è¡Œåˆ›å»º**ï¼›
2. **ä¹Ÿå¯ä»¥é€šè¿‡è¯»å–æ™®é€šæ–‡æœ¬åˆ›å»º--æ³¨æ„:ç›´æŽ¥è¯»å–æ²¡æœ‰å®Œæ•´çš„çº¦æŸ,éœ€è¦é€šè¿‡ RDD+Schema**ï¼›
3. **é€šè¿‡ josn/parquet ä¼šæœ‰å®Œæ•´çš„çº¦æŸ**ï¼›
4. **ä¸ç®¡æ˜¯ DataFrame è¿˜æ˜¯ DataSet éƒ½å¯ä»¥æ³¨å†Œæˆè¡¨ï¼Œä¹‹åŽå°±å¯ä»¥ä½¿ç”¨ SQL è¿›è¡ŒæŸ¥è¯¢äº†! ä¹Ÿå¯ä»¥ä½¿ç”¨ DSL**!

#### 3) Spark SQL å®Œæˆ WordCount

- **SQL é£Žæ ¼**ï¼š

```
import org.apache.spark.SparkContext
import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}


object WordCount {
  def main(args: Array[String]): Unit = {
    //1.åˆ›å»ºSparkSession
    val spark: SparkSession = SparkSession.builder().master("local[*]").appName("SparkSQL").getOrCreate()
    val sc: SparkContext = spark.sparkContext
    sc.setLogLevel("WARN")
    //2.è¯»å–æ–‡ä»¶
    val fileDF: DataFrame = spark.read.text("D:\\data\\words.txt")
    val fileDS: Dataset[String] = spark.read.textFile("D:\\data\\words.txt")
    //fileDF.show()
    //fileDS.show()
    //3.å¯¹æ¯ä¸€è¡ŒæŒ‰ç…§ç©ºæ ¼è¿›è¡Œåˆ‡åˆ†å¹¶åŽ‹å¹³
    //fileDF.flatMap(_.split(" ")) //æ³¨æ„:é”™è¯¯,å› ä¸ºDFæ²¡æœ‰æ³›åž‹,ä¸çŸ¥é“_æ˜¯String
    import spark.implicits._
    val wordDS: Dataset[String] = fileDS.flatMap(_.split(" "))//æ³¨æ„:æ­£ç¡®,å› ä¸ºDSæœ‰æ³›åž‹,çŸ¥é“_æ˜¯String
    //wordDS.show()
    /*
    +-----+
    |value|
    +-----+
    |hello|
    |   me|
    |hello|
    |  you|
      ...
     */
    //4.å¯¹ä¸Šé¢çš„æ•°æ®è¿›è¡ŒWordCount
    wordDS.createOrReplaceTempView("t_word")
    val sql =
      """
        |select value ,count(value) as count
        |from t_word
        |group by value
        |order by count desc
      """.stripMargin
    spark.sql(sql).show()

    sc.stop()
    spark.stop()
  }
}
```

- **DSL é£Žæ ¼**ï¼š

```
import org.apache.spark.SparkContext
import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}


object WordCount2 {
  def main(args: Array[String]): Unit = {
    //1.åˆ›å»ºSparkSession
    val spark: SparkSession = SparkSession.builder().master("local[*]").appName("SparkSQL").getOrCreate()
    val sc: SparkContext = spark.sparkContext
    sc.setLogLevel("WARN")
    //2.è¯»å–æ–‡ä»¶
    val fileDF: DataFrame = spark.read.text("D:\\data\\words.txt")
    val fileDS: Dataset[String] = spark.read.textFile("D:\\data\\words.txt")
    //fileDF.show()
    //fileDS.show()
    //3.å¯¹æ¯ä¸€è¡ŒæŒ‰ç…§ç©ºæ ¼è¿›è¡Œåˆ‡åˆ†å¹¶åŽ‹å¹³
    //fileDF.flatMap(_.split(" ")) //æ³¨æ„:é”™è¯¯,å› ä¸ºDFæ²¡æœ‰æ³›åž‹,ä¸çŸ¥é“_æ˜¯String
    import spark.implicits._
    val wordDS: Dataset[String] = fileDS.flatMap(_.split(" "))//æ³¨æ„:æ­£ç¡®,å› ä¸ºDSæœ‰æ³›åž‹,çŸ¥é“_æ˜¯String
    //wordDS.show()
    /*
    +-----+
    |value|
    +-----+
    |hello|
    |   me|
    |hello|
    |  you|
      ...
     */
    //4.å¯¹ä¸Šé¢çš„æ•°æ®è¿›è¡ŒWordCount
    wordDS.groupBy("value").count().orderBy($"count".desc).show()

    sc.stop()
    spark.stop()
  }
}
```

#### 4) Spark SQL å¤šæ•°æ®æºäº¤äº’

- **è¯»æ•°æ®**ï¼š

è¯»å– json æ–‡ä»¶ï¼š

```
spark.read.json("D:\\data\\output\\json").show()
```

è¯»å– csv æ–‡ä»¶ï¼š

```
spark.read.csv("D:\\data\\output\\csv").toDF("id","name","age").show()
```

è¯»å– parquet æ–‡ä»¶ï¼š

```
spark.read.parquet("D:\\data\\output\\parquet").show()
```

è¯»å– mysql è¡¨ï¼š

```
val prop = new Properties()
    prop.setProperty("user","root")
    prop.setProperty("password","root")
spark.read.jdbc(
"jdbc:mysql://localhost:3306/bigdata?characterEncoding=UTF-8","person",prop).show()
```

- **å†™æ•°æ®**ï¼š

å†™å…¥ json æ–‡ä»¶ï¼š

```
personDF.write.json("D:\\data\\output\\json")
```

å†™å…¥ csv æ–‡ä»¶ï¼š

```
personDF.write.csv("D:\\data\\output\\csv")
```

å†™å…¥ parquet æ–‡ä»¶ï¼š

```
personDF.write.parquet("D:\\data\\output\\parquet")
```

å†™å…¥ mysql è¡¨ï¼š

```
val prop = new Properties()
    prop.setProperty("user","root")
    prop.setProperty("password","root")
personDF.write.mode(SaveMode.Overwrite).jdbc(
"jdbc:mysql://localhost:3306/bigdata?characterEncoding=UTF-8","person",prop)
```

## å››ã€Spark Streaming

Spark Streaming æ˜¯ä¸€ä¸ªåŸºäºŽ Spark Core ä¹‹ä¸Šçš„**å®žæ—¶è®¡ç®—æ¡†æž¶**ï¼Œå¯ä»¥ä»Žå¾ˆå¤šæ•°æ®æºæ¶ˆè´¹æ•°æ®å¹¶å¯¹æ•°æ®è¿›è¡Œå®žæ—¶çš„å¤„ç†ï¼Œå…·æœ‰é«˜åžåé‡å’Œå®¹é”™èƒ½åŠ›å¼ºç­‰ç‰¹ç‚¹ã€‚

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BCl7hAHgu94BEfXE2ibGYfRDHzBxvhKibO5yV273bOvsLwicGicGzxLyuwrjQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**Spark Streaming çš„ç‰¹ç‚¹**ï¼š

1. æ˜“ç”¨

å¯ä»¥åƒç¼–å†™ç¦»çº¿æ‰¹å¤„ç†ä¸€æ ·åŽ»ç¼–å†™æµå¼ç¨‹åºï¼Œæ”¯æŒ java/scala/python è¯­è¨€ã€‚

1. å®¹é”™

SparkStreaming åœ¨æ²¡æœ‰é¢å¤–ä»£ç å’Œé…ç½®çš„æƒ…å†µä¸‹å¯ä»¥æ¢å¤ä¸¢å¤±çš„å·¥ä½œã€‚

1. æ˜“æ•´åˆåˆ° Spark ä½“ç³»

æµå¼å¤„ç†ä¸Žæ‰¹å¤„ç†å’Œäº¤äº’å¼æŸ¥è¯¢ç›¸ç»“åˆã€‚

### 1. æ•´ä½“æµç¨‹

Spark Streaming ä¸­ï¼Œä¼šæœ‰ä¸€ä¸ªæŽ¥æ”¶å™¨ç»„ä»¶ Receiverï¼Œä½œä¸ºä¸€ä¸ªé•¿æœŸè¿è¡Œçš„ task è·‘åœ¨ä¸€ä¸ª Executor ä¸Šã€‚Receiver æŽ¥æ”¶å¤–éƒ¨çš„æ•°æ®æµå½¢æˆ input DStreamã€‚

DStream ä¼šè¢«æŒ‰ç…§æ—¶é—´é—´éš”åˆ’åˆ†æˆä¸€æ‰¹ä¸€æ‰¹çš„ RDDï¼Œå½“æ‰¹å¤„ç†é—´éš”ç¼©çŸ­åˆ°ç§’çº§æ—¶ï¼Œä¾¿å¯ä»¥ç”¨äºŽå¤„ç†å®žæ—¶æ•°æ®æµã€‚æ—¶é—´é—´éš”çš„å¤§å°å¯ä»¥ç”±å‚æ•°æŒ‡å®šï¼Œä¸€èˆ¬è®¾åœ¨ 500 æ¯«ç§’åˆ°å‡ ç§’ä¹‹é—´ã€‚

å¯¹ DStream è¿›è¡Œæ“ä½œå°±æ˜¯å¯¹ RDD è¿›è¡Œæ“ä½œï¼Œè®¡ç®—å¤„ç†çš„ç»“æžœå¯ä»¥ä¼ ç»™å¤–éƒ¨ç³»ç»Ÿã€‚

Spark Streaming çš„å·¥ä½œæµç¨‹åƒä¸‹é¢çš„å›¾æ‰€ç¤ºä¸€æ ·ï¼ŒæŽ¥å—åˆ°å®žæ—¶æ•°æ®åŽï¼Œç»™æ•°æ®åˆ†æ‰¹æ¬¡ï¼Œç„¶åŽä¼ ç»™ Spark Engine å¤„ç†æœ€åŽç”Ÿæˆè¯¥æ‰¹æ¬¡çš„ç»“æžœã€‚

### 2. æ•°æ®æŠ½è±¡

Spark Streaming çš„åŸºç¡€æŠ½è±¡æ˜¯ DStream(Discretized Streamï¼Œç¦»æ•£åŒ–æ•°æ®æµï¼Œè¿žç»­ä¸æ–­çš„æ•°æ®æµ)ï¼Œä»£è¡¨æŒç»­æ€§çš„æ•°æ®æµå’Œç»è¿‡å„ç§ Spark ç®—å­æ“ä½œåŽçš„ç»“æžœæ•°æ®æµã€‚

å¯ä»¥ä»Žä»¥ä¸‹å¤šä¸ªè§’åº¦æ·±å…¥ç†è§£ DStreamï¼š

1. DStream æœ¬è´¨ä¸Šå°±æ˜¯ä¸€ç³»åˆ—æ—¶é—´ä¸Šè¿žç»­çš„ RDD

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClOnkVIibJAVE02VmVHC8Ln4dm9vwFCicEpYjFSYOhpwUnJS3SONFHXlTA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

1. å¯¹ DStream çš„æ•°æ®çš„è¿›è¡Œæ“ä½œä¹Ÿæ˜¯æŒ‰ç…§ RDD ä¸ºå•ä½æ¥è¿›è¡Œçš„

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClrGtkDb4dapfUlGBbhu4NpBJybiaU8kXr8SiaCzXu60LBeF4oA7efLfBw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

1. å®¹é”™æ€§ï¼Œåº•å±‚ RDD ä¹‹é—´å­˜åœ¨ä¾èµ–å…³ç³»ï¼ŒDStream ç›´æŽ¥ä¹Ÿæœ‰ä¾èµ–å…³ç³»ï¼ŒRDD å…·æœ‰å®¹é”™æ€§ï¼Œé‚£ä¹ˆ DStream ä¹Ÿå…·æœ‰å®¹é”™æ€§
2. å‡†å®žæ—¶æ€§/è¿‘å®žæ—¶æ€§

Spark Streaming å°†æµå¼è®¡ç®—åˆ†è§£æˆå¤šä¸ª Spark Jobï¼Œå¯¹äºŽæ¯ä¸€æ—¶é—´æ®µæ•°æ®çš„å¤„ç†éƒ½ä¼šç»è¿‡ Spark DAG å›¾åˆ†è§£ä»¥åŠ Spark çš„ä»»åŠ¡é›†çš„è°ƒåº¦è¿‡ç¨‹ã€‚

å¯¹äºŽç›®å‰ç‰ˆæœ¬çš„ Spark Streaming è€Œè¨€ï¼Œå…¶æœ€å°çš„ Batch Size çš„é€‰å–åœ¨ 0.5~5 ç§’é’Ÿä¹‹é—´ã€‚

**æ‰€ä»¥ Spark Streaming èƒ½å¤Ÿæ»¡è¶³æµå¼å‡†å®žæ—¶è®¡ç®—åœºæ™¯ï¼Œå¯¹å®žæ—¶æ€§è¦æ±‚éžå¸¸é«˜çš„å¦‚é«˜é¢‘å®žæ—¶äº¤æ˜“åœºæ™¯åˆ™ä¸å¤ªé€‚åˆ**ã€‚

- æ€»ç»“

ç®€å•æ¥è¯´ DStream å°±æ˜¯å¯¹ RDD çš„å°è£…ï¼Œä½ å¯¹ DStream è¿›è¡Œæ“ä½œï¼Œå°±æ˜¯å¯¹ RDD è¿›è¡Œæ“ä½œã€‚

å¯¹äºŽ DataFrame/DataSet/DStream æ¥è¯´æœ¬è´¨ä¸Šéƒ½å¯ä»¥ç†è§£æˆ RDDã€‚

### 3. DStream ç›¸å…³æ“ä½œ

DStream ä¸Šçš„æ“ä½œä¸Ž RDD çš„ç±»ä¼¼ï¼Œåˆ†ä¸ºä»¥ä¸‹ä¸¤ç§ï¼š

1. Transformations(è½¬æ¢)
2. Output Operations(è¾“å‡º)/Action

#### 1) Transformations

ä»¥ä¸‹æ˜¯å¸¸è§ Transformation---éƒ½æ˜¯æ— çŠ¶æ€è½¬æ¢ï¼šå³æ¯ä¸ªæ‰¹æ¬¡çš„å¤„ç†ä¸ä¾èµ–äºŽä¹‹å‰æ‰¹æ¬¡çš„æ•°æ®ï¼š

|        Transformation         |                             å«ä¹‰                             |
| :---------------------------: | :----------------------------------------------------------: |
|           map(func)           | å¯¹ DStream ä¸­çš„å„ä¸ªå…ƒç´ è¿›è¡Œ func å‡½æ•°æ“ä½œï¼Œç„¶åŽè¿”å›žä¸€ä¸ªæ–°çš„ DStream |
|         flatMap(func)         | ä¸Ž map æ–¹æ³•ç±»ä¼¼ï¼Œåªä¸è¿‡å„ä¸ªè¾“å…¥é¡¹å¯ä»¥è¢«è¾“å‡ºä¸ºé›¶ä¸ªæˆ–å¤šä¸ªè¾“å‡ºé¡¹ |
|         filter(func)          | è¿‡æ»¤å‡ºæ‰€æœ‰å‡½æ•° func è¿”å›žå€¼ä¸º true çš„ DStream å…ƒç´ å¹¶è¿”å›žä¸€ä¸ªæ–°çš„ DStream |
|      union(otherStream)       | å°†æº DStream å’Œè¾“å…¥å‚æ•°ä¸º otherDStream çš„å…ƒç´ åˆå¹¶ï¼Œå¹¶è¿”å›žä¸€ä¸ªæ–°çš„ DStream |
| reduceByKey(func, [numTasks]) | åˆ©ç”¨ func å‡½æ•°å¯¹æº DStream ä¸­çš„ key è¿›è¡Œèšåˆæ“ä½œï¼Œç„¶åŽè¿”å›žæ–°çš„(Kï¼ŒV)å¯¹æž„æˆçš„ DStream |
| join(otherStream, [numTasks]) | è¾“å…¥ä¸º(K,V)ã€(K,W)ç±»åž‹çš„ DStreamï¼Œè¿”å›žä¸€ä¸ªæ–°çš„(Kï¼Œ(Vï¼ŒW)ç±»åž‹çš„ DStream |
|      **transform(func)**      | é€šè¿‡ RDD-to-RDD å‡½æ•°ä½œç”¨äºŽ DStream ä¸­çš„å„ä¸ª RDDï¼Œå¯ä»¥æ˜¯ä»»æ„çš„ RDD æ“ä½œï¼Œä»Žè€Œè¿”å›žä¸€ä¸ªæ–°çš„ RDD |

é™¤æ­¤ä¹‹å¤–è¿˜æœ‰ä¸€ç±»ç‰¹æ®Šçš„ Transformations---æœ‰çŠ¶æ€è½¬æ¢ï¼šå½“å‰æ‰¹æ¬¡çš„å¤„ç†éœ€è¦ä½¿ç”¨ä¹‹å‰æ‰¹æ¬¡çš„æ•°æ®æˆ–è€…ä¸­é—´ç»“æžœã€‚

æœ‰çŠ¶æ€è½¬æ¢åŒ…æ‹¬åŸºäºŽè¿½è¸ªçŠ¶æ€å˜åŒ–çš„è½¬æ¢(updateStateByKey)å’Œæ»‘åŠ¨çª—å£çš„è½¬æ¢ï¼š

1. **UpdateStateByKey(func)**
2. **Window Operations çª—å£æ“ä½œ**

#### 2) Output/Action

Output Operations å¯ä»¥å°† DStream çš„æ•°æ®è¾“å‡ºåˆ°å¤–éƒ¨çš„æ•°æ®åº“æˆ–æ–‡ä»¶ç³»ç»Ÿã€‚

å½“æŸä¸ª Output Operations è¢«è°ƒç”¨æ—¶ï¼Œspark streaming ç¨‹åºæ‰ä¼šå¼€å§‹çœŸæ­£çš„è®¡ç®—è¿‡ç¨‹(ä¸Ž RDD çš„ Action ç±»ä¼¼)ã€‚

|          Output Operation          |                             å«ä¹‰                             |
| :--------------------------------: | :----------------------------------------------------------: |
|              print()               |                         æ‰“å°åˆ°æŽ§åˆ¶å°                         |
| saveAsTextFiles(prefix, [suffix])  | ä¿å­˜æµçš„å†…å®¹ä¸ºæ–‡æœ¬æ–‡ä»¶ï¼Œæ–‡ä»¶åä¸º"prefix-TIME_IN_MS[.suffix]" |
| saveAsObjectFiles(prefix,[suffix]) | ä¿å­˜æµçš„å†…å®¹ä¸º SequenceFileï¼Œæ–‡ä»¶åä¸º "prefix-TIME_IN_MS[.suffix]" |
| saveAsHadoopFiles(prefix,[suffix]) | ä¿å­˜æµçš„å†…å®¹ä¸º hadoop æ–‡ä»¶ï¼Œæ–‡ä»¶åä¸º"prefix-TIME_IN_MS[.suffix]" |
|          foreachRDD(func)          |             å¯¹ Dstream é‡Œé¢çš„æ¯ä¸ª RDD æ‰§è¡Œ func              |

### 4. Spark Streaming å®Œæˆå®žæ—¶éœ€æ±‚

#### 1) WordCount

- é¦–å…ˆåœ¨ linux æœåŠ¡å™¨ä¸Šå®‰è£… nc å·¥å…·

  nc æ˜¯ netcat çš„ç®€ç§°ï¼ŒåŽŸæœ¬æ˜¯ç”¨æ¥è®¾ç½®è·¯ç”±å™¨,æˆ‘ä»¬å¯ä»¥åˆ©ç”¨å®ƒå‘æŸä¸ªç«¯å£å‘é€æ•°æ® yum install -y nc

- å¯åŠ¨ä¸€ä¸ªæœåŠ¡ç«¯å¹¶å¼€æ”¾ 9999 ç«¯å£,ç­‰ä¸€ä¸‹å¾€è¿™ä¸ªç«¯å£å‘æ•°æ®

  nc -lk 9999

- å‘é€æ•°æ®

- æŽ¥æ”¶æ•°æ®ï¼Œä»£ç ç¤ºä¾‹ï¼š

```
import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream}
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.streaming.{Seconds, StreamingContext}

object WordCount {
  def main(args: Array[String]): Unit = {
    //1.åˆ›å»ºStreamingContext
    //spark.master should be set as local[n], n > 1
    val conf = new SparkConf().setAppName("wc").setMaster("local[*]")
    val sc = new SparkContext(conf)
    sc.setLogLevel("WARN")
    val ssc = new StreamingContext(sc,Seconds(5))//5è¡¨ç¤º5ç§’ä¸­å¯¹æ•°æ®è¿›è¡Œåˆ‡åˆ†å½¢æˆä¸€ä¸ªRDD
    //2.ç›‘å¬SocketæŽ¥æ”¶æ•°æ®
    //ReceiverInputDStreamå°±æ˜¯æŽ¥æ”¶åˆ°çš„æ‰€æœ‰çš„æ•°æ®ç»„æˆçš„RDD,å°è£…æˆäº†DStream,æŽ¥ä¸‹æ¥å¯¹DStreamè¿›è¡Œæ“ä½œå°±æ˜¯å¯¹RDDè¿›è¡Œæ“ä½œ
    val dataDStream: ReceiverInputDStream[String] = ssc.socketTextStream("node01",9999)
    //3.æ“ä½œæ•°æ®
    val wordDStream: DStream[String] = dataDStream.flatMap(_.split(" "))
    val wordAndOneDStream: DStream[(String, Int)] = wordDStream.map((_,1))
    val wordAndCount: DStream[(String, Int)] = wordAndOneDStream.reduceByKey(_+_)
    wordAndCount.print()
    ssc.start()//å¼€å¯
    ssc.awaitTermination()//ç­‰å¾…åœæ­¢
  }
}
```

#### 2) updateStateByKey

- **é—®é¢˜**ï¼š

åœ¨ä¸Šé¢çš„é‚£ä¸ªæ¡ˆä¾‹ä¸­å­˜åœ¨è¿™æ ·ä¸€ä¸ªé—®é¢˜ï¼š

æ¯ä¸ªæ‰¹æ¬¡çš„å•è¯æ¬¡æ•°éƒ½è¢«æ­£ç¡®çš„ç»Ÿè®¡å‡ºæ¥ï¼Œä½†æ˜¯ç»“æžœä¸èƒ½ç´¯åŠ ï¼

å¦‚æžœéœ€è¦ç´¯åŠ éœ€è¦ä½¿ç”¨ updateStateByKey(func)æ¥æ›´æ–°çŠ¶æ€ã€‚

ä»£ç ç¤ºä¾‹ï¼š

```
import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream}
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.{SparkConf, SparkContext}


object WordCount2 {
  def main(args: Array[String]): Unit = {
    //1.åˆ›å»ºStreamingContext
    //spark.master should be set as local[n], n > 1
    val conf = new SparkConf().setAppName("wc").setMaster("local[*]")
    val sc = new SparkContext(conf)
    sc.setLogLevel("WARN")
    val ssc = new StreamingContext(sc,Seconds(5))//5è¡¨ç¤º5ç§’ä¸­å¯¹æ•°æ®è¿›è¡Œåˆ‡åˆ†å½¢æˆä¸€ä¸ªRDD
    //requirement failed: ....Please set it by StreamingContext.checkpoint().
    //æ³¨æ„:æˆ‘ä»¬åœ¨ä¸‹é¢ä½¿ç”¨åˆ°äº†updateStateByKeyå¯¹å½“å‰æ•°æ®å’ŒåŽ†å²æ•°æ®è¿›è¡Œç´¯åŠ 
    //é‚£ä¹ˆåŽ†å²æ•°æ®å­˜åœ¨å“ª?æˆ‘ä»¬éœ€è¦ç»™ä»–è®¾ç½®ä¸€ä¸ªcheckpointç›®å½•
    ssc.checkpoint("./wc")//å¼€å‘ä¸­HDFS
    //2.ç›‘å¬SocketæŽ¥æ”¶æ•°æ®
    //ReceiverInputDStreamå°±æ˜¯æŽ¥æ”¶åˆ°çš„æ‰€æœ‰çš„æ•°æ®ç»„æˆçš„RDD,å°è£…æˆäº†DStream,æŽ¥ä¸‹æ¥å¯¹DStreamè¿›è¡Œæ“ä½œå°±æ˜¯å¯¹RDDè¿›è¡Œæ“ä½œ
    val dataDStream: ReceiverInputDStream[String] = ssc.socketTextStream("node01",9999)
    //3.æ“ä½œæ•°æ®
    val wordDStream: DStream[String] = dataDStream.flatMap(_.split(" "))
    val wordAndOneDStream: DStream[(String, Int)] = wordDStream.map((_,1))
    //val wordAndCount: DStream[(String, Int)] = wordAndOneDStream.reduceByKey(_+_)
    //====================ä½¿ç”¨updateStateByKeyå¯¹å½“å‰æ•°æ®å’ŒåŽ†å²æ•°æ®è¿›è¡Œç´¯åŠ ====================
    val wordAndCount: DStream[(String, Int)] =wordAndOneDStream.updateStateByKey(updateFunc)
    wordAndCount.print()
    ssc.start()//å¼€å¯
    ssc.awaitTermination()//ç­‰å¾…ä¼˜é›…åœæ­¢
  }
  //currentValues:å½“å‰æ‰¹æ¬¡çš„valueå€¼,å¦‚:1,1,1 (ä»¥æµ‹è¯•æ•°æ®ä¸­çš„hadoopä¸ºä¾‹)
  //historyValue:ä¹‹å‰ç´¯è®¡çš„åŽ†å²å€¼,ç¬¬ä¸€æ¬¡æ²¡æœ‰å€¼æ˜¯0,ç¬¬äºŒæ¬¡æ˜¯3
  //ç›®æ ‡æ˜¯æŠŠå½“å‰æ•°æ®+åŽ†å²æ•°æ®è¿”å›žä½œä¸ºæ–°çš„ç»“æžœ(ä¸‹æ¬¡çš„åŽ†å²æ•°æ®)
  def updateFunc(currentValues:Seq[Int], historyValue:Option[Int] ):Option[Int] ={
    val result: Int = currentValues.sum + historyValue.getOrElse(0)
    Some(result)
  }
}
```

#### 3) reduceByKeyAndWindow

ä½¿ç”¨ä¸Šé¢çš„ä»£ç å·²ç»èƒ½å¤Ÿå®Œæˆå¯¹æ‰€æœ‰åŽ†å²æ•°æ®çš„èšåˆï¼Œä½†æ˜¯å®žé™…ä¸­å¯èƒ½ä¼šæœ‰ä¸€äº›éœ€æ±‚,éœ€è¦å¯¹æŒ‡å®šæ—¶é—´èŒƒå›´çš„æ•°æ®è¿›è¡Œç»Ÿè®¡ã€‚

æ¯”å¦‚:

ç™¾åº¦/å¾®åšçš„çƒ­æœæŽ’è¡Œæ¦œ ç»Ÿè®¡æœ€è¿‘ 24 å°æ—¶çš„çƒ­æœè¯,æ¯éš” 5 åˆ†é’Ÿæ›´æ–°ä¸€æ¬¡ï¼Œæ‰€ä»¥é¢å¯¹è¿™æ ·çš„éœ€æ±‚æˆ‘ä»¬éœ€è¦ä½¿ç”¨çª—å£æ“ä½œ Window Operationsã€‚

**å›¾è§£**ï¼š

æˆ‘ä»¬å…ˆæå‡ºä¸€ä¸ªé—®é¢˜ï¼šç»Ÿè®¡ç»è¿‡æŸçº¢ç»¿ç¯çš„æ±½è½¦æ•°é‡ä¹‹å’Œï¼Ÿ

å‡è®¾åœ¨ä¸€ä¸ªçº¢ç»¿ç¯å¤„ï¼Œæˆ‘ä»¬æ¯éš” 15 ç§’ç»Ÿè®¡ä¸€æ¬¡é€šè¿‡æ­¤çº¢ç»¿ç¯çš„æ±½è½¦æ•°é‡ï¼Œå¦‚ä¸‹å›¾ï¼š![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClKYHRmESmOj4z14NN7pmJPIuiaftlXXkT25nlhP0tP96sLzcCuR34cJQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)å¯ä»¥æŠŠæ±½è½¦çš„ç»è¿‡çœ‹æˆä¸€ä¸ªæµï¼Œæ— ç©·çš„æµï¼Œä¸æ–­æœ‰æ±½è½¦ç»è¿‡æ­¤çº¢ç»¿ç¯ï¼Œå› æ­¤æ— æ³•ç»Ÿè®¡æ€»å…±çš„æ±½è½¦æ•°é‡ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥æ¢ä¸€ç§æ€è·¯ï¼Œæ¯éš” 15 ç§’ï¼Œæˆ‘ä»¬éƒ½å°†ä¸Žä¸Šä¸€æ¬¡çš„ç»“æžœè¿›è¡Œ sum æ“ä½œï¼ˆæ»‘åŠ¨èšåˆ, ä½†æ˜¯è¿™ä¸ªç»“æžœä¼¼ä¹Žè¿˜æ˜¯æ— æ³•å›žç­”æˆ‘ä»¬çš„é—®é¢˜ï¼Œæ ¹æœ¬åŽŸå› åœ¨äºŽæµæ˜¯æ— ç•Œçš„ï¼Œæˆ‘ä»¬ä¸èƒ½é™åˆ¶æµï¼Œä½†å¯ä»¥åœ¨æœ‰ä¸€ä¸ªæœ‰ç•Œçš„èŒƒå›´å†…å¤„ç†æ— ç•Œçš„æµæ•°æ®ã€‚

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BCl8zCJRL14dusVDicowatFOYOBiatcTrmhz9sbFuDvZKDz35icYuflXRcMQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦æ¢ä¸€ä¸ªé—®é¢˜çš„ææ³•ï¼šæ¯åˆ†é’Ÿç»è¿‡æŸçº¢ç»¿ç¯çš„æ±½è½¦æ•°é‡ä¹‹å’Œï¼Ÿ

è¿™ä¸ªé—®é¢˜ï¼Œå°±ç›¸å½“äºŽä¸€ä¸ªå®šä¹‰äº†ä¸€ä¸ª Windowï¼ˆçª—å£ï¼‰ï¼Œwindow çš„ç•Œé™æ˜¯ 1 åˆ†é’Ÿï¼Œä¸”æ¯åˆ†é’Ÿå†…çš„æ•°æ®äº’ä¸å¹²æ‰°ï¼Œå› æ­¤ä¹Ÿå¯ä»¥ç§°ä¸ºç¿»æ»šï¼ˆä¸é‡åˆï¼‰çª—å£ï¼Œå¦‚ä¸‹å›¾ï¼š

ç¬¬ä¸€åˆ†é’Ÿçš„æ•°é‡ä¸º 8ï¼Œç¬¬äºŒåˆ†é’Ÿæ˜¯ 22ï¼Œç¬¬ä¸‰åˆ†é’Ÿæ˜¯ 27ã€‚ã€‚ã€‚è¿™æ ·ï¼Œ1 ä¸ªå°æ—¶å†…ä¼šæœ‰ 60 ä¸ª windowã€‚

å†è€ƒè™‘ä¸€ç§æƒ…å†µï¼Œæ¯ 30 ç§’ç»Ÿè®¡ä¸€æ¬¡è¿‡åŽ» 1 åˆ†é’Ÿçš„æ±½è½¦æ•°é‡ä¹‹å’Œï¼š![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClEV9uibzngtUOmI09YwjhDYAbUWUezia79oMKialmAF8GToD1OwmrsFgww/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)æ­¤æ—¶ï¼Œwindow å‡ºçŽ°äº†é‡åˆã€‚è¿™æ ·ï¼Œ1 ä¸ªå°æ—¶å†…ä¼šæœ‰ 120 ä¸ª windowã€‚

æ»‘åŠ¨çª—å£è½¬æ¢æ“ä½œçš„è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClaibia3K6QQ6N1mACticPY5JaBCQp8NdsaqfuYLHGlfV1UpkQX8vAkozXw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

æˆ‘ä»¬å¯ä»¥äº‹å…ˆè®¾å®šä¸€ä¸ªæ»‘åŠ¨çª—å£çš„é•¿åº¦(ä¹Ÿå°±æ˜¯çª—å£çš„æŒç»­æ—¶é—´)ï¼Œå¹¶ä¸”è®¾å®šæ»‘åŠ¨çª—å£çš„æ—¶é—´é—´éš”(æ¯éš”å¤šé•¿æ—¶é—´æ‰§è¡Œä¸€æ¬¡è®¡ç®—)ï¼Œ

æ¯”å¦‚è®¾ç½®æ»‘åŠ¨çª—å£çš„é•¿åº¦(ä¹Ÿå°±æ˜¯çª—å£çš„æŒç»­æ—¶é—´)ä¸º 24H,è®¾ç½®æ»‘åŠ¨çª—å£çš„æ—¶é—´é—´éš”(æ¯éš”å¤šé•¿æ—¶é—´æ‰§è¡Œä¸€æ¬¡è®¡ç®—)ä¸º 1H

é‚£ä¹ˆæ„æ€å°±æ˜¯:æ¯éš” 1H è®¡ç®—æœ€è¿‘ 24H çš„æ•°æ®

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClfdDyHP0aeowab0j0lgicHia1k8VcWHRibgx1EddvkMgP9ACR9lJ5RJLiag/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**ä»£ç ç¤ºä¾‹**ï¼š

```
import org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream}
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.{SparkConf, SparkContext}

object WordCount3 {
  def main(args: Array[String]): Unit = {
    //1.åˆ›å»ºStreamingContext
    //spark.master should be set as local[n], n > 1
    val conf = new SparkConf().setAppName("wc").setMaster("local[*]")
    val sc = new SparkContext(conf)
    sc.setLogLevel("WARN")
    val ssc = new StreamingContext(sc,Seconds(5))//5è¡¨ç¤º5ç§’ä¸­å¯¹æ•°æ®è¿›è¡Œåˆ‡åˆ†å½¢æˆä¸€ä¸ªRDD
    //2.ç›‘å¬SocketæŽ¥æ”¶æ•°æ®
    //ReceiverInputDStreamå°±æ˜¯æŽ¥æ”¶åˆ°çš„æ‰€æœ‰çš„æ•°æ®ç»„æˆçš„RDD,å°è£…æˆäº†DStream,æŽ¥ä¸‹æ¥å¯¹DStreamè¿›è¡Œæ“ä½œå°±æ˜¯å¯¹RDDè¿›è¡Œæ“ä½œ
    val dataDStream: ReceiverInputDStream[String] = ssc.socketTextStream("node01",9999)
    //3.æ“ä½œæ•°æ®
    val wordDStream: DStream[String] = dataDStream.flatMap(_.split(" "))
    val wordAndOneDStream: DStream[(String, Int)] = wordDStream.map((_,1))
    //4.ä½¿ç”¨çª—å£å‡½æ•°è¿›è¡ŒWordCountè®¡æ•°
    //reduceFunc: (V, V) => V,é›†åˆå‡½æ•°
    //windowDuration: Duration,çª—å£é•¿åº¦/å®½åº¦
    //slideDuration: Duration,çª—å£æ»‘åŠ¨é—´éš”
    //æ³¨æ„:windowDurationå’ŒslideDurationå¿…é¡»æ˜¯batchDurationçš„å€æ•°
    //windowDuration=slideDuration:æ•°æ®ä¸ä¼šä¸¢å¤±ä¹Ÿä¸ä¼šé‡å¤è®¡ç®—==å¼€å‘ä¸­ä¼šä½¿ç”¨
    //windowDuration>slideDuration:æ•°æ®ä¼šé‡å¤è®¡ç®—==å¼€å‘ä¸­ä¼šä½¿ç”¨
    //windowDuration<slideDuration:æ•°æ®ä¼šä¸¢å¤±
    //ä¸‹é¢çš„ä»£ç è¡¨ç¤º:
    //windowDuration=10
    //slideDuration=5
    //é‚£ä¹ˆæ‰§è¡Œç»“æžœå°±æ˜¯æ¯éš”5sè®¡ç®—æœ€è¿‘10sçš„æ•°æ®
    //æ¯”å¦‚å¼€å‘ä¸­è®©ä½ ç»Ÿè®¡æœ€è¿‘1å°æ—¶çš„æ•°æ®,æ¯éš”1åˆ†é’Ÿè®¡ç®—ä¸€æ¬¡,é‚£ä¹ˆå‚æ•°è¯¥å¦‚ä½•è®¾ç½®?
    //windowDuration=Minutes(60)
    //slideDuration=Minutes(1)
    val wordAndCount: DStream[(String, Int)] = wordAndOneDStream.reduceByKeyAndWindow((a:Int,b:Int)=>a+b,Seconds(10),Seconds(5))
    wordAndCount.print()
    ssc.start()//å¼€å¯
    ssc.awaitTermination()//ç­‰å¾…ä¼˜é›…åœæ­¢
  }
}
```

## äº”ã€Structured Streaming

åœ¨ 2.0 ä¹‹å‰ï¼ŒSpark Streaming ä½œä¸ºæ ¸å¿ƒ API çš„æ‰©å±•ï¼Œé’ˆå¯¹å®žæ—¶æ•°æ®æµï¼Œæä¾›äº†ä¸€å¥—å¯æ‰©å±•ã€é«˜åžåã€å¯å®¹é”™çš„æµå¼è®¡ç®—æ¨¡åž‹ã€‚Spark Streaming ä¼šæŽ¥æ”¶å®žæ—¶æ•°æ®æºçš„æ•°æ®ï¼Œå¹¶åˆ‡åˆ†æˆå¾ˆå¤šå°çš„ batchesï¼Œç„¶åŽè¢« Spark Engine æ‰§è¡Œï¼Œäº§å‡ºåŒæ ·ç”±å¾ˆå¤šå°çš„ batchs ç»„æˆçš„ç»“æžœæµã€‚æœ¬è´¨ä¸Šï¼Œè¿™æ˜¯ä¸€ç§ micro-batchï¼ˆå¾®æ‰¹å¤„ç†ï¼‰çš„æ–¹å¼å¤„ç†ï¼Œç”¨æ‰¹çš„æ€æƒ³åŽ»å¤„ç†æµæ•°æ®.è¿™ç§è®¾è®¡è®©**Spark Streaming é¢å¯¹å¤æ‚çš„æµå¼å¤„ç†åœºæ™¯æ—¶æ‰è¥Ÿè§è‚˜**ã€‚

spark streaming è¿™ç§æž„å»ºåœ¨å¾®æ‰¹å¤„ç†ä¸Šçš„æµè®¡ç®—å¼•æ“Žï¼Œæ¯”è¾ƒçªå‡ºçš„é—®é¢˜å°±æ˜¯å¤„ç†å»¶æ—¶è¾ƒé«˜ï¼ˆæ— æ³•ä¼˜åŒ–åˆ°ç§’ä»¥ä¸‹çš„æ•°é‡çº§ï¼‰ï¼Œä»¥åŠæ— æ³•æ”¯æŒåŸºäºŽ event_time çš„æ—¶é—´çª—å£åšèšåˆé€»è¾‘ã€‚

spark åœ¨ 2.0 ç‰ˆæœ¬ä¸­å‘å¸ƒäº†æ–°çš„æµè®¡ç®—çš„ APIï¼ŒStructured Streaming/ç»“æž„åŒ–æµã€‚

**Structured Streaming æ˜¯ä¸€ä¸ªåŸºäºŽ Spark SQL å¼•æ“Žçš„å¯æ‰©å±•ã€å®¹é”™çš„æµå¤„ç†å¼•æ“Ž**ã€‚ç»Ÿä¸€äº†æµã€æ‰¹çš„ç¼–ç¨‹æ¨¡åž‹ï¼Œä½ å¯ä»¥ä½¿ç”¨é™æ€æ•°æ®æ‰¹å¤„ç†ä¸€æ ·çš„æ–¹å¼æ¥ç¼–å†™æµå¼è®¡ç®—æ“ä½œã€‚å¹¶ä¸”æ”¯æŒåŸºäºŽ event_time çš„æ—¶é—´çª—å£çš„å¤„ç†é€»è¾‘ã€‚

éšç€æ•°æ®ä¸æ–­åœ°åˆ°è¾¾ï¼ŒSpark å¼•æ“Žä¼šä»¥ä¸€ç§å¢žé‡çš„æ–¹å¼æ¥æ‰§è¡Œè¿™äº›æ“ä½œï¼Œå¹¶ä¸”æŒç»­æ›´æ–°ç»“ç®—ç»“æžœã€‚å¯ä»¥ä½¿ç”¨ Scalaã€Javaã€Python æˆ– R ä¸­çš„ DataSetï¼DataFrame API æ¥è¡¨ç¤ºæµèšåˆã€äº‹ä»¶æ—¶é—´çª—å£ã€æµåˆ°æ‰¹è¿žæŽ¥ç­‰ã€‚æ­¤å¤–ï¼ŒStructured Streaming ä¼šé€šè¿‡ checkpoint å’Œé¢„å†™æ—¥å¿—ç­‰æœºåˆ¶æ¥å®žçŽ° Exactly-Once è¯­ä¹‰ã€‚

ç®€å•æ¥è¯´ï¼Œå¯¹äºŽå¼€å‘äººå‘˜æ¥è¯´ï¼Œæ ¹æœ¬ä¸ç”¨åŽ»è€ƒè™‘æ˜¯æµå¼è®¡ç®—ï¼Œè¿˜æ˜¯æ‰¹å¤„ç†ï¼Œåªè¦ä½¿ç”¨åŒæ ·çš„æ–¹å¼æ¥ç¼–å†™è®¡ç®—æ“ä½œå³å¯ï¼ŒStructured Streaming æä¾›äº†å¿«é€Ÿã€å¯æ‰©å±•ã€å®¹é”™ã€ç«¯åˆ°ç«¯çš„ä¸€æ¬¡æ€§æµå¤„ç†ï¼Œè€Œç”¨æˆ·æ— éœ€è€ƒè™‘æ›´å¤šç»†èŠ‚ã€‚

é»˜è®¤æƒ…å†µä¸‹ï¼Œç»“æž„åŒ–æµå¼æŸ¥è¯¢ä½¿ç”¨å¾®æ‰¹å¤„ç†å¼•æ“Žè¿›è¡Œå¤„ç†ï¼Œè¯¥å¼•æ“Žå°†æ•°æ®æµä½œä¸ºä¸€ç³»åˆ—å°æ‰¹å¤„ç†ä½œä¸šè¿›è¡Œå¤„ç†ï¼Œä»Žè€Œå®žçŽ°ç«¯åˆ°ç«¯çš„å»¶è¿Ÿï¼Œæœ€çŸ­å¯è¾¾ 100 æ¯«ç§’ï¼Œå¹¶ä¸”å®Œå…¨å¯ä»¥ä¿è¯ä¸€æ¬¡å®¹é”™ã€‚**è‡ª Spark 2.3 ä»¥æ¥ï¼Œå¼•å…¥äº†ä¸€ç§æ–°çš„ä½Žå»¶è¿Ÿå¤„ç†æ¨¡å¼ï¼Œç§°ä¸ºè¿žç»­å¤„ç†ï¼Œå®ƒå¯ä»¥åœ¨è‡³å°‘ä¸€æ¬¡ä¿è¯çš„æƒ…å†µä¸‹å®žçŽ°ä½Žè‡³ 1 æ¯«ç§’çš„ç«¯åˆ°ç«¯å»¶è¿Ÿã€‚ä¹Ÿå°±æ˜¯ç±»ä¼¼äºŽ Flink é‚£æ ·çš„å®žæ—¶æµï¼Œè€Œä¸æ˜¯å°æ‰¹é‡å¤„ç†**ã€‚å®žé™…å¼€å‘å¯ä»¥æ ¹æ®åº”ç”¨ç¨‹åºè¦æ±‚é€‰æ‹©å¤„ç†æ¨¡å¼ï¼Œä½†æ˜¯è¿žç»­å¤„ç†åœ¨ä½¿ç”¨çš„æ—¶å€™ä»ç„¶æœ‰å¾ˆå¤šé™åˆ¶ï¼Œç›®å‰å¤§éƒ¨åˆ†æƒ…å†µè¿˜æ˜¯åº”è¯¥é‡‡ç”¨å°æ‰¹é‡æ¨¡å¼ã€‚

### 1. API

- **Spark Streaming æ—¶ä»£** -DStream-RDD

  Spark Streaming é‡‡ç”¨çš„æ•°æ®æŠ½è±¡æ˜¯ DStreamï¼Œè€Œæœ¬è´¨ä¸Šå°±æ˜¯æ—¶é—´ä¸Šè¿žç»­çš„ RDDï¼Œå¯¹æ•°æ®æµçš„æ“ä½œå°±æ˜¯é’ˆå¯¹ RDD çš„æ“ä½œã€‚

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClXG0YdOubor4y64xgeJ79VH6aCx59woxicaic6CPBhCQQzQx7n8D2kDeA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

- **Structured Streaming æ—¶ä»£ - DataSet/DataFrame -RDD**

  Structured Streaming æ˜¯ Spark2.0 æ–°å¢žçš„å¯æ‰©å±•å’Œé«˜å®¹é”™æ€§çš„å®žæ—¶è®¡ç®—æ¡†æž¶ï¼Œå®ƒæž„å»ºäºŽ Spark SQL å¼•æ“Žï¼ŒæŠŠæµå¼è®¡ç®—ä¹Ÿç»Ÿä¸€åˆ° DataFrame/Dataset é‡ŒåŽ»äº†ã€‚

  Structured Streaming ç›¸æ¯”äºŽ Spark Streaming çš„è¿›æ­¥å°±ç±»ä¼¼äºŽ Dataset ç›¸æ¯”äºŽ RDD çš„è¿›æ­¥ã€‚

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClMvB1icXowjWxB6H43iaxkLslSw7bT7cW8Z9mDo7GRIeuibv0l5XsMBysA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

### 2. æ ¸å¿ƒæ€æƒ³

Structured Streaming æœ€æ ¸å¿ƒçš„æ€æƒ³å°±æ˜¯å°†å®žæ—¶åˆ°è¾¾çš„æ•°æ®çœ‹ä½œæ˜¯ä¸€ä¸ªä¸æ–­è¿½åŠ çš„ unbound table æ— ç•Œè¡¨ï¼Œåˆ°è¾¾æµçš„æ¯ä¸ªæ•°æ®é¡¹(RDD)å°±åƒæ˜¯è¡¨ä¸­çš„ä¸€ä¸ªæ–°è¡Œè¢«é™„åŠ åˆ°æ— è¾¹ç•Œçš„è¡¨ä¸­.è¿™æ ·ç”¨æˆ·å°±å¯ä»¥ç”¨é™æ€ç»“æž„åŒ–æ•°æ®çš„æ‰¹å¤„ç†æŸ¥è¯¢æ–¹å¼è¿›è¡Œæµè®¡ç®—ï¼Œå¦‚å¯ä»¥ä½¿ç”¨ SQL å¯¹åˆ°æ¥çš„æ¯ä¸€è¡Œæ•°æ®è¿›è¡Œå®žæ—¶æŸ¥è¯¢å¤„ç†ã€‚

### 3. åº”ç”¨åœºæ™¯

Structured Streaming å°†æ•°æ®æºæ˜ å°„ä¸ºç±»ä¼¼äºŽå…³ç³»æ•°æ®åº“ä¸­çš„è¡¨ï¼Œç„¶åŽå°†ç»è¿‡è®¡ç®—å¾—åˆ°çš„ç»“æžœæ˜ å°„ä¸ºå¦ä¸€å¼ è¡¨ï¼Œå®Œå…¨ä»¥ç»“æž„åŒ–çš„æ–¹å¼åŽ»æ“ä½œæµå¼æ•°æ®ï¼Œ**è¿™ç§ç¼–ç¨‹æ¨¡åž‹éžå¸¸æœ‰åˆ©äºŽå¤„ç†åˆ†æžç»“æž„åŒ–çš„å®žæ—¶æ•°æ®**ï¼›

### 4. Structured Streaming å®žæˆ˜

#### 1) è¯»å– Socket æ•°æ®

```
import org.apache.spark.SparkContext
import org.apache.spark.sql.streaming.Trigger
import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}

object WordCount {
  def main(args: Array[String]): Unit = {
    //1.åˆ›å»ºSparkSession,å› ä¸ºStructuredStreamingçš„æ•°æ®æ¨¡åž‹ä¹Ÿæ˜¯DataFrame/DataSet
    val spark: SparkSession = SparkSession.builder().master("local[*]").appName("SparkSQL").getOrCreate()
    val sc: SparkContext = spark.sparkContext
    sc.setLogLevel("WARN")
    //2.æŽ¥æ”¶æ•°æ®
    val dataDF: DataFrame = spark.readStream
      .option("host", "node01")
      .option("port", 9999)
      .format("socket")
      .load()
    //3.å¤„ç†æ•°æ®
    import spark.implicits._
    val dataDS: Dataset[String] = dataDF.as[String]
    val wordDS: Dataset[String] = dataDS.flatMap(_.split(" "))
    val result: Dataset[Row] = wordDS.groupBy("value").count().sort($"count".desc)
    //result.show()
    //Queries with streaming sources must be executed with writeStream.start();
    result.writeStream
      .format("console")//å¾€æŽ§åˆ¶å°å†™
      .outputMode("complete")//æ¯æ¬¡å°†æ‰€æœ‰çš„æ•°æ®å†™å‡º
      .trigger(Trigger.ProcessingTime(0))//è§¦å‘æ—¶é—´é—´éš”,0è¡¨ç¤ºå°½å¯èƒ½çš„å¿«
      //.option("checkpointLocation","./ckp")//è®¾ç½®checkpointç›®å½•,socketä¸æ”¯æŒæ•°æ®æ¢å¤,æ‰€ä»¥ç¬¬äºŒæ¬¡å¯åŠ¨ä¼šæŠ¥é”™,éœ€è¦æ³¨æŽ‰
      .start()//å¼€å¯
      .awaitTermination()//ç­‰å¾…åœæ­¢
  }
}
```

#### 2) è¯»å–ç›®å½•ä¸‹æ–‡æœ¬æ•°æ®

```
import org.apache.spark.SparkContext
import org.apache.spark.sql.streaming.Trigger
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.{DataFrame, Dataset, Row, SparkSession}
/**
  * {"name":"json","age":23,"hobby":"running"}
  * {"name":"charles","age":32,"hobby":"basketball"}
  * {"name":"tom","age":28,"hobby":"football"}
  * {"name":"lili","age":24,"hobby":"running"}
  * {"name":"bob","age":20,"hobby":"swimming"}
  * ç»Ÿè®¡å¹´é¾„å°äºŽ25å²çš„äººç¾¤çš„çˆ±å¥½æŽ’è¡Œæ¦œ
  */
object WordCount2 {
  def main(args: Array[String]): Unit = {
    //1.åˆ›å»ºSparkSession,å› ä¸ºStructuredStreamingçš„æ•°æ®æ¨¡åž‹ä¹Ÿæ˜¯DataFrame/DataSet
    val spark: SparkSession = SparkSession.builder().master("local[*]").appName("SparkSQL").getOrCreate()
    val sc: SparkContext = spark.sparkContext
    sc.setLogLevel("WARN")
    val Schema: StructType = new StructType()
      .add("name","string")
      .add("age","integer")
      .add("hobby","string")
    //2.æŽ¥æ”¶æ•°æ®
    import spark.implicits._
    // Schema must be specified when creating a streaming source DataFrame.
    val dataDF: DataFrame = spark.readStream.schema(Schema).json("D:\\data\\spark\\data")
    //3.å¤„ç†æ•°æ®
    val result: Dataset[Row] = dataDF.filter($"age" < 25).groupBy("hobby").count().sort($"count".desc)
    //4.è¾“å‡ºç»“æžœ
    result.writeStream
      .format("console")
      .outputMode("complete")
      .trigger(Trigger.ProcessingTime(0))
      .start()
      .awaitTermination()
  }
}
```

#### 3) è®¡ç®—æ“ä½œ

**èŽ·å¾—åˆ° Source ä¹‹åŽçš„åŸºæœ¬æ•°æ®å¤„ç†æ–¹å¼å’Œä¹‹å‰å­¦ä¹ çš„ DataFrameã€DataSet ä¸€è‡´ï¼Œä¸å†èµ˜è¿°**ã€‚

**å®˜ç½‘ç¤ºä¾‹ä»£ç **ï¼š

```
case class DeviceData(device: String, deviceType: String, signal: Double, time: DateTime)
val df: DataFrame = ... // streaming DataFrame with IOT device data with schema { device: string, deviceType: string, signal: double, time: string }
val ds: Dataset[DeviceData] = df.as[DeviceData]    // streaming Dataset with IOT device data
// Select the devices which have signal more than 10
df.select("device").where("signal > 10")      // using untyped APIs
ds.filter(_.signal > 10).map(_.device)         // using typed APIs
// Running count of the number of updates for each device type
df.groupBy("deviceType").count()                 // using untyped API
// Running average signal for each device type
import org.apache.spark.sql.expressions.scalalang.typed
ds.groupByKey(_.deviceType).agg(typed.avg(_.signal))    // using typed API
```

#### 4) è¾“å‡º

è®¡ç®—ç»“æžœå¯ä»¥é€‰æ‹©è¾“å‡ºåˆ°å¤šç§è®¾å¤‡å¹¶è¿›è¡Œå¦‚ä¸‹è®¾å®šï¼š

1. output modeï¼šä»¥å“ªç§æ–¹å¼å°† result table çš„æ•°æ®å†™å…¥ sink,å³æ˜¯å…¨éƒ¨è¾“å‡º complete è¿˜æ˜¯åªè¾“å‡ºæ–°å¢žæ•°æ®ï¼›
2. format/output sink çš„ä¸€äº›ç»†èŠ‚ï¼šæ•°æ®æ ¼å¼ã€ä½ç½®ç­‰ã€‚å¦‚ consoleï¼›
3. query nameï¼šæŒ‡å®šæŸ¥è¯¢çš„æ ‡è¯†ã€‚ç±»ä¼¼ tempview çš„åå­—ï¼›
4. trigger intervalï¼šè§¦å‘é—´éš”ï¼Œå¦‚æžœä¸æŒ‡å®šï¼Œé»˜è®¤ä¼šå°½å¯èƒ½å¿«é€Ÿåœ°å¤„ç†æ•°æ®ï¼›
5. checkpointLocationï¼šä¸€èˆ¬æ˜¯ hdfs ä¸Šçš„ç›®å½•ã€‚æ³¨æ„ï¼šSocket ä¸æ”¯æŒæ•°æ®æ¢å¤ï¼Œå¦‚æžœè®¾ç½®äº†ï¼Œç¬¬äºŒæ¬¡å¯åŠ¨ä¼šæŠ¥é”™ï¼ŒKafka æ”¯æŒã€‚

**output mode**ï¼š

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BCl0NMmDvj0fA6nmdxQVHhQuCvcU5P4t76Sk8NuoyFjYJyyWOkJ1VTr3Q/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

æ¯å½“ç»“æžœè¡¨æ›´æ–°æ—¶ï¼Œæˆ‘ä»¬éƒ½å¸Œæœ›å°†æ›´æ”¹åŽçš„ç»“æžœè¡Œå†™å…¥å¤–éƒ¨æŽ¥æ”¶å™¨ã€‚

è¿™é‡Œæœ‰ä¸‰ç§è¾“å‡ºæ¨¡åž‹:

1. Append modeï¼šé»˜è®¤æ¨¡å¼ï¼Œæ–°å¢žçš„è¡Œæ‰è¾“å‡ºï¼Œæ¯æ¬¡æ›´æ–°ç»“æžœé›†æ—¶ï¼Œåªå°†æ–°æ·»åŠ åˆ°ç»“æžœé›†çš„ç»“æžœè¡Œè¾“å‡ºåˆ°æŽ¥æ”¶å™¨ã€‚ä»…æ”¯æŒé‚£äº›æ·»åŠ åˆ°ç»“æžœè¡¨ä¸­çš„è¡Œæ°¸è¿œä¸ä¼šæ›´æ”¹çš„æŸ¥è¯¢ã€‚å› æ­¤ï¼Œæ­¤æ¨¡å¼ä¿è¯æ¯è¡Œä»…è¾“å‡ºä¸€æ¬¡ã€‚ä¾‹å¦‚ï¼Œä»…æŸ¥è¯¢ selectï¼Œwhereï¼Œmapï¼ŒflatMapï¼Œfilterï¼Œjoin ç­‰ä¼šæ”¯æŒè¿½åŠ æ¨¡å¼ã€‚**ä¸æ”¯æŒèšåˆ**
2. Complete modeï¼šæ‰€æœ‰å†…å®¹éƒ½è¾“å‡ºï¼Œæ¯æ¬¡è§¦å‘åŽï¼Œæ•´ä¸ªç»“æžœè¡¨å°†è¾“å‡ºåˆ°æŽ¥æ”¶å™¨ã€‚èšåˆæŸ¥è¯¢æ”¯æŒæ­¤åŠŸèƒ½ã€‚**ä»…é€‚ç”¨äºŽåŒ…å«èšåˆæ“ä½œçš„æŸ¥è¯¢**ã€‚
3. Update modeï¼šæ›´æ–°çš„è¡Œæ‰è¾“å‡ºï¼Œæ¯æ¬¡æ›´æ–°ç»“æžœé›†æ—¶ï¼Œä»…å°†è¢«æ›´æ–°çš„ç»“æžœè¡Œè¾“å‡ºåˆ°æŽ¥æ”¶å™¨(è‡ª Spark 2.1.1 èµ·å¯ç”¨)ï¼Œ**ä¸æ”¯æŒæŽ’åº**

**output sink**ï¼š

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BCl4icicknMqPXm7NrB1Ziaxq3NOGdVwzk1pX5purQUWXXEl9Yr86vZY8m4A/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

- è¯´æ˜Žï¼š

**File sink**ï¼šè¾“å‡ºå­˜å‚¨åˆ°ä¸€ä¸ªç›®å½•ä¸­ã€‚æ”¯æŒ parquet æ–‡ä»¶ï¼Œä»¥åŠ append æ¨¡å¼ã€‚

```
writeStream
    .format("parquet")        // can be "orc", "json", "csv", etc.
    .option("path", "path/to/destination/dir")
    .start()
```

**Kafka sink**ï¼šå°†è¾“å‡ºå­˜å‚¨åˆ° Kafka ä¸­çš„ä¸€ä¸ªæˆ–å¤šä¸ª topics ä¸­ã€‚

```
writeStream
    .format("kafka")
    .option("kafka.bootstrap.servers", "host1:port1,host2:port2")
    .option("topic", "updates")
    .start()
```

**Foreach sink**ï¼šå¯¹è¾“å‡ºä¸­çš„è®°å½•è¿è¡Œä»»æ„è®¡ç®—

```
writeStream
    .foreach(...)
    .start()
```

**Console sink**ï¼šå°†è¾“å‡ºæ‰“å°åˆ°æŽ§åˆ¶å°

```
writeStream
    .format("console")
    .start()
```

## å…­ã€Spark çš„ä¸¤ç§æ ¸å¿ƒ Shuffle

åœ¨ MapReduce æ¡†æž¶ä¸­ï¼Œ Shuffle é˜¶æ®µæ˜¯è¿žæŽ¥ Map ä¸Ž Reduce ä¹‹é—´çš„æ¡¥æ¢ï¼Œ Map é˜¶æ®µé€šè¿‡ Shuffle è¿‡ç¨‹å°†æ•°æ®è¾“å‡ºåˆ° Reduce é˜¶æ®µä¸­ã€‚**ç”±äºŽ Shuffle æ¶‰åŠç£ç›˜çš„è¯»å†™å’Œç½‘ç»œ I/Oï¼Œå› æ­¤ Shuffle æ€§èƒ½çš„é«˜ä½Žç›´æŽ¥å½±å“æ•´ä¸ªç¨‹åºçš„æ€§èƒ½**ã€‚Spark ä¹Ÿæœ‰ Map é˜¶æ®µå’Œ Reduce é˜¶æ®µï¼Œå› æ­¤ä¹Ÿä¼šå‡ºçŽ° Shuffle ã€‚

### Spark Shuffle

Spark Shuffle åˆ†ä¸ºä¸¤ç§ï¼šä¸€ç§æ˜¯åŸºäºŽ Hash çš„ Shuffleï¼›å¦ä¸€ç§æ˜¯åŸºäºŽ Sort çš„ Shuffleã€‚å…ˆä»‹ç»ä¸‹å®ƒä»¬çš„å‘å±•åŽ†ç¨‹ï¼Œæœ‰åŠ©äºŽæˆ‘ä»¬æ›´å¥½çš„ç†è§£ Shuffleï¼š

åœ¨ Spark 1.1 ä¹‹å‰ï¼Œ Spark ä¸­åªå®žçŽ°äº†ä¸€ç§ Shuffle æ–¹å¼ï¼Œå³åŸºäºŽ Hash çš„ Shuffle ã€‚åœ¨ Spark 1.1 ç‰ˆæœ¬ä¸­å¼•å…¥äº†åŸºäºŽ Sort çš„ Shuffle å®žçŽ°æ–¹å¼ï¼Œå¹¶ä¸” Spark 1.2 ç‰ˆæœ¬ä¹‹åŽï¼Œé»˜è®¤çš„å®žçŽ°æ–¹å¼ä»ŽåŸºäºŽ Hash çš„ Shuffle ä¿®æ”¹ä¸ºåŸºäºŽ Sort çš„ Shuffle å®žçŽ°æ–¹å¼ï¼Œå³ä½¿ç”¨çš„ ShuffleManager ä»Žé»˜è®¤çš„ hash ä¿®æ”¹ä¸º sortã€‚**åœ¨ Spark 2.0 ç‰ˆæœ¬ä¸­ï¼Œ Hash Shuffle æ–¹å¼å·±ç»ä¸å†ä½¿ç”¨**ã€‚

Spark ä¹‹æ‰€ä»¥ä¸€å¼€å§‹å°±æä¾›åŸºäºŽ Hash çš„ Shuffle å®žçŽ°æœºåˆ¶ï¼Œå…¶ä¸»è¦ç›®çš„ä¹‹ä¸€å°±æ˜¯ä¸ºäº†é¿å…ä¸éœ€è¦çš„æŽ’åºï¼Œå¤§å®¶æƒ³ä¸‹ Hadoop ä¸­çš„ MapReduceï¼Œæ˜¯å°† sort ä½œä¸ºå›ºå®šæ­¥éª¤ï¼Œæœ‰è®¸å¤šå¹¶ä¸éœ€è¦æŽ’åºçš„ä»»åŠ¡ï¼ŒMapReduce ä¹Ÿä¼šå¯¹å…¶è¿›è¡ŒæŽ’åºï¼Œé€ æˆäº†è®¸å¤šä¸å¿…è¦çš„å¼€é”€ã€‚

åœ¨åŸºäºŽ Hash çš„ Shuffle å®žçŽ°æ–¹å¼ä¸­ï¼Œæ¯ä¸ª Mapper é˜¶æ®µçš„ Task ä¼šä¸ºæ¯ä¸ª Reduce é˜¶æ®µçš„ Task ç”Ÿæˆä¸€ä¸ªæ–‡ä»¶ï¼Œé€šå¸¸ä¼šäº§ç”Ÿå¤§é‡çš„æ–‡ä»¶ï¼ˆå³å¯¹åº”ä¸º M*R ä¸ªä¸­é—´æ–‡ä»¶ï¼Œå…¶ä¸­ï¼Œ M è¡¨ç¤º Mapper é˜¶æ®µçš„ Task ä¸ªæ•°ï¼Œ R è¡¨ç¤º Reduce é˜¶æ®µçš„ Task ä¸ªæ•°ï¼‰ ä¼´éšå¤§é‡çš„éšæœºç£ç›˜ I/O æ“ä½œä¸Žå¤§é‡çš„å†…å­˜å¼€é”€ã€‚

ä¸ºäº†ç¼“è§£ä¸Šè¿°é—®é¢˜ï¼Œåœ¨ Spark 0.8.1 ç‰ˆæœ¬ä¸­ä¸ºåŸºäºŽ Hash çš„ Shuffle å®žçŽ°å¼•å…¥äº† **Shuffle Consolidate æœºåˆ¶ï¼ˆå³æ–‡ä»¶åˆå¹¶æœºåˆ¶ï¼‰**ï¼Œå°† Mapper ç«¯ç”Ÿæˆçš„ä¸­é—´æ–‡ä»¶è¿›è¡Œåˆå¹¶çš„å¤„ç†æœºåˆ¶ã€‚é€šè¿‡é…ç½®å±žæ€§`spark.shuffie.consolidateFiles=true`ï¼Œå‡å°‘ä¸­é—´ç”Ÿæˆçš„æ–‡ä»¶æ•°é‡ã€‚é€šè¿‡æ–‡ä»¶åˆå¹¶ï¼Œå¯ä»¥å°†ä¸­é—´æ–‡ä»¶çš„ç”Ÿæˆæ–¹å¼ä¿®æ”¹ä¸ºæ¯ä¸ªæ‰§è¡Œå•ä½ä¸ºæ¯ä¸ª Reduce é˜¶æ®µçš„ Task ç”Ÿæˆä¸€ä¸ªæ–‡ä»¶ã€‚

> æ‰§è¡Œå•ä½å¯¹åº”ä¸ºï¼šæ¯ä¸ª Mapper ç«¯çš„ Cores æ•°ï¼æ¯ä¸ª Task åˆ†é…çš„ Cores æ•°ï¼ˆé»˜è®¤ä¸º 1) ã€‚æœ€ç»ˆå¯ä»¥å°†æ–‡ä»¶ä¸ªæ•°ä»Ž M*R ä¿®æ”¹ä¸º E*C/T*Rï¼Œå…¶ä¸­ï¼Œ E è¡¨ç¤º Executors ä¸ªæ•°ï¼Œ C è¡¨ç¤ºå¯ç”¨ Cores ä¸ªæ•°ï¼Œ T è¡¨ç¤º Task åˆ†é…çš„ Cores æ•°ã€‚

Spark1.1 ç‰ˆæœ¬å¼•å…¥äº† Sort Shuffleï¼š

åŸºäºŽ Hash çš„ Shuffle çš„å®žçŽ°æ–¹å¼ä¸­ï¼Œç”Ÿæˆçš„ä¸­é—´ç»“æžœæ–‡ä»¶çš„ä¸ªæ•°éƒ½ä¼šä¾èµ–äºŽ Reduce é˜¶æ®µçš„ Task ä¸ªæ•°ï¼Œå³ Reduce ç«¯çš„å¹¶è¡Œåº¦ï¼Œå› æ­¤æ–‡ä»¶æ•°ä»ç„¶ä¸å¯æŽ§ï¼Œæ— æ³•çœŸæ­£è§£å†³é—®é¢˜ã€‚ä¸ºäº†æ›´å¥½åœ°è§£å†³é—®é¢˜ï¼Œåœ¨ Spark1.1 ç‰ˆæœ¬å¼•å…¥äº†åŸºäºŽ Sort çš„ Shuffle å®žçŽ°æ–¹å¼ï¼Œå¹¶ä¸”åœ¨ Spark 1.2 ç‰ˆæœ¬ä¹‹åŽï¼Œé»˜è®¤çš„å®žçŽ°æ–¹å¼ä¹Ÿä»ŽåŸºäºŽ Hash çš„ Shuffleï¼Œä¿®æ”¹ä¸ºåŸºäºŽ Sort çš„ Shuffle å®žçŽ°æ–¹å¼ï¼Œå³ä½¿ç”¨çš„ ShuffleManager ä»Žé»˜è®¤çš„ hash ä¿®æ”¹ä¸º sortã€‚

åœ¨åŸºäºŽ Sort çš„ Shuffle ä¸­ï¼Œæ¯ä¸ª Mapper é˜¶æ®µçš„ Task ä¸ä¼šä¸ºæ¯ Reduce é˜¶æ®µçš„ Task ç”Ÿæˆä¸€ä¸ªå•ç‹¬çš„æ–‡ä»¶ï¼Œè€Œæ˜¯å…¨éƒ¨å†™åˆ°ä¸€ä¸ªæ•°æ®ï¼ˆDataï¼‰æ–‡ä»¶ä¸­ï¼ŒåŒæ—¶ç”Ÿæˆä¸€ä¸ªç´¢å¼•ï¼ˆIndexï¼‰æ–‡ä»¶ï¼Œ Reduce é˜¶æ®µçš„å„ä¸ª Task å¯ä»¥é€šè¿‡è¯¥ç´¢å¼•æ–‡ä»¶èŽ·å–ç›¸å…³çš„æ•°æ®ã€‚é¿å…äº§ç”Ÿå¤§é‡æ–‡ä»¶çš„ç›´æŽ¥æ”¶ç›Šå°±æ˜¯é™ä½Žéšæœºç£ç›˜ I/0 ä¸Žå†…å­˜çš„å¼€é”€ã€‚æœ€ç»ˆç”Ÿæˆçš„æ–‡ä»¶ä¸ªæ•°å‡å°‘åˆ° 2*M ï¼Œå…¶ä¸­ M è¡¨ç¤º Mapper é˜¶æ®µçš„ Task ä¸ªæ•°ï¼Œæ¯ä¸ª Mapper é˜¶æ®µçš„ Task åˆ†åˆ«ç”Ÿæˆä¸¤ä¸ªæ–‡ä»¶ï¼ˆ1 ä¸ªæ•°æ®æ–‡ä»¶ã€ 1 ä¸ªç´¢å¼•æ–‡ä»¶ï¼‰ï¼Œæœ€ç»ˆçš„æ–‡ä»¶ä¸ªæ•°ä¸º M ä¸ªæ•°æ®æ–‡ä»¶ä¸Ž M ä¸ªç´¢å¼•æ–‡ä»¶ã€‚å› æ­¤ï¼Œæœ€ç»ˆæ–‡ä»¶ä¸ªæ•°æ˜¯ 2*M ä¸ªã€‚

ä»Ž Spark 1.4 ç‰ˆæœ¬å¼€å§‹ï¼Œåœ¨ Shuffle è¿‡ç¨‹ä¸­ä¹Ÿå¼•å…¥äº†åŸºäºŽ Tungsten-Sort çš„ Shuffie å®žçŽ°æ–¹å¼ï¼Œé€š Tungsten é¡¹ç›®æ‰€åšçš„ä¼˜åŒ–ï¼Œå¯ä»¥æžå¤§æé«˜ Spark åœ¨æ•°æ®å¤„ç†ä¸Šçš„æ€§èƒ½ã€‚(Tungsten ç¿»è¯‘ä¸ºä¸­æ–‡æ˜¯é’¨ä¸)

> æ³¨ï¼šåœ¨ä¸€äº›ç‰¹å®šçš„åº”ç”¨åœºæ™¯ä¸‹ï¼Œé‡‡ç”¨åŸºäºŽ Hash å®žçŽ° Shuffle æœºåˆ¶çš„æ€§èƒ½ä¼šè¶…è¿‡åŸºäºŽ Sort çš„ Shuffle å®žçŽ°æœºåˆ¶ã€‚

ä¸€å¼ å›¾äº†è§£ä¸‹ Spark Shuffle çš„è¿­ä»£åŽ†å²ï¼š

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClAezPeqp0pPl4dk1xh9Q9y2pzkjVeMhVSfq6ua8Qn4aMaBeXJnzFzqA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)Spark Shuffle è¿­ä»£åŽ†å²

**ä¸ºä»€ä¹ˆ Spark æœ€ç»ˆè¿˜æ˜¯æ”¾å¼ƒäº† HashShuffle ï¼Œä½¿ç”¨äº† Sorted-Based Shuffleï¼Ÿ**

æˆ‘ä»¬å¯ä»¥ä»Ž Spark æœ€æ ¹æœ¬è¦ä¼˜åŒ–å’Œè¿«åˆ‡è¦è§£å†³çš„é—®é¢˜ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼Œä½¿ç”¨ HashShuffle çš„ Spark åœ¨ Shuffle æ—¶äº§ç”Ÿå¤§é‡çš„æ–‡ä»¶ã€‚å½“æ•°æ®é‡è¶Šæ¥è¶Šå¤šæ—¶ï¼Œäº§ç”Ÿçš„æ–‡ä»¶é‡æ˜¯ä¸å¯æŽ§çš„ï¼Œè¿™ä¸¥é‡åˆ¶çº¦äº† Spark çš„æ€§èƒ½åŠæ‰©å±•èƒ½åŠ›ï¼Œæ‰€ä»¥ Spark å¿…é¡»è¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå‡å°‘ Mapper ç«¯ ShuffleWriter äº§ç”Ÿçš„æ–‡ä»¶æ•°é‡ï¼Œè¿™æ ·ä¾¿å¯ä»¥è®© Spark ä»Žå‡ ç™¾å°é›†ç¾¤çš„è§„æ¨¡çž¬é—´å˜æˆå¯ä»¥æ”¯æŒå‡ åƒå°ï¼Œç”šè‡³å‡ ä¸‡å°é›†ç¾¤çš„è§„æ¨¡ã€‚

ä½†ä½¿ç”¨ Sorted-Based Shuffle å°±å®Œç¾Žäº†å—ï¼Œç­”æ¡ˆæ˜¯å¦å®šçš„ï¼ŒSorted-Based Shuffle ä¹Ÿæœ‰ç¼ºç‚¹ï¼Œå…¶ç¼ºç‚¹åè€Œæ˜¯å®ƒæŽ’åºçš„ç‰¹æ€§ï¼Œå®ƒå¼ºåˆ¶è¦æ±‚æ•°æ®åœ¨ Mapper ç«¯å¿…é¡»å…ˆè¿›è¡ŒæŽ’åºï¼Œæ‰€ä»¥å¯¼è‡´å®ƒæŽ’åºçš„é€Ÿåº¦æœ‰ç‚¹æ…¢ã€‚å¥½åœ¨å‡ºçŽ°äº† Tungsten-Sort Shuffle ï¼Œå®ƒå¯¹æŽ’åºç®—æ³•è¿›è¡Œäº†æ”¹è¿›ï¼Œä¼˜åŒ–äº†æŽ’åºçš„é€Ÿåº¦ã€‚Tungsten-Sort Shuffle å·²ç»å¹¶å…¥äº† Sorted-Based Shuffleï¼ŒSpark çš„å¼•æ“Žä¼šè‡ªåŠ¨è¯†åˆ«ç¨‹åºéœ€è¦çš„æ˜¯ Sorted-Based Shuffleï¼Œè¿˜æ˜¯ Tungsten-Sort Shuffleã€‚

**ä¸‹é¢è¯¦ç»†å‰–æžæ¯ä¸ª Shuffle çš„åº•å±‚æ‰§è¡ŒåŽŸç†ï¼š**

### ä¸€ã€Hash Shuffle è§£æž

ä»¥ä¸‹çš„è®¨è®ºéƒ½å‡è®¾æ¯ä¸ª Executor æœ‰ 1 ä¸ª cpu coreã€‚

#### 1. HashShuffleManager

shuffle write é˜¶æ®µï¼Œä¸»è¦å°±æ˜¯åœ¨ä¸€ä¸ª stage ç»“æŸè®¡ç®—ä¹‹åŽï¼Œä¸ºäº†ä¸‹ä¸€ä¸ª stage å¯ä»¥æ‰§è¡Œ shuffle ç±»çš„ç®—å­ï¼ˆæ¯”å¦‚ reduceByKeyï¼‰ï¼Œè€Œå°†æ¯ä¸ª task å¤„ç†çš„æ•°æ®æŒ‰ key è¿›è¡Œâ€œåˆ’åˆ†â€ã€‚æ‰€è°“â€œåˆ’åˆ†â€ï¼Œå°±æ˜¯**å¯¹ç›¸åŒçš„ key æ‰§è¡Œ hash ç®—æ³•**ï¼Œä»Žè€Œå°†ç›¸åŒ key éƒ½å†™å…¥åŒä¸€ä¸ªç£ç›˜æ–‡ä»¶ä¸­ï¼Œè€Œæ¯ä¸€ä¸ªç£ç›˜æ–‡ä»¶éƒ½åªå±žäºŽä¸‹æ¸¸ stage çš„ä¸€ä¸ª taskã€‚åœ¨å°†æ•°æ®å†™å…¥ç£ç›˜ä¹‹å‰ï¼Œä¼šå…ˆå°†æ•°æ®å†™å…¥å†…å­˜ç¼“å†²ä¸­ï¼Œå½“å†…å­˜ç¼“å†²å¡«æ»¡ä¹‹åŽï¼Œæ‰ä¼šæº¢å†™åˆ°ç£ç›˜æ–‡ä»¶ä¸­åŽ»ã€‚

ä¸‹ä¸€ä¸ª stage çš„ task æœ‰å¤šå°‘ä¸ªï¼Œå½“å‰ stage çš„æ¯ä¸ª task å°±è¦åˆ›å»ºå¤šå°‘ä»½ç£ç›˜æ–‡ä»¶ã€‚æ¯”å¦‚ä¸‹ä¸€ä¸ª stage æ€»å…±æœ‰ 100 ä¸ª taskï¼Œé‚£ä¹ˆå½“å‰ stage çš„æ¯ä¸ª task éƒ½è¦åˆ›å»º 100 ä»½ç£ç›˜æ–‡ä»¶ã€‚å¦‚æžœå½“å‰ stage æœ‰ 50 ä¸ª taskï¼Œæ€»å…±æœ‰ 10 ä¸ª Executorï¼Œæ¯ä¸ª Executor æ‰§è¡Œ 5 ä¸ª taskï¼Œé‚£ä¹ˆæ¯ä¸ª Executor ä¸Šæ€»å…±å°±è¦åˆ›å»º 500 ä¸ªç£ç›˜æ–‡ä»¶ï¼Œæ‰€æœ‰ Executor ä¸Šä¼šåˆ›å»º 5000 ä¸ªç£ç›˜æ–‡ä»¶ã€‚ç”±æ­¤å¯è§ï¼Œ**æœªç»ä¼˜åŒ–çš„ shuffle write æ“ä½œæ‰€äº§ç”Ÿçš„ç£ç›˜æ–‡ä»¶çš„æ•°é‡æ˜¯æžå…¶æƒŠäººçš„**ã€‚

shuffle read é˜¶æ®µï¼Œé€šå¸¸å°±æ˜¯ä¸€ä¸ª stage åˆšå¼€å§‹æ—¶è¦åšçš„äº‹æƒ…ã€‚æ­¤æ—¶è¯¥ stage çš„**æ¯ä¸€ä¸ª task å°±éœ€è¦å°†ä¸Šä¸€ä¸ª stage çš„è®¡ç®—ç»“æžœä¸­çš„æ‰€æœ‰ç›¸åŒ keyï¼Œä»Žå„ä¸ªèŠ‚ç‚¹ä¸Šé€šè¿‡ç½‘ç»œéƒ½æ‹‰å–åˆ°è‡ªå·±æ‰€åœ¨çš„èŠ‚ç‚¹ä¸Šï¼Œç„¶åŽè¿›è¡Œ key çš„èšåˆæˆ–è¿žæŽ¥ç­‰æ“ä½œ**ã€‚ç”±äºŽ shuffle write çš„è¿‡ç¨‹ä¸­ï¼Œmap task ç»™ä¸‹æ¸¸ stage çš„æ¯ä¸ª reduce task éƒ½åˆ›å»ºäº†ä¸€ä¸ªç£ç›˜æ–‡ä»¶ï¼Œå› æ­¤ shuffle read çš„è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ª reduce task åªè¦ä»Žä¸Šæ¸¸ stage çš„æ‰€æœ‰ map task æ‰€åœ¨èŠ‚ç‚¹ä¸Šï¼Œæ‹‰å–å±žäºŽè‡ªå·±çš„é‚£ä¸€ä¸ªç£ç›˜æ–‡ä»¶å³å¯ã€‚

shuffle read çš„æ‹‰å–è¿‡ç¨‹æ˜¯ä¸€è¾¹æ‹‰å–ä¸€è¾¹è¿›è¡Œèšåˆçš„ã€‚æ¯ä¸ª shuffle read task éƒ½ä¼šæœ‰ä¸€ä¸ªè‡ªå·±çš„ buffer ç¼“å†²ï¼Œæ¯æ¬¡éƒ½åªèƒ½æ‹‰å–ä¸Ž buffer ç¼“å†²ç›¸åŒå¤§å°çš„æ•°æ®ï¼Œç„¶åŽé€šè¿‡å†…å­˜ä¸­çš„ä¸€ä¸ª Map è¿›è¡Œèšåˆç­‰æ“ä½œã€‚èšåˆå®Œä¸€æ‰¹æ•°æ®åŽï¼Œå†æ‹‰å–ä¸‹ä¸€æ‰¹æ•°æ®ï¼Œå¹¶æ”¾åˆ° buffer ç¼“å†²ä¸­è¿›è¡Œèšåˆæ“ä½œã€‚ä»¥æ­¤ç±»æŽ¨ï¼Œç›´åˆ°æœ€åŽå°†æ‰€æœ‰æ•°æ®åˆ°æ‹‰å–å®Œï¼Œå¹¶å¾—åˆ°æœ€ç»ˆçš„ç»“æžœã€‚

HashShuffleManager å·¥ä½œåŽŸç†å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClsAiabF0CzMbzcAjKicbLhZ8olRfTdUBcICia9zf56a8blDdkmsaHiaSclw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)æœªä¼˜åŒ–çš„HashShuffleManagerå·¥ä½œåŽŸç†

#### 2. ä¼˜åŒ–çš„ HashShuffleManager

ä¸ºäº†ä¼˜åŒ– HashShuffleManager æˆ‘ä»¬å¯ä»¥è®¾ç½®ä¸€ä¸ªå‚æ•°ï¼š`spark.shuffle.consolidateFiles`ï¼Œè¯¥å‚æ•°é»˜è®¤å€¼ä¸º falseï¼Œå°†å…¶è®¾ç½®ä¸º true å³å¯å¼€å¯ä¼˜åŒ–æœºåˆ¶ï¼Œé€šå¸¸æ¥è¯´ï¼Œ**å¦‚æžœæˆ‘ä»¬ä½¿ç”¨ HashShuffleManagerï¼Œé‚£ä¹ˆéƒ½å»ºè®®å¼€å¯è¿™ä¸ªé€‰é¡¹**ã€‚

å¼€å¯ consolidate æœºåˆ¶ä¹‹åŽï¼Œåœ¨ shuffle write è¿‡ç¨‹ä¸­ï¼Œtask å°±ä¸æ˜¯ä¸ºä¸‹æ¸¸ stage çš„æ¯ä¸ª task åˆ›å»ºä¸€ä¸ªç£ç›˜æ–‡ä»¶äº†ï¼Œæ­¤æ—¶ä¼šå‡ºçŽ°**shuffleFileGroup**çš„æ¦‚å¿µï¼Œæ¯ä¸ª shuffleFileGroup ä¼šå¯¹åº”ä¸€æ‰¹ç£ç›˜æ–‡ä»¶ï¼Œç£ç›˜æ–‡ä»¶çš„æ•°é‡ä¸Žä¸‹æ¸¸ stage çš„ task æ•°é‡æ˜¯ç›¸åŒçš„ã€‚ä¸€ä¸ª Executor ä¸Šæœ‰å¤šå°‘ä¸ª cpu coreï¼Œå°±å¯ä»¥å¹¶è¡Œæ‰§è¡Œå¤šå°‘ä¸ª taskã€‚è€Œç¬¬ä¸€æ‰¹å¹¶è¡Œæ‰§è¡Œçš„æ¯ä¸ª task éƒ½ä¼šåˆ›å»ºä¸€ä¸ª shuffleFileGroupï¼Œå¹¶å°†æ•°æ®å†™å…¥å¯¹åº”çš„ç£ç›˜æ–‡ä»¶å†…ã€‚

å½“ Executor çš„ cpu core æ‰§è¡Œå®Œä¸€æ‰¹ taskï¼ŒæŽ¥ç€æ‰§è¡Œä¸‹ä¸€æ‰¹ task æ—¶ï¼Œä¸‹ä¸€æ‰¹ task å°±ä¼šå¤ç”¨ä¹‹å‰å·²æœ‰çš„ shuffleFileGroupï¼ŒåŒ…æ‹¬å…¶ä¸­çš„ç£ç›˜æ–‡ä»¶ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæ­¤æ—¶ task ä¼šå°†æ•°æ®å†™å…¥å·²æœ‰çš„ç£ç›˜æ–‡ä»¶ä¸­ï¼Œè€Œä¸ä¼šå†™å…¥æ–°çš„ç£ç›˜æ–‡ä»¶ä¸­ã€‚å› æ­¤ï¼Œ**consolidate æœºåˆ¶å…è®¸ä¸åŒçš„ task å¤ç”¨åŒä¸€æ‰¹ç£ç›˜æ–‡ä»¶ï¼Œè¿™æ ·å°±å¯ä»¥æœ‰æ•ˆå°†å¤šä¸ª task çš„ç£ç›˜æ–‡ä»¶è¿›è¡Œä¸€å®šç¨‹åº¦ä¸Šçš„åˆå¹¶ï¼Œä»Žè€Œå¤§å¹…åº¦å‡å°‘ç£ç›˜æ–‡ä»¶çš„æ•°é‡ï¼Œè¿›è€Œæå‡ shuffle write çš„æ€§èƒ½**ã€‚

å‡è®¾ç¬¬äºŒä¸ª stage æœ‰ 100 ä¸ª taskï¼Œç¬¬ä¸€ä¸ª stage æœ‰ 50 ä¸ª taskï¼Œæ€»å…±è¿˜æ˜¯æœ‰ 10 ä¸ª Executorï¼ˆExecutor CPU ä¸ªæ•°ä¸º 1ï¼‰ï¼Œæ¯ä¸ª Executor æ‰§è¡Œ 5 ä¸ª taskã€‚é‚£ä¹ˆåŽŸæœ¬ä½¿ç”¨æœªç»ä¼˜åŒ–çš„ HashShuffleManager æ—¶ï¼Œæ¯ä¸ª Executor ä¼šäº§ç”Ÿ 500 ä¸ªç£ç›˜æ–‡ä»¶ï¼Œæ‰€æœ‰ Executor ä¼šäº§ç”Ÿ 5000 ä¸ªç£ç›˜æ–‡ä»¶çš„ã€‚ä½†æ˜¯æ­¤æ—¶ç»è¿‡ä¼˜åŒ–ä¹‹åŽï¼Œæ¯ä¸ª Executor åˆ›å»ºçš„ç£ç›˜æ–‡ä»¶çš„æ•°é‡çš„è®¡ç®—å…¬å¼ä¸ºï¼š`cpu coreçš„æ•°é‡ * ä¸‹ä¸€ä¸ªstageçš„taskæ•°é‡`ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæ¯ä¸ª Executor æ­¤æ—¶åªä¼šåˆ›å»º 100 ä¸ªç£ç›˜æ–‡ä»¶ï¼Œæ‰€æœ‰ Executor åªä¼šåˆ›å»º 1000 ä¸ªç£ç›˜æ–‡ä»¶ã€‚

> è¿™ä¸ªåŠŸèƒ½ä¼˜ç‚¹æ˜Žæ˜¾ï¼Œä½†ä¸ºä»€ä¹ˆ Spark ä¸€ç›´æ²¡æœ‰åœ¨åŸºäºŽ Hash Shuffle çš„å®žçŽ°ä¸­å°†åŠŸèƒ½è®¾ç½®ä¸ºé»˜è®¤é€‰é¡¹å‘¢ï¼Œå®˜æ–¹ç»™å‡ºçš„è¯´æ³•æ˜¯è¿™ä¸ªåŠŸèƒ½è¿˜æ¬ ç¨³å®šã€‚

ä¼˜åŒ–åŽçš„ HashShuffleManager å·¥ä½œåŽŸç†å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClibdednS4E4OwekDtqpPvbUYSDhcbkWuIbnTnh30qQrXrdHk0mm0kw1A/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)ä¼˜åŒ–åŽçš„HashShuffleManagerå·¥ä½œåŽŸç†

##### åŸºäºŽ Hash çš„ Shuffle æœºåˆ¶çš„ä¼˜ç¼ºç‚¹

**ä¼˜ç‚¹**ï¼š

- å¯ä»¥çœç•¥ä¸å¿…è¦çš„æŽ’åºå¼€é”€ã€‚
- é¿å…äº†æŽ’åºæ‰€éœ€çš„å†…å­˜å¼€é”€ã€‚

**ç¼ºç‚¹**ï¼š

- ç”Ÿäº§çš„æ–‡ä»¶è¿‡å¤šï¼Œä¼šå¯¹æ–‡ä»¶ç³»ç»Ÿé€ æˆåŽ‹åŠ›ã€‚
- å¤§é‡å°æ–‡ä»¶çš„éšæœºè¯»å†™å¸¦æ¥ä¸€å®šçš„ç£ç›˜å¼€é”€ã€‚
- æ•°æ®å—å†™å…¥æ—¶æ‰€éœ€çš„ç¼“å­˜ç©ºé—´ä¹Ÿä¼šéšä¹‹å¢žåŠ ï¼Œå¯¹å†…å­˜é€ æˆåŽ‹åŠ›ã€‚

### äºŒã€SortShuffle è§£æž

SortShuffleManager çš„è¿è¡Œæœºåˆ¶ä¸»è¦åˆ†æˆä¸‰ç§ï¼š

1. **æ™®é€šè¿è¡Œæœºåˆ¶**ï¼›
2. **bypass è¿è¡Œæœºåˆ¶**ï¼Œå½“ shuffle read task çš„æ•°é‡å°äºŽç­‰äºŽ`spark.shuffle.sort.bypassMergeThreshold`å‚æ•°çš„å€¼æ—¶ï¼ˆé»˜è®¤ä¸º 200ï¼‰ï¼Œå°±ä¼šå¯ç”¨ bypass æœºåˆ¶ï¼›
3. **Tungsten Sort è¿è¡Œæœºåˆ¶**ï¼Œå¼€å¯æ­¤è¿è¡Œæœºåˆ¶éœ€è®¾ç½®é…ç½®é¡¹ `spark.shuffle.manager=tungsten-sort`ã€‚å¼€å¯æ­¤é¡¹é…ç½®ä¹Ÿä¸èƒ½ä¿è¯å°±ä¸€å®šé‡‡ç”¨æ­¤è¿è¡Œæœºåˆ¶ï¼ˆåŽé¢ä¼šè§£é‡Šï¼‰ã€‚

#### 1. æ™®é€šè¿è¡Œæœºåˆ¶

åœ¨è¯¥æ¨¡å¼ä¸‹ï¼Œ**æ•°æ®ä¼šå…ˆå†™å…¥ä¸€ä¸ªå†…å­˜æ•°æ®ç»“æž„ä¸­**ï¼Œæ­¤æ—¶æ ¹æ®ä¸åŒçš„ shuffle ç®—å­ï¼Œå¯èƒ½é€‰ç”¨ä¸åŒçš„æ•°æ®ç»“æž„ã€‚å¦‚æžœæ˜¯ reduceByKey è¿™ç§èšåˆç±»çš„ shuffle ç®—å­ï¼Œé‚£ä¹ˆä¼šé€‰ç”¨ Map æ•°æ®ç»“æž„ï¼Œä¸€è¾¹é€šè¿‡ Map è¿›è¡Œèšåˆï¼Œä¸€è¾¹å†™å…¥å†…å­˜ï¼›å¦‚æžœæ˜¯ join è¿™ç§æ™®é€šçš„ shuffle ç®—å­ï¼Œé‚£ä¹ˆä¼šé€‰ç”¨ Array æ•°æ®ç»“æž„ï¼Œç›´æŽ¥å†™å…¥å†…å­˜ã€‚æŽ¥ç€ï¼Œæ¯å†™ä¸€æ¡æ•°æ®è¿›å…¥å†…å­˜æ•°æ®ç»“æž„ä¹‹åŽï¼Œå°±ä¼šåˆ¤æ–­ä¸€ä¸‹ï¼Œæ˜¯å¦è¾¾åˆ°äº†æŸä¸ªä¸´ç•Œé˜ˆå€¼ã€‚å¦‚æžœè¾¾åˆ°ä¸´ç•Œé˜ˆå€¼çš„è¯ï¼Œé‚£ä¹ˆå°±ä¼šå°è¯•å°†å†…å­˜æ•°æ®ç»“æž„ä¸­çš„æ•°æ®æº¢å†™åˆ°ç£ç›˜ï¼Œç„¶åŽæ¸…ç©ºå†…å­˜æ•°æ®ç»“æž„ã€‚

åœ¨æº¢å†™åˆ°ç£ç›˜æ–‡ä»¶ä¹‹å‰ï¼Œä¼šå…ˆæ ¹æ® key å¯¹å†…å­˜æ•°æ®ç»“æž„ä¸­å·²æœ‰çš„æ•°æ®è¿›è¡ŒæŽ’åºã€‚æŽ’åºè¿‡åŽï¼Œä¼šåˆ†æ‰¹å°†æ•°æ®å†™å…¥ç£ç›˜æ–‡ä»¶ã€‚é»˜è®¤çš„ batch æ•°é‡æ˜¯ 10000 æ¡ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼ŒæŽ’åºå¥½çš„æ•°æ®ï¼Œä¼šä»¥æ¯æ‰¹ 1 ä¸‡æ¡æ•°æ®çš„å½¢å¼åˆ†æ‰¹å†™å…¥ç£ç›˜æ–‡ä»¶ã€‚å†™å…¥ç£ç›˜æ–‡ä»¶æ˜¯é€šè¿‡ Java çš„ BufferedOutputStream å®žçŽ°çš„ã€‚**BufferedOutputStream æ˜¯ Java çš„ç¼“å†²è¾“å‡ºæµï¼Œé¦–å…ˆä¼šå°†æ•°æ®ç¼“å†²åœ¨å†…å­˜ä¸­ï¼Œå½“å†…å­˜ç¼“å†²æ»¡æº¢ä¹‹åŽå†ä¸€æ¬¡å†™å…¥ç£ç›˜æ–‡ä»¶ä¸­ï¼Œè¿™æ ·å¯ä»¥å‡å°‘ç£ç›˜ IO æ¬¡æ•°ï¼Œæå‡æ€§èƒ½**ã€‚

ä¸€ä¸ª task å°†æ‰€æœ‰æ•°æ®å†™å…¥å†…å­˜æ•°æ®ç»“æž„çš„è¿‡ç¨‹ä¸­ï¼Œä¼šå‘ç”Ÿå¤šæ¬¡ç£ç›˜æº¢å†™æ“ä½œï¼Œä¹Ÿå°±ä¼šäº§ç”Ÿå¤šä¸ªä¸´æ—¶æ–‡ä»¶ã€‚æœ€åŽä¼šå°†ä¹‹å‰æ‰€æœ‰çš„ä¸´æ—¶ç£ç›˜æ–‡ä»¶éƒ½è¿›è¡Œåˆå¹¶ï¼Œè¿™å°±æ˜¯**merge è¿‡ç¨‹**ï¼Œæ­¤æ—¶ä¼šå°†ä¹‹å‰æ‰€æœ‰ä¸´æ—¶ç£ç›˜æ–‡ä»¶ä¸­çš„æ•°æ®è¯»å–å‡ºæ¥ï¼Œç„¶åŽä¾æ¬¡å†™å…¥æœ€ç»ˆçš„ç£ç›˜æ–‡ä»¶ä¹‹ä¸­ã€‚æ­¤å¤–ï¼Œç”±äºŽä¸€ä¸ª task å°±åªå¯¹åº”ä¸€ä¸ªç£ç›˜æ–‡ä»¶ï¼Œä¹Ÿå°±æ„å‘³ç€è¯¥ task ä¸ºä¸‹æ¸¸ stage çš„ task å‡†å¤‡çš„æ•°æ®éƒ½åœ¨è¿™ä¸€ä¸ªæ–‡ä»¶ä¸­ï¼Œå› æ­¤è¿˜ä¼šå•ç‹¬å†™ä¸€ä»½**ç´¢å¼•æ–‡ä»¶**ï¼Œå…¶ä¸­æ ‡è¯†äº†ä¸‹æ¸¸å„ä¸ª task çš„æ•°æ®åœ¨æ–‡ä»¶ä¸­çš„ start offset ä¸Ž end offsetã€‚

SortShuffleManager ç”±äºŽæœ‰ä¸€ä¸ªç£ç›˜æ–‡ä»¶ merge çš„è¿‡ç¨‹ï¼Œå› æ­¤å¤§å¤§å‡å°‘äº†æ–‡ä»¶æ•°é‡ã€‚æ¯”å¦‚ç¬¬ä¸€ä¸ª stage æœ‰ 50 ä¸ª taskï¼Œæ€»å…±æœ‰ 10 ä¸ª Executorï¼Œæ¯ä¸ª Executor æ‰§è¡Œ 5 ä¸ª taskï¼Œè€Œç¬¬äºŒä¸ª stage æœ‰ 100 ä¸ª taskã€‚ç”±äºŽæ¯ä¸ª task æœ€ç»ˆåªæœ‰ä¸€ä¸ªç£ç›˜æ–‡ä»¶ï¼Œå› æ­¤æ­¤æ—¶æ¯ä¸ª Executor ä¸Šåªæœ‰ 5 ä¸ªç£ç›˜æ–‡ä»¶ï¼Œæ‰€æœ‰ Executor åªæœ‰ 50 ä¸ªç£ç›˜æ–‡ä»¶ã€‚

æ™®é€šè¿è¡Œæœºåˆ¶çš„ SortShuffleManager å·¥ä½œåŽŸç†å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClxic0zHibkAEtbsnTXf9GGicz6xDeNMj2d4L7Y6KWxnrzzJN2AEucaxpFg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)æ™®é€šè¿è¡Œæœºåˆ¶çš„SortShuffleManagerå·¥ä½œåŽŸç†

#### 2. bypass è¿è¡Œæœºåˆ¶

**Reducer ç«¯ä»»åŠ¡æ•°æ¯”è¾ƒå°‘çš„æƒ…å†µä¸‹ï¼ŒåŸºäºŽ Hash Shuffle å®žçŽ°æœºåˆ¶æ˜Žæ˜¾æ¯”åŸºäºŽ Sort Shuffle å®žçŽ°æœºåˆ¶è¦å¿«ï¼Œå› æ­¤åŸºäºŽ Sort huffle å®žçŽ°æœºåˆ¶æä¾›äº†ä¸€ä¸ªå›žé€€æ–¹æ¡ˆï¼Œå°±æ˜¯ bypass è¿è¡Œæœºåˆ¶**ã€‚å¯¹äºŽ Reducer ç«¯ä»»åŠ¡æ•°å°‘äºŽé…ç½®å±žæ€§`spark.shuffle.sort.bypassMergeThreshold`è®¾ç½®çš„ä¸ªæ•°æ—¶ï¼Œä½¿ç”¨å¸¦ Hash é£Žæ ¼çš„å›žé€€è®¡åˆ’ã€‚

bypass è¿è¡Œæœºåˆ¶çš„è§¦å‘æ¡ä»¶å¦‚ä¸‹ï¼š

- shuffle map task æ•°é‡å°äºŽ`spark.shuffle.sort.bypassMergeThreshold=200`å‚æ•°çš„å€¼ã€‚
- ä¸æ˜¯èšåˆç±»çš„ shuffle ç®—å­ã€‚

æ­¤æ—¶ï¼Œæ¯ä¸ª task ä¼šä¸ºæ¯ä¸ªä¸‹æ¸¸ task éƒ½åˆ›å»ºä¸€ä¸ªä¸´æ—¶ç£ç›˜æ–‡ä»¶ï¼Œå¹¶å°†æ•°æ®æŒ‰ key è¿›è¡Œ hash ç„¶åŽæ ¹æ® key çš„ hash å€¼ï¼Œå°† key å†™å…¥å¯¹åº”çš„ç£ç›˜æ–‡ä»¶ä¹‹ä¸­ã€‚å½“ç„¶ï¼Œå†™å…¥ç£ç›˜æ–‡ä»¶æ—¶ä¹Ÿæ˜¯å…ˆå†™å…¥å†…å­˜ç¼“å†²ï¼Œç¼“å†²å†™æ»¡ä¹‹åŽå†æº¢å†™åˆ°ç£ç›˜æ–‡ä»¶çš„ã€‚æœ€åŽï¼ŒåŒæ ·ä¼šå°†æ‰€æœ‰ä¸´æ—¶ç£ç›˜æ–‡ä»¶éƒ½åˆå¹¶æˆä¸€ä¸ªç£ç›˜æ–‡ä»¶ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªå•ç‹¬çš„ç´¢å¼•æ–‡ä»¶ã€‚

è¯¥è¿‡ç¨‹çš„ç£ç›˜å†™æœºåˆ¶å…¶å®žè·Ÿæœªç»ä¼˜åŒ–çš„ HashShuffleManager æ˜¯ä¸€æ¨¡ä¸€æ ·çš„ï¼Œå› ä¸ºéƒ½è¦åˆ›å»ºæ•°é‡æƒŠäººçš„ç£ç›˜æ–‡ä»¶ï¼Œåªæ˜¯åœ¨æœ€åŽä¼šåšä¸€ä¸ªç£ç›˜æ–‡ä»¶çš„åˆå¹¶è€Œå·²ã€‚å› æ­¤å°‘é‡çš„æœ€ç»ˆç£ç›˜æ–‡ä»¶ï¼Œä¹Ÿè®©è¯¥æœºåˆ¶ç›¸å¯¹æœªç»ä¼˜åŒ–çš„ HashShuffleManager æ¥è¯´ï¼Œshuffle read çš„æ€§èƒ½ä¼šæ›´å¥½ã€‚

è€Œè¯¥æœºåˆ¶ä¸Žæ™®é€š SortShuffleManager è¿è¡Œæœºåˆ¶çš„ä¸åŒåœ¨äºŽï¼šç¬¬ä¸€ï¼Œç£ç›˜å†™æœºåˆ¶ä¸åŒï¼›ç¬¬äºŒï¼Œä¸ä¼šè¿›è¡ŒæŽ’åºã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œ**å¯ç”¨è¯¥æœºåˆ¶çš„æœ€å¤§å¥½å¤„åœ¨äºŽï¼Œshuffle write è¿‡ç¨‹ä¸­ï¼Œä¸éœ€è¦è¿›è¡Œæ•°æ®çš„æŽ’åºæ“ä½œ**ï¼Œä¹Ÿå°±èŠ‚çœæŽ‰äº†è¿™éƒ¨åˆ†çš„æ€§èƒ½å¼€é”€ã€‚

bypass è¿è¡Œæœºåˆ¶çš„ SortShuffleManager å·¥ä½œåŽŸç†å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClx87Q1KVIFYdSMNFhPJ9eJ5B5ibCvHULQV5LgiaXN9MrhZbgnSZ5gVQKA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)bypassè¿è¡Œæœºåˆ¶çš„SortShuffleManagerå·¥ä½œåŽŸç†

#### 3. Tungsten Sort Shuffle è¿è¡Œæœºåˆ¶

åŸºäºŽ Tungsten Sort çš„ Shuffle å®žçŽ°æœºåˆ¶ä¸»è¦æ˜¯å€ŸåŠ© Tungsten é¡¹ç›®æ‰€åšçš„ä¼˜åŒ–æ¥é«˜æ•ˆå¤„ç† Shuffleã€‚

Spark æä¾›äº†é…ç½®å±žæ€§ï¼Œç”¨äºŽé€‰æ‹©å…·ä½“çš„ Shuffle å®žçŽ°æœºåˆ¶ï¼Œä½†éœ€è¦è¯´æ˜Žçš„æ˜¯ï¼Œè™½ç„¶é»˜è®¤æƒ…å†µä¸‹ Spark é»˜è®¤å¼€å¯çš„æ˜¯åŸºäºŽ SortShuffle å®žçŽ°æœºåˆ¶ï¼Œä½†å®žé™…ä¸Šï¼Œå‚è€ƒ Shuffle çš„æ¡†æž¶å†…æ ¸éƒ¨åˆ†å¯çŸ¥åŸºäºŽ SortShuffle çš„å®žçŽ°æœºåˆ¶ä¸ŽåŸºäºŽ Tungsten Sort Shuffle å®žçŽ°æœºåˆ¶éƒ½æ˜¯ä½¿ç”¨ SortShuffleManagerï¼Œè€Œå†…éƒ¨ä½¿ç”¨çš„å…·ä½“çš„å®žçŽ°æœºåˆ¶ï¼Œæ˜¯é€šè¿‡æä¾›çš„ä¸¤ä¸ªæ–¹æ³•è¿›è¡Œåˆ¤æ–­çš„ï¼š

**å¯¹åº”éžåŸºäºŽ Tungsten Sort æ—¶ï¼Œé€šè¿‡ SortShuffleWriter.shouldBypassMergeSort æ–¹æ³•åˆ¤æ–­æ˜¯å¦éœ€è¦å›žé€€åˆ° Hash é£Žæ ¼çš„ Shuffle å®žçŽ°æœºåˆ¶ï¼Œå½“è¯¥æ–¹æ³•è¿”å›žçš„æ¡ä»¶ä¸æ»¡è¶³æ—¶ï¼Œåˆ™é€šè¿‡ SortShuffleManager.canUseSerializedShuffle æ–¹æ³•åˆ¤æ–­æ˜¯å¦éœ€è¦é‡‡ç”¨åŸºäºŽ Tungsten Sort Shuffle å®žçŽ°æœºåˆ¶ï¼Œè€Œå½“è¿™ä¸¤ä¸ªæ–¹æ³•è¿”å›žéƒ½ä¸º falseï¼Œå³éƒ½ä¸æ»¡è¶³å¯¹åº”çš„æ¡ä»¶æ—¶ï¼Œä¼šè‡ªåŠ¨é‡‡ç”¨æ™®é€šè¿è¡Œæœºåˆ¶ã€‚**

å› æ­¤ï¼Œå½“è®¾ç½®äº† `spark.shuffle.manager=tungsten-sort` æ—¶ï¼Œä¹Ÿä¸èƒ½ä¿è¯å°±ä¸€å®šé‡‡ç”¨åŸºäºŽ Tungsten Sort çš„ Shuffle å®žçŽ°æœºåˆ¶ã€‚

è¦å®žçŽ° Tungsten Sort Shuffle æœºåˆ¶éœ€è¦æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼š

1. Shuffle ä¾èµ–ä¸­ä¸å¸¦èšåˆæ“ä½œæˆ–æ²¡æœ‰å¯¹è¾“å‡ºè¿›è¡ŒæŽ’åºçš„è¦æ±‚ã€‚
2. Shuffle çš„åºåˆ—åŒ–å™¨æ”¯æŒåºåˆ—åŒ–å€¼çš„é‡å®šä½ï¼ˆå½“å‰ä»…æ”¯æŒ KryoSerializer Spark SQL æ¡†æž¶è‡ªå®šä¹‰çš„åºåˆ—åŒ–å™¨ï¼‰ã€‚
3. Shuffle è¿‡ç¨‹ä¸­çš„è¾“å‡ºåˆ†åŒºä¸ªæ•°å°‘äºŽ 16777216 ä¸ªã€‚

å®žé™…ä¸Šï¼Œä½¿ç”¨è¿‡ç¨‹ä¸­è¿˜æœ‰å…¶ä»–ä¸€äº›é™åˆ¶ï¼Œå¦‚å¼•å…¥ Page å½¢å¼çš„å†…å­˜ç®¡ç†æ¨¡åž‹åŽï¼Œå†…éƒ¨å•æ¡è®°å½•çš„é•¿åº¦ä¸èƒ½è¶…è¿‡ 128 MB ï¼ˆå…·ä½“å†…å­˜æ¨¡åž‹å¯ä»¥å‚è€ƒ PackedRecordPointer ç±»ï¼‰ã€‚å¦å¤–ï¼Œåˆ†åŒºä¸ªæ•°çš„é™åˆ¶ä¹Ÿæ˜¯è¯¥å†…å­˜æ¨¡åž‹å¯¼è‡´çš„ã€‚

æ‰€ä»¥ï¼Œç›®å‰ä½¿ç”¨åŸºäºŽ Tungsten Sort Shuffle å®žçŽ°æœºåˆ¶æ¡ä»¶è¿˜æ˜¯æ¯”è¾ƒè‹›åˆ»çš„ã€‚

##### åŸºäºŽ Sort çš„ Shuffle æœºåˆ¶çš„ä¼˜ç¼ºç‚¹

**ä¼˜ç‚¹**ï¼š

- å°æ–‡ä»¶çš„æ•°é‡å¤§é‡å‡å°‘ï¼ŒMapper ç«¯çš„å†…å­˜å ç”¨å˜å°‘ï¼›
- Spark ä¸ä»…å¯ä»¥å¤„ç†å°è§„æ¨¡çš„æ•°æ®ï¼Œå³ä½¿å¤„ç†å¤§è§„æ¨¡çš„æ•°æ®ï¼Œä¹Ÿä¸ä¼šå¾ˆå®¹æ˜“è¾¾åˆ°æ€§èƒ½ç“¶é¢ˆã€‚

**ç¼ºç‚¹**ï¼š

- å¦‚æžœ Mapper ä¸­ Task çš„æ•°é‡è¿‡å¤§ï¼Œä¾æ—§ä¼šäº§ç”Ÿå¾ˆå¤šå°æ–‡ä»¶ï¼Œæ­¤æ—¶åœ¨ Shuffle ä¼ æ•°æ®çš„è¿‡ç¨‹ä¸­åˆ° Reducer ç«¯ï¼Œ Reducer ä¼šéœ€è¦åŒæ—¶å¤§é‡åœ°è®°å½•è¿›è¡Œååºåˆ—åŒ–ï¼Œå¯¼è‡´å¤§é‡å†…å­˜æ¶ˆè€—å’Œ GC è´Ÿæ‹…å·¨å¤§ï¼Œé€ æˆç³»ç»Ÿç¼“æ…¢ï¼Œç”šè‡³å´©æºƒï¼›
- å¼ºåˆ¶äº†åœ¨ Mapper ç«¯å¿…é¡»è¦æŽ’åºï¼Œå³ä½¿æ•°æ®æœ¬èº«å¹¶ä¸éœ€è¦æŽ’åºï¼›
- å®ƒè¦åŸºäºŽè®°å½•æœ¬èº«è¿›è¡ŒæŽ’åºï¼Œè¿™å°±æ˜¯ Sort-Based Shuffle æœ€è‡´å‘½çš„æ€§èƒ½æ¶ˆè€—ã€‚

## ä¸ƒã€Spark åº•å±‚æ‰§è¡ŒåŽŸç†

### Spark è¿è¡Œæµç¨‹

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BCllQj5O8mXHv1UaLeibJyxv53T5bKv5obibZK88WqX1PicVYnszouzOQ6CA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)Sparkè¿è¡Œæµç¨‹

å…·ä½“è¿è¡Œæµç¨‹å¦‚ä¸‹ï¼š

1. SparkContext å‘èµ„æºç®¡ç†å™¨æ³¨å†Œå¹¶å‘èµ„æºç®¡ç†å™¨ç”³è¯·è¿è¡Œ Executor
2. èµ„æºç®¡ç†å™¨åˆ†é… Executorï¼Œç„¶åŽèµ„æºç®¡ç†å™¨å¯åŠ¨ Executor
3. Executor å‘é€å¿ƒè·³è‡³èµ„æºç®¡ç†å™¨
4. **SparkContext æž„å»º DAG æœ‰å‘æ— çŽ¯å›¾**
5. **å°† DAG åˆ†è§£æˆ Stageï¼ˆTaskSetï¼‰**
6. **æŠŠ Stage å‘é€ç»™ TaskScheduler**
7. **Executor å‘ SparkContext ç”³è¯· Task**
8. **TaskScheduler å°† Task å‘é€ç»™ Executor è¿è¡Œ**
9. **åŒæ—¶ SparkContext å°†åº”ç”¨ç¨‹åºä»£ç å‘æ”¾ç»™ Executor**
10. Task åœ¨ Executor ä¸Šè¿è¡Œï¼Œè¿è¡Œå®Œæ¯•é‡Šæ”¾æ‰€æœ‰èµ„æº

#### 1. ä»Žä»£ç è§’åº¦çœ‹ DAG å›¾çš„æž„å»º

```
Val lines1 = sc.textFile(inputPath1).map(...).map(...)

Val lines2 = sc.textFile(inputPath2).map(...)

Val lines3 = sc.textFile(inputPath3)

Val dtinone1 = lines2.union(lines3)

Val dtinone = lines1.join(dtinone1)

dtinone.saveAsTextFile(...)

dtinone.filter(...).foreach(...)
```

ä¸Šè¿°ä»£ç çš„ DAG å›¾å¦‚ä¸‹æ‰€ç¤ºï¼š

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClicxzqwN9qGEtCdHukNrS7qKMj9XgCk881G1OKAF3G7FGIehIZRQxib1Q/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)æž„å»ºDAGå›¾

Spark å†…æ ¸ä¼šåœ¨éœ€è¦è®¡ç®—å‘ç”Ÿçš„æ—¶åˆ»ç»˜åˆ¶ä¸€å¼ å…³äºŽè®¡ç®—è·¯å¾„çš„æœ‰å‘æ— çŽ¯å›¾ï¼Œä¹Ÿå°±æ˜¯å¦‚ä¸Šå›¾æ‰€ç¤ºçš„ DAGã€‚

**Spark çš„è®¡ç®—å‘ç”Ÿåœ¨ RDD çš„ Action æ“ä½œï¼Œè€Œå¯¹ Action ä¹‹å‰çš„æ‰€æœ‰ Transformationï¼ŒSpark åªæ˜¯è®°å½•ä¸‹ RDD ç”Ÿæˆçš„è½¨è¿¹ï¼Œè€Œä¸ä¼šè§¦å‘çœŸæ­£çš„è®¡ç®—**ã€‚

#### 2. å°† DAG åˆ’åˆ†ä¸º Stage æ ¸å¿ƒç®—æ³•

ä¸€ä¸ª Application å¯ä»¥æœ‰å¤šä¸ª job å¤šä¸ª Stageï¼š

Spark Application ä¸­å¯ä»¥å› ä¸ºä¸åŒçš„ Action è§¦å‘ä¼—å¤šçš„ jobï¼Œä¸€ä¸ª Application ä¸­å¯ä»¥æœ‰å¾ˆå¤šçš„ jobï¼Œæ¯ä¸ª job æ˜¯ç”±ä¸€ä¸ªæˆ–è€…å¤šä¸ª Stage æž„æˆçš„ï¼ŒåŽé¢çš„ Stage ä¾èµ–äºŽå‰é¢çš„ Stageï¼Œä¹Ÿå°±æ˜¯è¯´åªæœ‰å‰é¢ä¾èµ–çš„ Stage è®¡ç®—å®Œæ¯•åŽï¼ŒåŽé¢çš„ Stage æ‰ä¼šè¿è¡Œã€‚

åˆ’åˆ†ä¾æ®ï¼š

**Stage åˆ’åˆ†çš„ä¾æ®å°±æ˜¯å®½ä¾èµ–**ï¼Œåƒ reduceByKeyï¼ŒgroupByKey ç­‰ç®—å­ï¼Œä¼šå¯¼è‡´å®½ä¾èµ–çš„äº§ç”Ÿã€‚

> å›žé¡¾ä¸‹å®½çª„ä¾èµ–çš„åˆ’åˆ†åŽŸåˆ™ï¼š
> **çª„ä¾èµ–**ï¼šçˆ¶ RDD çš„ä¸€ä¸ªåˆ†åŒºåªä¼šè¢«å­ RDD çš„ä¸€ä¸ªåˆ†åŒºä¾èµ–ã€‚å³ä¸€å¯¹ä¸€æˆ–è€…å¤šå¯¹ä¸€çš„å…³ç³»ï¼Œå¯ç†è§£ä¸ºç‹¬ç”Ÿå­å¥³ã€‚å¸¸è§çš„çª„ä¾èµ–æœ‰ï¼šmapã€filterã€unionã€mapPartitionsã€mapValuesã€joinï¼ˆçˆ¶ RDD æ˜¯ hash-partitionedï¼‰ç­‰ã€‚
> **å®½ä¾èµ–**ï¼šçˆ¶ RDD çš„ä¸€ä¸ªåˆ†åŒºä¼šè¢«å­ RDD çš„å¤šä¸ªåˆ†åŒºä¾èµ–(æ¶‰åŠåˆ° shuffle)ã€‚å³ä¸€å¯¹å¤šçš„å…³ç³»ï¼Œå¯ç†è§£ä¸ºè¶…ç”Ÿã€‚å¸¸è§çš„å®½ä¾èµ–æœ‰ groupByKeyã€partitionByã€reduceByKeyã€joinï¼ˆçˆ¶ RDD ä¸æ˜¯ hash-partitionedï¼‰ç­‰ã€‚

**æ ¸å¿ƒç®—æ³•ï¼šå›žæº¯ç®—æ³•**

**ä»ŽåŽå¾€å‰å›žæº¯/åå‘è§£æžï¼Œé‡åˆ°çª„ä¾èµ–åŠ å…¥æœ¬ Stageï¼Œé‡è§å®½ä¾èµ–è¿›è¡Œ Stage åˆ‡åˆ†ã€‚**

Spark å†…æ ¸ä¼šä»Žè§¦å‘ Action æ“ä½œçš„é‚£ä¸ª RDD å¼€å§‹**ä»ŽåŽå¾€å‰æŽ¨**ï¼Œé¦–å…ˆä¼šä¸ºæœ€åŽä¸€ä¸ª RDD åˆ›å»ºä¸€ä¸ª Stageï¼Œç„¶åŽç»§ç»­å€’æŽ¨ï¼Œå¦‚æžœå‘çŽ°å¯¹æŸä¸ª RDD æ˜¯å®½ä¾èµ–ï¼Œé‚£ä¹ˆå°±ä¼šå°†å®½ä¾èµ–çš„é‚£ä¸ª RDD åˆ›å»ºä¸€ä¸ªæ–°çš„ Stageï¼Œé‚£ä¸ª RDD å°±æ˜¯æ–°çš„ Stage çš„æœ€åŽä¸€ä¸ª RDDã€‚ç„¶åŽä¾æ¬¡ç±»æŽ¨ï¼Œç»§ç»­å€’æŽ¨ï¼Œæ ¹æ®çª„ä¾èµ–æˆ–è€…å®½ä¾èµ–è¿›è¡Œ Stage çš„åˆ’åˆ†ï¼Œç›´åˆ°æ‰€æœ‰çš„ RDD å…¨éƒ¨éåŽ†å®Œæˆä¸ºæ­¢ã€‚

#### 3. å°† DAG åˆ’åˆ†ä¸º Stage å‰–æž

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClUh9icmu8GUhXHpptjuVvudsRJOWKUCmQaO892YS24tUZ4QbiaibyY3psw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)DAGåˆ’åˆ†Stage

**ä¸€ä¸ª Spark ç¨‹åºå¯ä»¥æœ‰å¤šä¸ª DAG(æœ‰å‡ ä¸ª Actionï¼Œå°±æœ‰å‡ ä¸ª DAGï¼Œä¸Šå›¾æœ€åŽåªæœ‰ä¸€ä¸ª Actionï¼ˆå›¾ä¸­æœªè¡¨çŽ°ï¼‰,é‚£ä¹ˆå°±æ˜¯ä¸€ä¸ª DAG)**ã€‚

ä¸€ä¸ª DAG å¯ä»¥æœ‰å¤šä¸ª Stage(æ ¹æ®å®½ä¾èµ–/shuffle è¿›è¡Œåˆ’åˆ†)ã€‚

**åŒä¸€ä¸ª Stage å¯ä»¥æœ‰å¤šä¸ª Task å¹¶è¡Œæ‰§è¡Œ**(**task æ•°=åˆ†åŒºæ•°**ï¼Œå¦‚ä¸Šå›¾ï¼ŒStage1 ä¸­æœ‰ä¸‰ä¸ªåˆ†åŒº P1ã€P2ã€P3ï¼Œå¯¹åº”çš„ä¹Ÿæœ‰ä¸‰ä¸ª Task)ã€‚

å¯ä»¥çœ‹åˆ°è¿™ä¸ª DAG ä¸­åª reduceByKey æ“ä½œæ˜¯ä¸€ä¸ªå®½ä¾èµ–ï¼ŒSpark å†…æ ¸ä¼šä»¥æ­¤ä¸ºè¾¹ç•Œå°†å…¶å‰åŽåˆ’åˆ†æˆä¸åŒçš„ Stageã€‚

åŒæ—¶æˆ‘ä»¬å¯ä»¥æ³¨æ„åˆ°ï¼Œåœ¨å›¾ä¸­ Stage1 ä¸­ï¼Œ**ä»Ž textFile åˆ° flatMap åˆ° map éƒ½æ˜¯çª„ä¾èµ–ï¼Œè¿™å‡ æ­¥æ“ä½œå¯ä»¥å½¢æˆä¸€ä¸ªæµæ°´çº¿æ“ä½œï¼Œé€šè¿‡ flatMap æ“ä½œç”Ÿæˆçš„ partition å¯ä»¥ä¸ç”¨ç­‰å¾…æ•´ä¸ª RDD è®¡ç®—ç»“æŸï¼Œè€Œæ˜¯ç»§ç»­è¿›è¡Œ map æ“ä½œï¼Œè¿™æ ·å¤§å¤§æé«˜äº†è®¡ç®—çš„æ•ˆçŽ‡**ã€‚

#### 4. æäº¤ Stages

è°ƒåº¦é˜¶æ®µçš„æäº¤ï¼Œæœ€ç»ˆä¼šè¢«è½¬æ¢æˆä¸€ä¸ªä»»åŠ¡é›†çš„æäº¤ï¼ŒDAGScheduler é€šè¿‡ TaskScheduler æŽ¥å£æäº¤ä»»åŠ¡é›†ï¼Œè¿™ä¸ªä»»åŠ¡é›†æœ€ç»ˆä¼šè§¦å‘ TaskScheduler æž„å»ºä¸€ä¸ª TaskSetManager çš„å®žä¾‹æ¥ç®¡ç†è¿™ä¸ªä»»åŠ¡é›†çš„ç”Ÿå‘½å‘¨æœŸï¼Œå¯¹äºŽ DAGScheduler æ¥è¯´ï¼Œæäº¤è°ƒåº¦é˜¶æ®µçš„å·¥ä½œåˆ°æ­¤å°±å®Œæˆäº†ã€‚

è€Œ TaskScheduler çš„å…·ä½“å®žçŽ°åˆ™ä¼šåœ¨å¾—åˆ°è®¡ç®—èµ„æºçš„æ—¶å€™ï¼Œè¿›ä¸€æ­¥é€šè¿‡ TaskSetManager è°ƒåº¦å…·ä½“çš„ä»»åŠ¡åˆ°å¯¹åº”çš„ Executor èŠ‚ç‚¹ä¸Šè¿›è¡Œè¿ç®—ã€‚

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClPkNJ0YHYe5RA2fyLklFFUzPpUibuWrGyF6EkZDAbq0PBVr7mLO2PCrA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

#### 5. ç›‘æŽ§ Jobã€Taskã€Executor

1. DAGScheduler ç›‘æŽ§ Job ä¸Ž Taskï¼š

è¦ä¿è¯ç›¸äº’ä¾èµ–çš„ä½œä¸šè°ƒåº¦é˜¶æ®µèƒ½å¤Ÿå¾—åˆ°é¡ºåˆ©çš„è°ƒåº¦æ‰§è¡Œï¼ŒDAGScheduler éœ€è¦ç›‘æŽ§å½“å‰ä½œä¸šè°ƒåº¦é˜¶æ®µä¹ƒè‡³ä»»åŠ¡çš„å®Œæˆæƒ…å†µã€‚

è¿™é€šè¿‡å¯¹å¤–æš´éœ²ä¸€ç³»åˆ—çš„å›žè°ƒå‡½æ•°æ¥å®žçŽ°çš„ï¼Œå¯¹äºŽ TaskScheduler æ¥è¯´ï¼Œè¿™äº›å›žè°ƒå‡½æ•°ä¸»è¦åŒ…æ‹¬ä»»åŠ¡çš„å¼€å§‹ç»“æŸå¤±è´¥ã€ä»»åŠ¡é›†çš„å¤±è´¥ï¼ŒDAGScheduler æ ¹æ®è¿™äº›ä»»åŠ¡çš„ç”Ÿå‘½å‘¨æœŸä¿¡æ¯è¿›ä¸€æ­¥ç»´æŠ¤ä½œä¸šå’Œè°ƒåº¦é˜¶æ®µçš„çŠ¶æ€ä¿¡æ¯ã€‚

1. DAGScheduler ç›‘æŽ§ Executor çš„ç”Ÿå‘½çŠ¶æ€ï¼š

TaskScheduler é€šè¿‡å›žè°ƒå‡½æ•°é€šçŸ¥ DAGScheduler å…·ä½“çš„ Executor çš„ç”Ÿå‘½çŠ¶æ€ï¼Œ**å¦‚æžœæŸä¸€ä¸ª Executor å´©æºƒäº†ï¼Œåˆ™å¯¹åº”çš„è°ƒåº¦é˜¶æ®µä»»åŠ¡é›†çš„ ShuffleMapTask çš„è¾“å‡ºç»“æžœä¹Ÿå°†æ ‡å¿—ä¸ºä¸å¯ç”¨ï¼Œè¿™å°†å¯¼è‡´å¯¹åº”ä»»åŠ¡é›†çŠ¶æ€çš„å˜æ›´ï¼Œè¿›è€Œé‡æ–°æ‰§è¡Œç›¸å…³è®¡ç®—ä»»åŠ¡ï¼Œä»¥èŽ·å–ä¸¢å¤±çš„ç›¸å…³æ•°æ®**ã€‚

#### 6. èŽ·å–ä»»åŠ¡æ‰§è¡Œç»“æžœ

1. ç»“æžœ DAGSchedulerï¼š

ä¸€ä¸ªå…·ä½“çš„ä»»åŠ¡åœ¨ Executor ä¸­æ‰§è¡Œå®Œæ¯•åŽï¼Œå…¶ç»“æžœéœ€è¦ä»¥æŸç§å½¢å¼è¿”å›žç»™ DAGSchedulerï¼Œæ ¹æ®ä»»åŠ¡ç±»åž‹çš„ä¸åŒï¼Œä»»åŠ¡ç»“æžœçš„è¿”å›žæ–¹å¼ä¹Ÿä¸åŒã€‚

1. ä¸¤ç§ç»“æžœï¼Œä¸­é—´ç»“æžœä¸Žæœ€ç»ˆç»“æžœï¼š

å¯¹äºŽ FinalStage æ‰€å¯¹åº”çš„ä»»åŠ¡ï¼Œè¿”å›žç»™ DAGScheduler çš„æ˜¯è¿ç®—ç»“æžœæœ¬èº«ã€‚

è€Œå¯¹äºŽä¸­é—´è°ƒåº¦é˜¶æ®µå¯¹åº”çš„ä»»åŠ¡ ShuffleMapTaskï¼Œè¿”å›žç»™ DAGScheduler çš„æ˜¯ä¸€ä¸ª MapStatus é‡Œçš„ç›¸å…³å­˜å‚¨ä¿¡æ¯ï¼Œè€Œéžç»“æžœæœ¬èº«ï¼Œè¿™äº›å­˜å‚¨ä½ç½®ä¿¡æ¯å°†ä½œä¸ºä¸‹ä¸€ä¸ªè°ƒåº¦é˜¶æ®µçš„ä»»åŠ¡èŽ·å–è¾“å…¥æ•°æ®çš„ä¾æ®ã€‚

1. ä¸¤ç§ç±»åž‹ï¼Œ**DirectTaskResult ä¸Ž IndirectTaskResult**ï¼š

æ ¹æ®ä»»åŠ¡ç»“æžœå¤§å°çš„ä¸åŒï¼ŒResultTask è¿”å›žçš„ç»“æžœåˆåˆ†ä¸ºä¸¤ç±»ï¼š

å¦‚æžœç»“æžœè¶³å¤Ÿå°ï¼Œåˆ™ç›´æŽ¥æ”¾åœ¨ DirectTaskResult å¯¹è±¡å†…ä¸­ã€‚

å¦‚æžœè¶…è¿‡ç‰¹å®šå°ºå¯¸åˆ™åœ¨ Executor ç«¯ä¼šå°† DirectTaskResult å…ˆåºåˆ—åŒ–ï¼Œå†æŠŠåºåˆ—åŒ–çš„ç»“æžœä½œä¸ºä¸€ä¸ªæ•°æ®å—å­˜æ”¾åœ¨ BlockManager ä¸­ï¼Œç„¶åŽå°† BlockManager è¿”å›žçš„ BlockID æ”¾åœ¨ IndirectTaskResult å¯¹è±¡ä¸­è¿”å›žç»™ TaskSchedulerï¼ŒTaskScheduler è¿›è€Œè°ƒç”¨ TaskResultGetter å°† IndirectTaskResult ä¸­çš„ BlockID å–å‡ºå¹¶é€šè¿‡ BlockManager æœ€ç»ˆå–å¾—å¯¹åº”çš„ DirectTaskResultã€‚

#### 7. ä»»åŠ¡è°ƒåº¦æ€»ä½“è¯ é‡Š

**ä¸€å¼ å›¾è¯´æ˜Žä»»åŠ¡æ€»ä½“è°ƒåº¦ï¼š**

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClojkLjQHYgr34kddApkJiaAo9qnXblwtb9qe8tmjXpCyGkiabDRmIbyOA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)ä»»åŠ¡æ€»ä½“è°ƒåº¦

### Spark è¿è¡Œæž¶æž„ç‰¹ç‚¹

#### 1. Executor è¿›ç¨‹ä¸“å±ž

**æ¯ä¸ª Application èŽ·å–ä¸“å±žçš„ Executor è¿›ç¨‹ï¼Œè¯¥è¿›ç¨‹åœ¨ Application æœŸé—´ä¸€ç›´é©»ç•™ï¼Œå¹¶ä»¥å¤šçº¿ç¨‹æ–¹å¼è¿è¡Œ Tasks**ã€‚

Spark Application ä¸èƒ½è·¨åº”ç”¨ç¨‹åºå…±äº«æ•°æ®ï¼Œé™¤éžå°†æ•°æ®å†™å…¥åˆ°å¤–éƒ¨å­˜å‚¨ç³»ç»Ÿã€‚å¦‚å›¾æ‰€ç¤ºï¼š

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClEUx0BvfKJH11htDdtOXyJ75Mibzs09mv6ql6qiauZa8ZthEG9rhwfkSw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)Executorè¿›ç¨‹ä¸“å±ž

#### 2. æ”¯æŒå¤šç§èµ„æºç®¡ç†å™¨

Spark ä¸Žèµ„æºç®¡ç†å™¨æ— å…³ï¼Œåªè¦èƒ½å¤ŸèŽ·å– Executor è¿›ç¨‹ï¼Œå¹¶èƒ½ä¿æŒç›¸äº’é€šä¿¡å°±å¯ä»¥äº†ã€‚

Spark æ”¯æŒèµ„æºç®¡ç†å™¨åŒ…å«ï¼šStandaloneã€On Mesosã€On YARNã€Or On EC2ã€‚å¦‚å›¾æ‰€ç¤º:

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClW2U7iaUh1PyfnVRpZVbpLuLme91whfffBCibcMAypCRbdDPF9BCab6cA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)æ”¯æŒå¤šç§èµ„æºç®¡ç†å™¨

#### 3. Job æäº¤å°±è¿‘åŽŸåˆ™

**æäº¤ SparkContext çš„ Client åº”è¯¥é è¿‘ Worker èŠ‚ç‚¹(è¿è¡Œ Executor çš„èŠ‚ç‚¹)**ï¼Œæœ€å¥½æ˜¯åœ¨åŒä¸€ä¸ª Rack(æœºæž¶)é‡Œï¼Œå› ä¸º Spark Application è¿è¡Œè¿‡ç¨‹ä¸­ SparkContext å’Œ Executor ä¹‹é—´æœ‰å¤§é‡çš„ä¿¡æ¯äº¤æ¢;

å¦‚æžœæƒ³åœ¨è¿œç¨‹é›†ç¾¤ä¸­è¿è¡Œï¼Œæœ€å¥½ä½¿ç”¨ RPC å°† SparkContext æäº¤ç»™é›†ç¾¤ï¼Œ**ä¸è¦è¿œç¦» Worker è¿è¡Œ SparkContext**ã€‚

å¦‚å›¾æ‰€ç¤º:

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClMrwB946RL5EYb0D6Df9xoLJTKTgtZaWpoqoELjqQKicZE67ZmialwY4w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)Jobæäº¤å°±è¿‘åŽŸåˆ™

#### 4. ç§»åŠ¨ç¨‹åºè€Œéžç§»åŠ¨æ•°æ®çš„åŽŸåˆ™æ‰§è¡Œ

**ç§»åŠ¨ç¨‹åºè€Œéžç§»åŠ¨æ•°æ®çš„åŽŸåˆ™æ‰§è¡Œï¼ŒTask é‡‡ç”¨äº†æ•°æ®æœ¬åœ°æ€§å’ŒæŽ¨æµ‹æ‰§è¡Œçš„ä¼˜åŒ–æœºåˆ¶**ã€‚

å…³é”®æ–¹æ³•ï¼štaskIdToLocationsã€getPreferedLocationsã€‚

å¦‚å›¾æ‰€ç¤º:

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClmXs39yOT7wTfNvMicrEFJtsBKib2tuy7PhJ8xgwicecSBOzn0rK0Kkibtw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)æ•°æ®æœ¬åœ°æ€§

## å…«ã€Spark æ•°æ®å€¾æ–œ

å°±æ˜¯æ•°æ®åˆ†åˆ°å„ä¸ªåŒºçš„æ•°é‡ä¸å¤ªå‡åŒ€,å¯ä»¥è‡ªå®šä¹‰åˆ†åŒºå™¨,æƒ³æ€Žä¹ˆåˆ†å°±æ€Žä¹ˆåˆ†ã€‚

**Sparkä¸­çš„æ•°æ®å€¾æ–œé—®é¢˜ä¸»è¦æŒ‡shuffleè¿‡ç¨‹ä¸­å‡ºçŽ°çš„æ•°æ®å€¾æ–œé—®é¢˜ï¼Œæ˜¯ç”±äºŽä¸åŒçš„keyå¯¹åº”çš„æ•°æ®é‡ä¸åŒå¯¼è‡´çš„ä¸åŒtaskæ‰€å¤„ç†çš„æ•°æ®é‡ä¸åŒçš„é—®é¢˜**ã€‚

ä¾‹å¦‚ï¼Œreducedç«¯ä¸€å…±è¦å¤„ç†100ä¸‡æ¡æ•°æ®ï¼Œç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªtaskåˆ†åˆ«è¢«åˆ†é…åˆ°äº†1ä¸‡æ¡æ•°æ®ï¼Œè®¡ç®—5åˆ†é’Ÿå†…å®Œæˆï¼Œç¬¬ä¸‰ä¸ªtaskåˆ†é…åˆ°äº†98ä¸‡æ•°æ®ï¼Œæ­¤æ—¶ç¬¬ä¸‰ä¸ªtaskå¯èƒ½éœ€è¦10ä¸ªå°æ—¶å®Œæˆï¼Œè¿™ä½¿å¾—æ•´ä¸ªSparkä½œä¸šéœ€è¦10ä¸ªå°æ—¶æ‰èƒ½è¿è¡Œå®Œæˆï¼Œè¿™å°±æ˜¯æ•°æ®å€¾æ–œæ‰€å¸¦æ¥çš„åŽæžœã€‚

> æ³¨æ„ï¼Œè¦åŒºåˆ†å¼€**æ•°æ®å€¾æ–œ**ä¸Ž**æ•°æ®è¿‡é‡**è¿™ä¸¤ç§æƒ…å†µï¼Œæ•°æ®å€¾æ–œæ˜¯æŒ‡å°‘æ•°taskè¢«åˆ†é…äº†ç»å¤§å¤šæ•°çš„æ•°æ®ï¼Œå› æ­¤å°‘æ•°taskè¿è¡Œç¼“æ…¢ï¼›æ•°æ®è¿‡é‡æ˜¯æŒ‡æ‰€æœ‰taskè¢«åˆ†é…çš„æ•°æ®é‡éƒ½å¾ˆå¤§ï¼Œç›¸å·®ä¸å¤šï¼Œæ‰€æœ‰taskéƒ½è¿è¡Œç¼“æ…¢ã€‚

æ•°æ®å€¾æ–œçš„è¡¨çŽ°ï¼š

1. Sparkä½œä¸šçš„å¤§éƒ¨åˆ†taskéƒ½æ‰§è¡Œè¿…é€Ÿï¼Œåªæœ‰æœ‰é™çš„å‡ ä¸ªtaskæ‰§è¡Œçš„éžå¸¸æ…¢ï¼Œæ­¤æ—¶å¯èƒ½å‡ºçŽ°äº†æ•°æ®å€¾æ–œï¼Œä½œä¸šå¯ä»¥è¿è¡Œï¼Œä½†æ˜¯è¿è¡Œå¾—éžå¸¸æ…¢ï¼›
2. Sparkä½œä¸šçš„å¤§éƒ¨åˆ†taskéƒ½æ‰§è¡Œè¿…é€Ÿï¼Œä½†æ˜¯æœ‰çš„taskåœ¨è¿è¡Œè¿‡ç¨‹ä¸­ä¼šçªç„¶æŠ¥å‡ºOOMï¼Œåå¤æ‰§è¡Œå‡ æ¬¡éƒ½åœ¨æŸä¸€ä¸ªtaskæŠ¥å‡ºOOMé”™è¯¯ï¼Œæ­¤æ—¶å¯èƒ½å‡ºçŽ°äº†æ•°æ®å€¾æ–œï¼Œä½œä¸šæ— æ³•æ­£å¸¸è¿è¡Œã€‚å®šä½æ•°æ®å€¾æ–œé—®é¢˜ï¼š
3. æŸ¥é˜…ä»£ç ä¸­çš„shuffleç®—å­ï¼Œä¾‹å¦‚reduceByKeyã€countByKeyã€groupByKeyã€joinç­‰ç®—å­ï¼Œæ ¹æ®ä»£ç é€»è¾‘åˆ¤æ–­æ­¤å¤„æ˜¯å¦ä¼šå‡ºçŽ°æ•°æ®å€¾æ–œï¼›
4. æŸ¥çœ‹Sparkä½œä¸šçš„logæ–‡ä»¶ï¼Œlogæ–‡ä»¶å¯¹äºŽé”™è¯¯çš„è®°å½•ä¼šç²¾ç¡®åˆ°ä»£ç çš„æŸä¸€è¡Œï¼Œå¯ä»¥æ ¹æ®å¼‚å¸¸å®šä½åˆ°çš„ä»£ç ä½ç½®æ¥æ˜Žç¡®é”™è¯¯å‘ç”Ÿåœ¨ç¬¬å‡ ä¸ªstageï¼Œå¯¹åº”çš„shuffleç®—å­æ˜¯å“ªä¸€ä¸ªï¼›

### 1. é¢„èšåˆåŽŸå§‹æ•°æ®

**1. é¿å…shuffleè¿‡ç¨‹**

ç»å¤§å¤šæ•°æƒ…å†µä¸‹ï¼ŒSparkä½œä¸šçš„æ•°æ®æ¥æºéƒ½æ˜¯Hiveè¡¨ï¼Œè¿™äº›Hiveè¡¨åŸºæœ¬éƒ½æ˜¯ç»è¿‡ETLä¹‹åŽçš„æ˜¨å¤©çš„æ•°æ®ã€‚ä¸ºäº†é¿å…æ•°æ®å€¾æ–œï¼Œæˆ‘ä»¬å¯ä»¥è€ƒè™‘é¿å…shuffleè¿‡ç¨‹ï¼Œå¦‚æžœé¿å…äº†shuffleè¿‡ç¨‹ï¼Œé‚£ä¹ˆä»Žæ ¹æœ¬ä¸Šå°±æ¶ˆé™¤äº†å‘ç”Ÿæ•°æ®å€¾æ–œé—®é¢˜çš„å¯èƒ½ã€‚

å¦‚æžœSparkä½œä¸šçš„æ•°æ®æ¥æºäºŽHiveè¡¨ï¼Œé‚£ä¹ˆå¯ä»¥å…ˆåœ¨Hiveè¡¨ä¸­å¯¹æ•°æ®è¿›è¡Œèšåˆï¼Œä¾‹å¦‚æŒ‰ç…§keyè¿›è¡Œåˆ†ç»„ï¼Œå°†åŒä¸€keyå¯¹åº”çš„æ‰€æœ‰valueç”¨ä¸€ç§ç‰¹æ®Šçš„æ ¼å¼æ‹¼æŽ¥åˆ°ä¸€ä¸ªå­—ç¬¦ä¸²é‡ŒåŽ»ï¼Œè¿™æ ·ï¼Œä¸€ä¸ªkeyå°±åªæœ‰ä¸€æ¡æ•°æ®äº†ï¼›ä¹‹åŽï¼Œå¯¹ä¸€ä¸ªkeyçš„æ‰€æœ‰valueè¿›è¡Œå¤„ç†æ—¶ï¼Œåªéœ€è¦è¿›è¡Œmapæ“ä½œå³å¯ï¼Œæ— éœ€å†è¿›è¡Œä»»ä½•çš„shuffleæ“ä½œã€‚é€šè¿‡ä¸Šè¿°æ–¹å¼å°±é¿å…äº†æ‰§è¡Œshuffleæ“ä½œï¼Œä¹Ÿå°±ä¸å¯èƒ½ä¼šå‘ç”Ÿä»»ä½•çš„æ•°æ®å€¾æ–œé—®é¢˜ã€‚

å¯¹äºŽHiveè¡¨ä¸­æ•°æ®çš„æ“ä½œï¼Œä¸ä¸€å®šæ˜¯æ‹¼æŽ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä¹Ÿå¯ä»¥æ˜¯ç›´æŽ¥å¯¹keyçš„æ¯ä¸€æ¡æ•°æ®è¿›è¡Œç´¯è®¡è®¡ç®—ã€‚è¦åŒºåˆ†å¼€ï¼Œå¤„ç†çš„æ•°æ®é‡å¤§å’Œæ•°æ®å€¾æ–œçš„åŒºåˆ«ã€‚

**2. å¢žå¤§keyç²’åº¦ï¼ˆå‡å°æ•°æ®å€¾æ–œå¯èƒ½æ€§ï¼Œå¢žå¤§æ¯ä¸ªtaskçš„æ•°æ®é‡ï¼‰**

å¦‚æžœæ²¡æœ‰åŠžæ³•å¯¹æ¯ä¸ªkeyèšåˆå‡ºæ¥ä¸€æ¡æ•°æ®ï¼Œåœ¨ç‰¹å®šåœºæ™¯ä¸‹ï¼Œå¯ä»¥è€ƒè™‘æ‰©å¤§keyçš„èšåˆç²’åº¦ã€‚

ä¾‹å¦‚ï¼Œç›®å‰æœ‰10ä¸‡æ¡ç”¨æˆ·æ•°æ®ï¼Œå½“å‰keyçš„ç²’åº¦æ˜¯ï¼ˆçœï¼ŒåŸŽå¸‚ï¼ŒåŒºï¼Œæ—¥æœŸï¼‰ï¼ŒçŽ°åœ¨æˆ‘ä»¬è€ƒè™‘æ‰©å¤§ç²’åº¦ï¼Œå°†keyçš„ç²’åº¦æ‰©å¤§ä¸ºï¼ˆçœï¼ŒåŸŽå¸‚ï¼Œæ—¥æœŸï¼‰ï¼Œè¿™æ ·çš„è¯ï¼Œkeyçš„æ•°é‡ä¼šå‡å°‘ï¼Œkeyä¹‹é—´çš„æ•°æ®é‡å·®å¼‚ä¹Ÿæœ‰å¯èƒ½ä¼šå‡å°‘ï¼Œç”±æ­¤å¯ä»¥å‡è½»æ•°æ®å€¾æ–œçš„çŽ°è±¡å’Œé—®é¢˜ã€‚ï¼ˆæ­¤æ–¹æ³•åªé’ˆå¯¹ç‰¹å®šç±»åž‹çš„æ•°æ®æœ‰æ•ˆï¼Œå½“åº”ç”¨åœºæ™¯ä¸é€‚å®œæ—¶ï¼Œä¼šåŠ é‡æ•°æ®å€¾æ–œï¼‰

### 2. é¢„å¤„ç†å¯¼è‡´å€¾æ–œçš„key

**1. è¿‡æ»¤**

å¦‚æžœåœ¨Sparkä½œä¸šä¸­å…è®¸ä¸¢å¼ƒæŸäº›æ•°æ®ï¼Œé‚£ä¹ˆå¯ä»¥è€ƒè™‘å°†å¯èƒ½å¯¼è‡´æ•°æ®å€¾æ–œçš„keyè¿›è¡Œè¿‡æ»¤ï¼Œæ»¤é™¤å¯èƒ½å¯¼è‡´æ•°æ®å€¾æ–œçš„keyå¯¹åº”çš„æ•°æ®ï¼Œè¿™æ ·ï¼Œåœ¨Sparkä½œä¸šä¸­å°±ä¸ä¼šå‘ç”Ÿæ•°æ®å€¾æ–œäº†ã€‚

**2. ä½¿ç”¨éšæœºkey**

å½“ä½¿ç”¨äº†ç±»ä¼¼äºŽgroupByKeyã€reduceByKeyè¿™æ ·çš„ç®—å­æ—¶ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨éšæœºkeyå®žçŽ°åŒé‡èšåˆï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClZWcIfPWer9WXxwqN1UJCmXAUzUNB6XalXf7wdgibu8GKDoPk858xEhw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)éšæœºkeyå®žçŽ°åŒé‡èšåˆ

é¦–å…ˆï¼Œé€šè¿‡mapç®—å­ç»™æ¯ä¸ªæ•°æ®çš„keyæ·»åŠ éšæœºæ•°å‰ç¼€ï¼Œå¯¹keyè¿›è¡Œæ‰“æ•£ï¼Œå°†åŽŸå…ˆä¸€æ ·çš„keyå˜æˆä¸ä¸€æ ·çš„keyï¼Œç„¶åŽè¿›è¡Œç¬¬ä¸€æ¬¡èšåˆï¼Œè¿™æ ·å°±å¯ä»¥è®©åŽŸæœ¬è¢«ä¸€ä¸ªtaskå¤„ç†çš„æ•°æ®åˆ†æ•£åˆ°å¤šä¸ªtaskä¸ŠåŽ»åšå±€éƒ¨èšåˆï¼›éšåŽï¼ŒåŽ»é™¤æŽ‰æ¯ä¸ªkeyçš„å‰ç¼€ï¼Œå†æ¬¡è¿›è¡Œèšåˆã€‚

æ­¤æ–¹æ³•å¯¹äºŽç”±groupByKeyã€reduceByKeyè¿™ç±»ç®—å­é€ æˆçš„æ•°æ®å€¾æ–œæœ‰æ¯”è¾ƒå¥½çš„æ•ˆæžœï¼Œä»…ä»…é€‚ç”¨äºŽèšåˆç±»çš„shuffleæ“ä½œï¼Œé€‚ç”¨èŒƒå›´ç›¸å¯¹è¾ƒçª„ã€‚å¦‚æžœæ˜¯joinç±»çš„shuffleæ“ä½œï¼Œè¿˜å¾—ç”¨å…¶ä»–çš„è§£å†³æ–¹æ¡ˆã€‚

æ­¤æ–¹æ³•ä¹Ÿæ˜¯å‰å‡ ç§æ–¹æ¡ˆæ²¡æœ‰æ¯”è¾ƒå¥½çš„æ•ˆæžœæ—¶è¦å°è¯•çš„è§£å†³æ–¹æ¡ˆã€‚

**3. sampleé‡‡æ ·å¯¹å€¾æ–œkeyå•ç‹¬è¿›è¡Œjoin**

åœ¨Sparkä¸­ï¼Œ**å¦‚æžœæŸä¸ªRDDåªæœ‰ä¸€ä¸ªkeyï¼Œé‚£ä¹ˆåœ¨shuffleè¿‡ç¨‹ä¸­ä¼šé»˜è®¤å°†æ­¤keyå¯¹åº”çš„æ•°æ®æ‰“æ•£ï¼Œç”±ä¸åŒçš„reduceç«¯taskè¿›è¡Œå¤„ç†**ã€‚

æ‰€ä»¥å½“ç”±å•ä¸ªkeyå¯¼è‡´æ•°æ®å€¾æ–œæ—¶ï¼Œå¯æœ‰å°†å‘ç”Ÿæ•°æ®å€¾æ–œçš„keyå•ç‹¬æå–å‡ºæ¥ï¼Œç»„æˆä¸€ä¸ªRDDï¼Œç„¶åŽç”¨è¿™ä¸ªåŽŸæœ¬ä¼šå¯¼è‡´å€¾æ–œçš„keyç»„æˆçš„RDDå’Œå…¶ä»–RDDå•ç‹¬joinï¼Œæ­¤æ—¶ï¼Œæ ¹æ®Sparkçš„è¿è¡Œæœºåˆ¶ï¼Œæ­¤RDDä¸­çš„æ•°æ®ä¼šåœ¨shuffleé˜¶æ®µè¢«åˆ†æ•£åˆ°å¤šä¸ªtaskä¸­åŽ»è¿›è¡Œjoinæ“ä½œã€‚

å€¾æ–œkeyå•ç‹¬joinçš„æµç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClK8gdxuDmC7o2amajrGnicWTPadyG2r6efQo6Lyico4Ve9tjtEJj1Np2g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)å€¾æ–œkeyå•ç‹¬joinæµç¨‹

é€‚ç”¨åœºæ™¯åˆ†æžï¼š

å¯¹äºŽRDDä¸­çš„æ•°æ®ï¼Œå¯ä»¥å°†å…¶è½¬æ¢ä¸ºä¸€ä¸ªä¸­é—´è¡¨ï¼Œæˆ–è€…æ˜¯ç›´æŽ¥ä½¿ç”¨countByKey()çš„æ–¹å¼ï¼Œçœ‹ä¸€ä¸‹è¿™ä¸ªRDDä¸­å„ä¸ªkeyå¯¹åº”çš„æ•°æ®é‡ï¼Œæ­¤æ—¶å¦‚æžœä½ å‘çŽ°æ•´ä¸ªRDDå°±ä¸€ä¸ªkeyçš„æ•°æ®é‡ç‰¹åˆ«å¤šï¼Œé‚£ä¹ˆå°±å¯ä»¥è€ƒè™‘ä½¿ç”¨è¿™ç§æ–¹æ³•ã€‚

å½“æ•°æ®é‡éžå¸¸å¤§æ—¶ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨sampleé‡‡æ ·èŽ·å–10%çš„æ•°æ®ï¼Œç„¶åŽåˆ†æžè¿™10%çš„æ•°æ®ä¸­å“ªä¸ªkeyå¯èƒ½ä¼šå¯¼è‡´æ•°æ®å€¾æ–œï¼Œç„¶åŽå°†è¿™ä¸ªkeyå¯¹åº”çš„æ•°æ®å•ç‹¬æå–å‡ºæ¥ã€‚

ä¸é€‚ç”¨åœºæ™¯åˆ†æžï¼š

å¦‚æžœä¸€ä¸ªRDDä¸­å¯¼è‡´æ•°æ®å€¾æ–œçš„keyå¾ˆå¤šï¼Œé‚£ä¹ˆæ­¤æ–¹æ¡ˆä¸é€‚ç”¨ã€‚

### 3. æé«˜reduceå¹¶è¡Œåº¦

å½“æ–¹æ¡ˆä¸€å’Œæ–¹æ¡ˆäºŒå¯¹äºŽæ•°æ®å€¾æ–œçš„å¤„ç†æ²¡æœ‰å¾ˆå¥½çš„æ•ˆæžœæ—¶ï¼Œå¯ä»¥è€ƒè™‘æé«˜shuffleè¿‡ç¨‹ä¸­çš„reduceç«¯å¹¶è¡Œåº¦ï¼Œreduceç«¯å¹¶è¡Œåº¦çš„æé«˜å°±å¢žåŠ äº†reduceç«¯taskçš„æ•°é‡ï¼Œé‚£ä¹ˆæ¯ä¸ªtaskåˆ†é…åˆ°çš„æ•°æ®é‡å°±ä¼šç›¸åº”å‡å°‘ï¼Œç”±æ­¤ç¼“è§£æ•°æ®å€¾æ–œé—®é¢˜ã€‚

**1. reduceç«¯å¹¶è¡Œåº¦çš„è®¾ç½®**

åœ¨å¤§éƒ¨åˆ†çš„shuffleç®—å­ä¸­ï¼Œéƒ½å¯ä»¥ä¼ å…¥ä¸€ä¸ªå¹¶è¡Œåº¦çš„è®¾ç½®å‚æ•°ï¼Œæ¯”å¦‚reduceByKey(500)ï¼Œè¿™ä¸ªå‚æ•°ä¼šå†³å®šshuffleè¿‡ç¨‹ä¸­reduceç«¯çš„å¹¶è¡Œåº¦ï¼Œåœ¨è¿›è¡Œshuffleæ“ä½œçš„æ—¶å€™ï¼Œå°±ä¼šå¯¹åº”ç€åˆ›å»ºæŒ‡å®šæ•°é‡çš„reduce taskã€‚å¯¹äºŽSpark SQLä¸­çš„shuffleç±»è¯­å¥ï¼Œæ¯”å¦‚group byã€joinç­‰ï¼Œéœ€è¦è®¾ç½®ä¸€ä¸ªå‚æ•°ï¼Œå³`spark.sql.shuffle.partitions`ï¼Œè¯¥å‚æ•°ä»£è¡¨äº†shuffle read taskçš„å¹¶è¡Œåº¦ï¼Œè¯¥å€¼é»˜è®¤æ˜¯200ï¼Œå¯¹äºŽå¾ˆå¤šåœºæ™¯æ¥è¯´éƒ½æœ‰ç‚¹è¿‡å°ã€‚

å¢žåŠ shuffle read taskçš„æ•°é‡ï¼Œå¯ä»¥è®©åŽŸæœ¬åˆ†é…ç»™ä¸€ä¸ªtaskçš„å¤šä¸ªkeyåˆ†é…ç»™å¤šä¸ªtaskï¼Œä»Žè€Œè®©æ¯ä¸ªtaskå¤„ç†æ¯”åŽŸæ¥æ›´å°‘çš„æ•°æ®ã€‚

ä¸¾ä¾‹æ¥è¯´ï¼Œå¦‚æžœåŽŸæœ¬æœ‰5ä¸ªkeyï¼Œæ¯ä¸ªkeyå¯¹åº”10æ¡æ•°æ®ï¼Œè¿™5ä¸ªkeyéƒ½æ˜¯åˆ†é…ç»™ä¸€ä¸ªtaskçš„ï¼Œé‚£ä¹ˆè¿™ä¸ªtaskå°±è¦å¤„ç†50æ¡æ•°æ®ã€‚è€Œå¢žåŠ äº†shuffle read taskä»¥åŽï¼Œæ¯ä¸ªtaskå°±åˆ†é…åˆ°ä¸€ä¸ªkeyï¼Œå³æ¯ä¸ªtaskå°±å¤„ç†10æ¡æ•°æ®ï¼Œé‚£ä¹ˆè‡ªç„¶æ¯ä¸ªtaskçš„æ‰§è¡Œæ—¶é—´éƒ½ä¼šå˜çŸ­äº†ã€‚

**2. reduceç«¯å¹¶è¡Œåº¦è®¾ç½®å­˜åœ¨çš„ç¼ºé™·**

æé«˜reduceç«¯å¹¶è¡Œåº¦å¹¶æ²¡æœ‰ä»Žæ ¹æœ¬ä¸Šæ”¹å˜æ•°æ®å€¾æ–œçš„æœ¬è´¨å’Œé—®é¢˜ï¼ˆæ–¹æ¡ˆä¸€å’Œæ–¹æ¡ˆäºŒä»Žæ ¹æœ¬ä¸Šé¿å…äº†æ•°æ®å€¾æ–œçš„å‘ç”Ÿï¼‰ï¼Œåªæ˜¯å°½å¯èƒ½åœ°åŽ»ç¼“è§£å’Œå‡è½»shuffle reduce taskçš„æ•°æ®åŽ‹åŠ›ï¼Œä»¥åŠæ•°æ®å€¾æ–œçš„é—®é¢˜ï¼Œé€‚ç”¨äºŽæœ‰è¾ƒå¤škeyå¯¹åº”çš„æ•°æ®é‡éƒ½æ¯”è¾ƒå¤§çš„æƒ…å†µã€‚

è¯¥æ–¹æ¡ˆé€šå¸¸æ— æ³•å½»åº•è§£å†³æ•°æ®å€¾æ–œï¼Œå› ä¸ºå¦‚æžœå‡ºçŽ°ä¸€äº›æžç«¯æƒ…å†µï¼Œæ¯”å¦‚æŸä¸ªkeyå¯¹åº”çš„æ•°æ®é‡æœ‰100ä¸‡ï¼Œé‚£ä¹ˆæ— è®ºä½ çš„taskæ•°é‡å¢žåŠ åˆ°å¤šå°‘ï¼Œè¿™ä¸ªå¯¹åº”ç€100ä¸‡æ•°æ®çš„keyè‚¯å®šè¿˜æ˜¯ä¼šåˆ†é…åˆ°ä¸€ä¸ªtaskä¸­åŽ»å¤„ç†ï¼Œå› æ­¤æ³¨å®šè¿˜æ˜¯ä¼šå‘ç”Ÿæ•°æ®å€¾æ–œçš„ã€‚æ‰€ä»¥è¿™ç§æ–¹æ¡ˆåªèƒ½è¯´æ˜¯åœ¨å‘çŽ°æ•°æ®å€¾æ–œæ—¶å°è¯•ä½¿ç”¨çš„ä¸€ç§æ‰‹æ®µï¼Œå°è¯•åŽ»ç”¨æœ€ç®€å•çš„æ–¹æ³•ç¼“è§£æ•°æ®å€¾æ–œè€Œå·²ï¼Œæˆ–è€…æ˜¯å’Œå…¶ä»–æ–¹æ¡ˆç»“åˆèµ·æ¥ä½¿ç”¨ã€‚

åœ¨ç†æƒ³æƒ…å†µä¸‹ï¼Œreduceç«¯å¹¶è¡Œåº¦æå‡åŽï¼Œä¼šåœ¨ä¸€å®šç¨‹åº¦ä¸Šå‡è½»æ•°æ®å€¾æ–œçš„é—®é¢˜ï¼Œç”šè‡³åŸºæœ¬æ¶ˆé™¤æ•°æ®å€¾æ–œï¼›ä½†æ˜¯ï¼Œåœ¨ä¸€äº›æƒ…å†µä¸‹ï¼Œåªä¼šè®©åŽŸæ¥ç”±äºŽæ•°æ®å€¾æ–œè€Œè¿è¡Œç¼“æ…¢çš„taskè¿è¡Œé€Ÿåº¦ç¨æœ‰æå‡ï¼Œæˆ–è€…é¿å…äº†æŸäº›taskçš„OOMé—®é¢˜ï¼Œä½†æ˜¯ï¼Œä»ç„¶è¿è¡Œç¼“æ…¢ï¼Œæ­¤æ—¶ï¼Œè¦åŠæ—¶æ”¾å¼ƒæ–¹æ¡ˆä¸‰ï¼Œå¼€å§‹å°è¯•åŽé¢çš„æ–¹æ¡ˆã€‚

### 4. ä½¿ç”¨map join

æ­£å¸¸æƒ…å†µä¸‹ï¼Œjoinæ“ä½œéƒ½ä¼šæ‰§è¡Œshuffleè¿‡ç¨‹ï¼Œå¹¶ä¸”æ‰§è¡Œçš„æ˜¯reduce joinï¼Œä¹Ÿå°±æ˜¯å…ˆå°†æ‰€æœ‰ç›¸åŒçš„keyå’Œå¯¹åº”çš„valueæ±‡èšåˆ°ä¸€ä¸ªreduce taskä¸­ï¼Œç„¶åŽå†è¿›è¡Œjoinã€‚æ™®é€šjoinçš„è¿‡ç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClK5VKboavTe4gyf9LvH52QIic8vxNUgFTibym8vNXbb0k77wMJicOARn8Q/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)æ™®é€šjoinè¿‡ç¨‹

æ™®é€šçš„joinæ˜¯ä¼šèµ°shuffleè¿‡ç¨‹çš„ï¼Œè€Œä¸€æ—¦shuffleï¼Œå°±ç›¸å½“äºŽä¼šå°†ç›¸åŒkeyçš„æ•°æ®æ‹‰å–åˆ°ä¸€ä¸ªshuffle read taskä¸­å†è¿›è¡Œjoinï¼Œæ­¤æ—¶å°±æ˜¯reduce joinã€‚ä½†æ˜¯å¦‚æžœä¸€ä¸ªRDDæ˜¯æ¯”è¾ƒå°çš„ï¼Œåˆ™å¯ä»¥é‡‡ç”¨å¹¿æ’­å°RDDå…¨é‡æ•°æ®+mapç®—å­æ¥å®žçŽ°ä¸ŽjoinåŒæ ·çš„æ•ˆæžœï¼Œä¹Ÿå°±æ˜¯map joinï¼Œæ­¤æ—¶å°±ä¸ä¼šå‘ç”Ÿshuffleæ“ä½œï¼Œä¹Ÿå°±ä¸ä¼šå‘ç”Ÿæ•°æ®å€¾æ–œã€‚

> **æ³¨æ„ï¼šRDDæ˜¯å¹¶ä¸èƒ½ç›´æŽ¥è¿›è¡Œå¹¿æ’­çš„ï¼Œåªèƒ½å°†RDDå†…éƒ¨çš„æ•°æ®é€šè¿‡collectæ‹‰å–åˆ°Driverå†…å­˜ç„¶åŽå†è¿›è¡Œå¹¿æ’­**ã€‚

**1. æ ¸å¿ƒæ€è·¯ï¼š**

ä¸ä½¿ç”¨joinç®—å­è¿›è¡Œè¿žæŽ¥æ“ä½œï¼Œè€Œä½¿ç”¨broadcastå˜é‡ä¸Žmapç±»ç®—å­å®žçŽ°joinæ“ä½œï¼Œè¿›è€Œå®Œå…¨è§„é¿æŽ‰shuffleç±»çš„æ“ä½œï¼Œå½»åº•é¿å…æ•°æ®å€¾æ–œçš„å‘ç”Ÿå’Œå‡ºçŽ°ã€‚å°†è¾ƒå°RDDä¸­çš„æ•°æ®ç›´æŽ¥é€šè¿‡collectç®—å­æ‹‰å–åˆ°Driverç«¯çš„å†…å­˜ä¸­æ¥ï¼Œç„¶åŽå¯¹å…¶åˆ›å»ºä¸€ä¸ªbroadcastå˜é‡ï¼›æŽ¥ç€å¯¹å¦å¤–ä¸€ä¸ªRDDæ‰§è¡Œmapç±»ç®—å­ï¼Œåœ¨ç®—å­å‡½æ•°å†…ï¼Œä»Žbroadcastå˜é‡ä¸­èŽ·å–è¾ƒå°RDDçš„å…¨é‡æ•°æ®ï¼Œä¸Žå½“å‰RDDçš„æ¯ä¸€æ¡æ•°æ®æŒ‰ç…§è¿žæŽ¥keyè¿›è¡Œæ¯”å¯¹ï¼Œå¦‚æžœè¿žæŽ¥keyç›¸åŒçš„è¯ï¼Œé‚£ä¹ˆå°±å°†ä¸¤ä¸ªRDDçš„æ•°æ®ç”¨ä½ éœ€è¦çš„æ–¹å¼è¿žæŽ¥èµ·æ¥ã€‚

æ ¹æ®ä¸Šè¿°æ€è·¯ï¼Œæ ¹æœ¬ä¸ä¼šå‘ç”Ÿshuffleæ“ä½œï¼Œä»Žæ ¹æœ¬ä¸Šæœç»äº†joinæ“ä½œå¯èƒ½å¯¼è‡´çš„æ•°æ®å€¾æ–œé—®é¢˜ã€‚

å½“joinæ“ä½œæœ‰æ•°æ®å€¾æ–œé—®é¢˜å¹¶ä¸”å…¶ä¸­ä¸€ä¸ªRDDçš„æ•°æ®é‡è¾ƒå°æ—¶ï¼Œå¯ä»¥ä¼˜å…ˆè€ƒè™‘è¿™ç§æ–¹å¼ï¼Œæ•ˆæžœéžå¸¸å¥½ã€‚

map joinçš„è¿‡ç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClK5VKboavTe4gyf9LvH52QIic8vxNUgFTibym8vNXbb0k77wMJicOARn8Q/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)map joinè¿‡ç¨‹

**2. ä¸é€‚ç”¨åœºæ™¯åˆ†æžï¼š**

ç”±äºŽSparkçš„å¹¿æ’­å˜é‡æ˜¯åœ¨æ¯ä¸ªExecutorä¸­ä¿å­˜ä¸€ä¸ªå‰¯æœ¬ï¼Œå¦‚æžœä¸¤ä¸ªRDDæ•°æ®é‡éƒ½æ¯”è¾ƒå¤§ï¼Œé‚£ä¹ˆå¦‚æžœå°†ä¸€ä¸ªæ•°æ®é‡æ¯”è¾ƒå¤§çš„RDDåšæˆå¹¿æ’­å˜é‡ï¼Œé‚£ä¹ˆå¾ˆæœ‰å¯èƒ½ä¼šé€ æˆå†…å­˜æº¢å‡ºã€‚

## ä¹ã€Sparkæ€§èƒ½ä¼˜åŒ–

## Sparkè°ƒä¼˜ä¹‹RDDç®—å­è°ƒä¼˜

### 1. RDDå¤ç”¨

åœ¨å¯¹RDDè¿›è¡Œç®—å­æ—¶ï¼Œè¦é¿å…ç›¸åŒçš„ç®—å­å’Œè®¡ç®—é€»è¾‘ä¹‹ä¸‹å¯¹RDDè¿›è¡Œé‡å¤çš„è®¡ç®—ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClERkXibI16JFqccMs6cxiaAdjPQ8HrsWYHw3p0cbsQu9fnjE92MoicWE2w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)RDDçš„é‡å¤è®¡ç®—

å¯¹ä¸Šå›¾ä¸­çš„RDDè®¡ç®—æž¶æž„è¿›è¡Œä¿®æ”¹ï¼Œå¾—åˆ°å¦‚ä¸‹å›¾æ‰€ç¤ºçš„ä¼˜åŒ–ç»“æžœï¼š

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClicEDzeDwxibiadlSX9ibwl1MV4rv67IiaOuaKD7ahkAc20jIsS2RBJjKRHA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)RDDæž¶æž„ä¼˜åŒ–

### 2. å°½æ—©filter

èŽ·å–åˆ°åˆå§‹RDDåŽï¼Œåº”è¯¥è€ƒè™‘**å°½æ—©åœ°è¿‡æ»¤æŽ‰ä¸éœ€è¦çš„æ•°æ®**ï¼Œè¿›è€Œå‡å°‘å¯¹å†…å­˜çš„å ç”¨ï¼Œä»Žè€Œæå‡Sparkä½œä¸šçš„è¿è¡Œæ•ˆçŽ‡ã€‚

> æœ¬æ–‡é¦–å‘äºŽå…¬ä¼—å·ï¼šäº”åˆ†é’Ÿå­¦å¤§æ•°æ®ï¼Œæ¬¢è¿Žå›´è§‚ï¼å›žå¤ã€ä¹¦ç±ã€‘å³å¯èŽ·å¾—ä¸Šç™¾æœ¬å¤§æ•°æ®ä¹¦ç±

### 3. è¯»å–å¤§é‡å°æ–‡ä»¶-ç”¨wholeTextFiles

å½“æˆ‘ä»¬å°†ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶è¯»å–ä¸º RDD æ—¶ï¼Œè¾“å…¥çš„æ¯ä¸€è¡Œéƒ½ä¼šæˆä¸ºRDDçš„ä¸€ä¸ªå…ƒç´ ã€‚

ä¹Ÿå¯ä»¥å°†å¤šä¸ªå®Œæ•´çš„æ–‡æœ¬æ–‡ä»¶ä¸€æ¬¡æ€§è¯»å–ä¸ºä¸€ä¸ªpairRDDï¼Œå…¶ä¸­é”®æ˜¯æ–‡ä»¶åï¼Œå€¼æ˜¯æ–‡ä»¶å†…å®¹ã€‚

```
val input:RDD[String] = sc.textFile("dir/*.log") 
```

å¦‚æžœä¼ é€’ç›®å½•ï¼Œåˆ™å°†ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶è¯»å–ä½œä¸ºRDDã€‚æ–‡ä»¶è·¯å¾„æ”¯æŒé€šé…ç¬¦ã€‚

ä½†æ˜¯è¿™æ ·å¯¹äºŽå¤§é‡çš„å°æ–‡ä»¶è¯»å–æ•ˆçŽ‡å¹¶ä¸é«˜ï¼Œåº”è¯¥ä½¿ç”¨ **wholeTextFiles**è¿”å›žå€¼ä¸ºRDD[(String, String)]ï¼Œå…¶ä¸­Keyæ˜¯æ–‡ä»¶çš„åç§°ï¼ŒValueæ˜¯æ–‡ä»¶çš„å†…å®¹ã€‚

```
def wholeTextFiles(path: String, minPartitions: Int = defaultMinPartitions): RDD[(String, String)])
```

------

wholeTextFilesè¯»å–å°æ–‡ä»¶:

```
val filesRDD: RDD[(String, String)] =
sc.wholeTextFiles("D:\\data\\files", minPartitions = 3)
val linesRDD: RDD[String] = filesRDD.flatMap(_._2.split("\\r\\n"))
val wordsRDD: RDD[String] = linesRDD.flatMap(_.split(" "))
wordsRDD.map((_, 1)).reduceByKey(_ + _).collect().foreach(println)
```

### 4. mapPartitionå’ŒforeachPartition

- **mapPartitions**

map(_....)  è¡¨ç¤ºæ¯ä¸€ä¸ªå…ƒç´ 

mapPartitions(_....)  è¡¨ç¤ºæ¯ä¸ªåˆ†åŒºçš„æ•°æ®ç»„æˆçš„è¿­ä»£å™¨

æ™®é€šçš„mapç®—å­å¯¹RDDä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ è¿›è¡Œæ“ä½œï¼Œè€ŒmapPartitionsç®—å­å¯¹RDDä¸­æ¯ä¸€ä¸ªåˆ†åŒºè¿›è¡Œæ“ä½œã€‚

å¦‚æžœæ˜¯æ™®é€šçš„mapç®—å­ï¼Œå‡è®¾ä¸€ä¸ªpartitionæœ‰1ä¸‡æ¡æ•°æ®ï¼Œé‚£ä¹ˆmapç®—å­ä¸­çš„functionè¦æ‰§è¡Œ1ä¸‡æ¬¡ï¼Œä¹Ÿå°±æ˜¯å¯¹æ¯ä¸ªå…ƒç´ è¿›è¡Œæ“ä½œã€‚

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClO6Wwaz4gmv5DGVcugbT0EeCPsE5xYnaicMYJ6NgqCPib3UAwbwzENmLQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)map ç®—å­

å¦‚æžœæ˜¯mapPartitionç®—å­ï¼Œç”±äºŽä¸€ä¸ªtaskå¤„ç†ä¸€ä¸ªRDDçš„partitionï¼Œé‚£ä¹ˆä¸€ä¸ªtaskåªä¼šæ‰§è¡Œä¸€æ¬¡functionï¼Œfunctionä¸€æ¬¡æŽ¥æ”¶æ‰€æœ‰çš„partitionæ•°æ®ï¼Œæ•ˆçŽ‡æ¯”è¾ƒé«˜ã€‚

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClctVIQbaqwaZz49uowbRAJTUp7kXfuR0iaDXl0Cbqs0MLcAzjttwWmWA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)mapPartition ç®—å­

æ¯”å¦‚ï¼Œ**å½“è¦æŠŠRDDä¸­çš„æ‰€æœ‰æ•°æ®é€šè¿‡JDBCå†™å…¥æ•°æ®ï¼Œå¦‚æžœä½¿ç”¨mapç®—å­ï¼Œé‚£ä¹ˆéœ€è¦å¯¹RDDä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ éƒ½åˆ›å»ºä¸€ä¸ªæ•°æ®åº“è¿žæŽ¥ï¼Œè¿™æ ·å¯¹èµ„æºçš„æ¶ˆè€—å¾ˆå¤§ï¼Œå¦‚æžœä½¿ç”¨mapPartitionsç®—å­ï¼Œé‚£ä¹ˆé’ˆå¯¹ä¸€ä¸ªåˆ†åŒºçš„æ•°æ®ï¼Œåªéœ€è¦å»ºç«‹ä¸€ä¸ªæ•°æ®åº“è¿žæŽ¥**ã€‚

mapPartitionsç®—å­ä¹Ÿå­˜åœ¨ä¸€äº›ç¼ºç‚¹ï¼šå¯¹äºŽæ™®é€šçš„mapæ“ä½œï¼Œä¸€æ¬¡å¤„ç†ä¸€æ¡æ•°æ®ï¼Œå¦‚æžœåœ¨å¤„ç†äº†2000æ¡æ•°æ®åŽå†…å­˜ä¸è¶³ï¼Œé‚£ä¹ˆå¯ä»¥å°†å·²ç»å¤„ç†å®Œçš„2000æ¡æ•°æ®ä»Žå†…å­˜ä¸­åžƒåœ¾å›žæ”¶æŽ‰ï¼›ä½†æ˜¯å¦‚æžœä½¿ç”¨mapPartitionsç®—å­ï¼Œä½†æ•°æ®é‡éžå¸¸å¤§æ—¶ï¼Œfunctionä¸€æ¬¡å¤„ç†ä¸€ä¸ªåˆ†åŒºçš„æ•°æ®ï¼Œå¦‚æžœä¸€æ—¦å†…å­˜ä¸è¶³ï¼Œæ­¤æ—¶æ— æ³•å›žæ”¶å†…å­˜ï¼Œå°±å¯èƒ½ä¼šOOMï¼Œå³å†…å­˜æº¢å‡ºã€‚

å› æ­¤ï¼Œ**mapPartitionsç®—å­é€‚ç”¨äºŽæ•°æ®é‡ä¸æ˜¯ç‰¹åˆ«å¤§çš„æ—¶å€™ï¼Œæ­¤æ—¶ä½¿ç”¨mapPartitionsç®—å­å¯¹æ€§èƒ½çš„æå‡æ•ˆæžœè¿˜æ˜¯ä¸é”™çš„**ã€‚ï¼ˆå½“æ•°æ®é‡å¾ˆå¤§çš„æ—¶å€™ï¼Œä¸€æ—¦ä½¿ç”¨mapPartitionsç®—å­ï¼Œå°±ä¼šç›´æŽ¥OOMï¼‰

åœ¨é¡¹ç›®ä¸­ï¼Œåº”è¯¥é¦–å…ˆä¼°ç®—ä¸€ä¸‹RDDçš„æ•°æ®é‡ã€æ¯ä¸ªpartitionçš„æ•°æ®é‡ï¼Œä»¥åŠåˆ†é…ç»™æ¯ä¸ªExecutorçš„å†…å­˜èµ„æºï¼Œå¦‚æžœèµ„æºå…è®¸ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨mapPartitionsç®—å­ä»£æ›¿mapã€‚

- **foreachPartition**

rrd.foreache(_....) è¡¨ç¤ºæ¯ä¸€ä¸ªå…ƒç´ 

rrd.forPartitions(_....)  è¡¨ç¤ºæ¯ä¸ªåˆ†åŒºçš„æ•°æ®ç»„æˆçš„è¿­ä»£å™¨

åœ¨ç”Ÿäº§çŽ¯å¢ƒä¸­ï¼Œé€šå¸¸ä½¿ç”¨foreachPartitionç®—å­æ¥å®Œæˆæ•°æ®åº“çš„å†™å…¥ï¼Œé€šè¿‡foreachPartitionç®—å­çš„ç‰¹æ€§ï¼Œå¯ä»¥ä¼˜åŒ–å†™æ•°æ®åº“çš„æ€§èƒ½ã€‚

å¦‚æžœä½¿ç”¨foreachç®—å­å®Œæˆæ•°æ®åº“çš„æ“ä½œï¼Œç”±äºŽforeachç®—å­æ˜¯éåŽ†RDDçš„æ¯æ¡æ•°æ®ï¼Œå› æ­¤ï¼Œæ¯æ¡æ•°æ®éƒ½ä¼šå»ºç«‹ä¸€ä¸ªæ•°æ®åº“è¿žæŽ¥ï¼Œè¿™æ˜¯å¯¹èµ„æºçš„æžå¤§æµªè´¹ï¼Œå› æ­¤ï¼Œ**å¯¹äºŽå†™æ•°æ®åº“æ“ä½œï¼Œæˆ‘ä»¬åº”å½“ä½¿ç”¨foreachPartitionç®—å­**ã€‚

ä¸ŽmapPartitionsç®—å­éžå¸¸ç›¸ä¼¼ï¼ŒforeachPartitionæ˜¯å°†RDDçš„æ¯ä¸ªåˆ†åŒºä½œä¸ºéåŽ†å¯¹è±¡ï¼Œä¸€æ¬¡å¤„ç†ä¸€ä¸ªåˆ†åŒºçš„æ•°æ®ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æžœæ¶‰åŠæ•°æ®åº“çš„ç›¸å…³æ“ä½œï¼Œä¸€ä¸ªåˆ†åŒºçš„æ•°æ®åªéœ€è¦åˆ›å»ºä¸€æ¬¡æ•°æ®åº“è¿žæŽ¥ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![å›¾ç‰‡](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)foreachPartition ç®—å­

ä½¿ç”¨äº†foreachPartition ç®—å­åŽï¼Œå¯ä»¥èŽ·å¾—ä»¥ä¸‹çš„æ€§èƒ½æå‡ï¼š

1. å¯¹äºŽæˆ‘ä»¬å†™çš„functionå‡½æ•°ï¼Œä¸€æ¬¡å¤„ç†ä¸€æ•´ä¸ªåˆ†åŒºçš„æ•°æ®ï¼›
2. å¯¹äºŽä¸€ä¸ªåˆ†åŒºå†…çš„æ•°æ®ï¼Œåˆ›å»ºå”¯ä¸€çš„æ•°æ®åº“è¿žæŽ¥ï¼›
3. åªéœ€è¦å‘æ•°æ®åº“å‘é€ä¸€æ¬¡SQLè¯­å¥å’Œå¤šç»„å‚æ•°ï¼›

**åœ¨ç”Ÿäº§çŽ¯å¢ƒä¸­ï¼Œå…¨éƒ¨éƒ½ä¼šä½¿ç”¨foreachPartitionç®—å­å®Œæˆæ•°æ®åº“æ“ä½œã€‚foreachPartitionç®—å­å­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼Œä¸ŽmapPartitionsç®—å­ç±»ä¼¼ï¼Œå¦‚æžœä¸€ä¸ªåˆ†åŒºçš„æ•°æ®é‡ç‰¹åˆ«å¤§ï¼Œå¯èƒ½ä¼šé€ æˆOOMï¼Œå³å†…å­˜æº¢å‡º**ã€‚

### 5. filter+coalesce/repartition(å‡å°‘åˆ†åŒº)

åœ¨Sparkä»»åŠ¡ä¸­æˆ‘ä»¬ç»å¸¸ä¼šä½¿ç”¨filterç®—å­å®ŒæˆRDDä¸­æ•°æ®çš„è¿‡æ»¤ï¼Œåœ¨ä»»åŠ¡åˆå§‹é˜¶æ®µï¼Œä»Žå„ä¸ªåˆ†åŒºä¸­åŠ è½½åˆ°çš„æ•°æ®é‡æ˜¯ç›¸è¿‘çš„ï¼Œä½†æ˜¯ä¸€æ—¦è¿›è¿‡filterè¿‡æ»¤åŽï¼Œæ¯ä¸ªåˆ†åŒºçš„æ•°æ®é‡æœ‰å¯èƒ½ä¼šå­˜åœ¨è¾ƒå¤§å·®å¼‚ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![å›¾ç‰‡](data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==)åˆ†åŒºæ•°æ®è¿‡æ»¤ç»“æžœ

æ ¹æ®ä¸Šå›¾æˆ‘ä»¬å¯ä»¥å‘çŽ°ä¸¤ä¸ªé—®é¢˜ï¼š

1. æ¯ä¸ªpartitionçš„æ•°æ®é‡å˜å°äº†ï¼Œå¦‚æžœè¿˜æŒ‰ç…§ä¹‹å‰ä¸Žpartitionç›¸ç­‰çš„taskä¸ªæ•°åŽ»å¤„ç†å½“å‰æ•°æ®ï¼Œæœ‰ç‚¹æµªè´¹taskçš„è®¡ç®—èµ„æºï¼›
2. æ¯ä¸ªpartitionçš„æ•°æ®é‡ä¸ä¸€æ ·ï¼Œä¼šå¯¼è‡´åŽé¢çš„æ¯ä¸ªtaskå¤„ç†æ¯ä¸ªpartitionæ•°æ®çš„æ—¶å€™ï¼Œæ¯ä¸ªtaskè¦å¤„ç†çš„æ•°æ®é‡ä¸åŒï¼Œè¿™å¾ˆæœ‰å¯èƒ½å¯¼è‡´æ•°æ®å€¾æ–œé—®é¢˜ã€‚

å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œç¬¬äºŒä¸ªåˆ†åŒºçš„æ•°æ®è¿‡æ»¤åŽåªå‰©100æ¡ï¼Œè€Œç¬¬ä¸‰ä¸ªåˆ†åŒºçš„æ•°æ®è¿‡æ»¤åŽå‰©ä¸‹800æ¡ï¼Œåœ¨ç›¸åŒçš„å¤„ç†é€»è¾‘ä¸‹ï¼Œç¬¬äºŒä¸ªåˆ†åŒºå¯¹åº”çš„taskå¤„ç†çš„æ•°æ®é‡ä¸Žç¬¬ä¸‰ä¸ªåˆ†åŒºå¯¹åº”çš„taskå¤„ç†çš„æ•°æ®é‡å·®è·è¾¾åˆ°äº†8å€ï¼Œè¿™ä¹Ÿä¼šå¯¼è‡´è¿è¡Œé€Ÿåº¦å¯èƒ½å­˜åœ¨æ•°å€çš„å·®è·ï¼Œè¿™ä¹Ÿå°±æ˜¯**æ•°æ®å€¾æ–œé—®é¢˜**ã€‚

é’ˆå¯¹ä¸Šè¿°çš„ä¸¤ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬åˆ†åˆ«è¿›è¡Œåˆ†æžï¼š

1. é’ˆå¯¹ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œæ—¢ç„¶åˆ†åŒºçš„æ•°æ®é‡å˜å°äº†ï¼Œæˆ‘ä»¬å¸Œæœ›å¯ä»¥å¯¹åˆ†åŒºæ•°æ®è¿›è¡Œé‡æ–°åˆ†é…ï¼Œæ¯”å¦‚å°†åŽŸæ¥4ä¸ªåˆ†åŒºçš„æ•°æ®è½¬åŒ–åˆ°2ä¸ªåˆ†åŒºä¸­ï¼Œè¿™æ ·åªéœ€è¦ç”¨åŽé¢çš„ä¸¤ä¸ªtaskè¿›è¡Œå¤„ç†å³å¯ï¼Œé¿å…äº†èµ„æºçš„æµªè´¹ã€‚
2. é’ˆå¯¹ç¬¬äºŒä¸ªé—®é¢˜ï¼Œè§£å†³æ–¹æ³•å’Œç¬¬ä¸€ä¸ªé—®é¢˜çš„è§£å†³æ–¹æ³•éžå¸¸ç›¸ä¼¼ï¼Œå¯¹åˆ†åŒºæ•°æ®é‡æ–°åˆ†é…ï¼Œè®©æ¯ä¸ªpartitionä¸­çš„æ•°æ®é‡å·®ä¸å¤šï¼Œè¿™å°±é¿å…äº†æ•°æ®å€¾æ–œé—®é¢˜ã€‚

é‚£ä¹ˆå…·ä½“åº”è¯¥å¦‚ä½•å®žçŽ°ä¸Šé¢çš„è§£å†³æ€è·¯ï¼Ÿæˆ‘ä»¬éœ€è¦coalesceç®—å­ã€‚

repartitionä¸Žcoalesceéƒ½å¯ä»¥ç”¨æ¥è¿›è¡Œé‡åˆ†åŒºï¼Œå…¶ä¸­repartitionåªæ˜¯coalesceæŽ¥å£ä¸­shuffleä¸ºtrueçš„ç®€æ˜“å®žçŽ°ï¼Œcoalesceé»˜è®¤æƒ…å†µä¸‹ä¸è¿›è¡Œshuffleï¼Œä½†æ˜¯å¯ä»¥é€šè¿‡å‚æ•°è¿›è¡Œè®¾ç½®ã€‚

å‡è®¾æˆ‘ä»¬å¸Œæœ›å°†åŽŸæœ¬çš„åˆ†åŒºä¸ªæ•°Aé€šè¿‡é‡æ–°åˆ†åŒºå˜ä¸ºBï¼Œé‚£ä¹ˆæœ‰ä»¥ä¸‹å‡ ç§æƒ…å†µï¼š

1. A > Bï¼ˆå¤šæ•°åˆ†åŒºåˆå¹¶ä¸ºå°‘æ•°åˆ†åŒºï¼‰

2. - Aä¸ŽBç›¸å·®å€¼ä¸å¤§

     æ­¤æ—¶ä½¿ç”¨coalesceå³å¯ï¼Œæ— éœ€shuffleè¿‡ç¨‹ã€‚

   - Aä¸ŽBç›¸å·®å€¼å¾ˆå¤§

     æ­¤æ—¶å¯ä»¥ä½¿ç”¨coalesceå¹¶ä¸”ä¸å¯ç”¨shuffleè¿‡ç¨‹ï¼Œä½†æ˜¯ä¼šå¯¼è‡´åˆå¹¶è¿‡ç¨‹æ€§èƒ½ä½Žä¸‹ï¼Œæ‰€ä»¥æŽ¨èè®¾ç½®coalesceçš„ç¬¬äºŒä¸ªå‚æ•°ä¸ºtrueï¼Œå³å¯åŠ¨shuffleè¿‡ç¨‹ã€‚

3. A < Bï¼ˆå°‘æ•°åˆ†åŒºåˆ†è§£ä¸ºå¤šæ•°åˆ†åŒºï¼‰

æ­¤æ—¶ä½¿ç”¨repartitionå³å¯ï¼Œå¦‚æžœä½¿ç”¨coalesceéœ€è¦å°†shuffleè®¾ç½®ä¸ºtrueï¼Œå¦åˆ™coalesceæ— æ•ˆã€‚

**æˆ‘ä»¬å¯ä»¥åœ¨filteræ“ä½œä¹‹åŽï¼Œä½¿ç”¨coalesceç®—å­é’ˆå¯¹æ¯ä¸ªpartitionçš„æ•°æ®é‡å„ä¸ç›¸åŒçš„æƒ…å†µï¼ŒåŽ‹ç¼©partitionçš„æ•°é‡ï¼Œè€Œä¸”è®©æ¯ä¸ªpartitionçš„æ•°æ®é‡å°½é‡å‡åŒ€ç´§å‡‘ï¼Œä»¥ä¾¿äºŽåŽé¢çš„taskè¿›è¡Œè®¡ç®—æ“ä½œï¼Œåœ¨æŸç§ç¨‹åº¦ä¸Šèƒ½å¤Ÿåœ¨ä¸€å®šç¨‹åº¦ä¸Šæå‡æ€§èƒ½**ã€‚

æ³¨æ„ï¼šlocalæ¨¡å¼æ˜¯è¿›ç¨‹å†…æ¨¡æ‹Ÿé›†ç¾¤è¿è¡Œï¼Œå·²ç»å¯¹å¹¶è¡Œåº¦å’Œåˆ†åŒºæ•°é‡æœ‰äº†ä¸€å®šçš„å†…éƒ¨ä¼˜åŒ–ï¼Œå› æ­¤ä¸ç”¨åŽ»è®¾ç½®å¹¶è¡Œåº¦å’Œåˆ†åŒºæ•°é‡ã€‚

### 6. å¹¶è¡Œåº¦è®¾ç½®

**Sparkä½œä¸šä¸­çš„å¹¶è¡Œåº¦æŒ‡å„ä¸ªstageçš„taskçš„æ•°é‡**ã€‚

å¦‚æžœå¹¶è¡Œåº¦è®¾ç½®ä¸åˆç†è€Œå¯¼è‡´å¹¶è¡Œåº¦è¿‡ä½Žï¼Œä¼šå¯¼è‡´èµ„æºçš„æžå¤§æµªè´¹ï¼Œä¾‹å¦‚ï¼Œ20ä¸ªExecutorï¼Œæ¯ä¸ªExecutoråˆ†é…3ä¸ªCPU coreï¼Œè€ŒSparkä½œä¸šæœ‰40ä¸ªtaskï¼Œè¿™æ ·æ¯ä¸ªExecutoråˆ†é…åˆ°çš„taskä¸ªæ•°æ˜¯2ä¸ªï¼Œè¿™å°±ä½¿å¾—æ¯ä¸ªExecutoræœ‰ä¸€ä¸ªCPU coreç©ºé—²ï¼Œå¯¼è‡´èµ„æºçš„æµªè´¹ã€‚

ç†æƒ³çš„å¹¶è¡Œåº¦è®¾ç½®ï¼Œåº”è¯¥æ˜¯è®©å¹¶è¡Œåº¦ä¸Žèµ„æºç›¸åŒ¹é…ï¼Œç®€å•æ¥è¯´å°±æ˜¯åœ¨èµ„æºå…è®¸çš„å‰æä¸‹ï¼Œå¹¶è¡Œåº¦è¦è®¾ç½®çš„å°½å¯èƒ½å¤§ï¼Œè¾¾åˆ°å¯ä»¥å……åˆ†åˆ©ç”¨é›†ç¾¤èµ„æºã€‚åˆç†çš„è®¾ç½®å¹¶è¡Œåº¦ï¼Œå¯ä»¥æå‡æ•´ä¸ªSparkä½œä¸šçš„æ€§èƒ½å’Œè¿è¡Œé€Ÿåº¦ã€‚

Sparkå®˜æ–¹æŽ¨èï¼Œ**taskæ•°é‡åº”è¯¥è®¾ç½®ä¸ºSparkä½œä¸šæ€»CPU coreæ•°é‡çš„2~3å€**ã€‚ä¹‹æ‰€ä»¥æ²¡æœ‰æŽ¨ètaskæ•°é‡ä¸ŽCPU coreæ€»æ•°ç›¸ç­‰ï¼Œæ˜¯å› ä¸ºtaskçš„æ‰§è¡Œæ—¶é—´ä¸åŒï¼Œæœ‰çš„taskæ‰§è¡Œé€Ÿåº¦å¿«è€Œæœ‰çš„taskæ‰§è¡Œé€Ÿåº¦æ…¢ï¼Œå¦‚æžœtaskæ•°é‡ä¸ŽCPU coreæ€»æ•°ç›¸ç­‰ï¼Œé‚£ä¹ˆæ‰§è¡Œå¿«çš„taskæ‰§è¡Œå®ŒæˆåŽï¼Œä¼šå‡ºçŽ°CPU coreç©ºé—²çš„æƒ…å†µã€‚å¦‚æžœtaskæ•°é‡è®¾ç½®ä¸ºCPU coreæ€»æ•°çš„2~3å€ï¼Œé‚£ä¹ˆä¸€ä¸ªtaskæ‰§è¡Œå®Œæ¯•åŽï¼ŒCPU coreä¼šç«‹åˆ»æ‰§è¡Œä¸‹ä¸€ä¸ªtaskï¼Œé™ä½Žäº†èµ„æºçš„æµªè´¹ï¼ŒåŒæ—¶æå‡äº†Sparkä½œä¸šè¿è¡Œçš„æ•ˆçŽ‡ã€‚

Sparkä½œä¸šå¹¶è¡Œåº¦çš„è®¾ç½®å¦‚ä¸‹ï¼š

```
val conf = new SparkConf().set("spark.default.parallelism", "500")
```

åŽŸåˆ™ï¼š**è®© cpu çš„ Coreï¼ˆcpu æ ¸å¿ƒæ•°ï¼‰ å……åˆ†åˆ©ç”¨èµ·æ¥ï¼Œ å¦‚æœ‰100ä¸ª Core,é‚£ä¹ˆå¹¶è¡Œåº¦å¯ä»¥è®¾ç½®ä¸º200~300**ã€‚

### 7. repartition/coalesceè°ƒèŠ‚å¹¶è¡Œåº¦

æˆ‘ä»¬çŸ¥é“ Spark ä¸­æœ‰å¹¶è¡Œåº¦çš„è°ƒèŠ‚ç­–ç•¥ï¼Œä½†æ˜¯ï¼Œ**å¹¶è¡Œåº¦çš„è®¾ç½®å¯¹äºŽSpark SQLæ˜¯ä¸ç”Ÿæ•ˆçš„ï¼Œç”¨æˆ·è®¾ç½®çš„å¹¶è¡Œåº¦åªå¯¹äºŽSpark SQLä»¥å¤–çš„æ‰€æœ‰Sparkçš„stageç”Ÿæ•ˆ**ã€‚

Spark SQLçš„å¹¶è¡Œåº¦ä¸å…è®¸ç”¨æˆ·è‡ªå·±æŒ‡å®šï¼ŒSpark SQLè‡ªå·±ä¼šé»˜è®¤æ ¹æ®hiveè¡¨å¯¹åº”çš„HDFSæ–‡ä»¶çš„splitä¸ªæ•°è‡ªåŠ¨è®¾ç½®Spark SQLæ‰€åœ¨çš„é‚£ä¸ªstageçš„å¹¶è¡Œåº¦ï¼Œç”¨æˆ·è‡ªå·±é€š **spark.default.parallelism** å‚æ•°æŒ‡å®šçš„å¹¶è¡Œåº¦ï¼Œåªä¼šåœ¨æ²¡Spark SQLçš„stageä¸­ç”Ÿæ•ˆã€‚

ç”±äºŽSpark SQLæ‰€åœ¨stageçš„å¹¶è¡Œåº¦æ— æ³•æ‰‹åŠ¨è®¾ç½®ï¼Œå¦‚æžœæ•°æ®é‡è¾ƒå¤§ï¼Œå¹¶ä¸”æ­¤stageä¸­åŽç»­çš„transformationæ“ä½œæœ‰ç€å¤æ‚çš„ä¸šåŠ¡é€»è¾‘ï¼Œè€ŒSpark SQLè‡ªåŠ¨è®¾ç½®çš„taskæ•°é‡å¾ˆå°‘ï¼Œè¿™å°±æ„å‘³ç€æ¯ä¸ªtaskè¦å¤„ç†ä¸ºæ•°ä¸å°‘çš„æ•°æ®é‡ï¼Œç„¶åŽè¿˜è¦æ‰§è¡Œéžå¸¸å¤æ‚çš„å¤„ç†é€»è¾‘ï¼Œè¿™å°±å¯èƒ½è¡¨çŽ°ä¸ºç¬¬ä¸€ä¸ªæœ‰Spark SQLçš„stageé€Ÿåº¦å¾ˆæ…¢ï¼Œè€ŒåŽç»­çš„æ²¡æœ‰Spark SQLçš„stageè¿è¡Œé€Ÿåº¦éžå¸¸å¿«ã€‚

ä¸ºäº†è§£å†³Spark SQLæ— æ³•è®¾ç½®å¹¶è¡Œåº¦å’Œtaskæ•°é‡çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨repartitionç®—å­ã€‚

repartition ç®—å­ä½¿ç”¨å‰åŽå¯¹æ¯”å›¾å¦‚ä¸‹ï¼š

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClluuQGrck6TicLu0Xo0hZc9T9RUhPib1YCGHrfCrWtm8R6k2862AoEwPg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)repartition ç®—å­ä½¿ç”¨å‰åŽå¯¹æ¯”å›¾

**Spark SQLè¿™ä¸€æ­¥çš„å¹¶è¡Œåº¦å’Œtaskæ•°é‡è‚¯å®šæ˜¯æ²¡æœ‰åŠžæ³•åŽ»æ”¹å˜äº†ï¼Œä½†æ˜¯ï¼Œå¯¹äºŽSpark SQLæŸ¥è¯¢å‡ºæ¥çš„RDDï¼Œç«‹å³ä½¿ç”¨repartitionç®—å­ï¼ŒåŽ»é‡æ–°è¿›è¡Œåˆ†åŒºï¼Œè¿™æ ·å¯ä»¥é‡æ–°åˆ†åŒºä¸ºå¤šä¸ªpartitionï¼Œä»Žrepartitionä¹‹åŽçš„RDDæ“ä½œï¼Œç”±äºŽä¸å†æ¶‰åŠSpark SQLï¼Œå› æ­¤stageçš„å¹¶è¡Œåº¦å°±ä¼šç­‰äºŽä½ æ‰‹åŠ¨è®¾ç½®çš„å€¼ï¼Œè¿™æ ·å°±é¿å…äº†Spark SQLæ‰€åœ¨çš„stageåªèƒ½ç”¨å°‘é‡çš„taskåŽ»å¤„ç†å¤§é‡æ•°æ®å¹¶æ‰§è¡Œå¤æ‚çš„ç®—æ³•é€»è¾‘ã€‚ä½¿ç”¨repartitionç®—å­çš„å‰åŽå¯¹æ¯”å¦‚ä¸Šå›¾æ‰€ç¤º**ã€‚

### 8. reduceByKeyæœ¬åœ°é¢„èšåˆ

**reduceByKeyç›¸è¾ƒäºŽæ™®é€šçš„shuffleæ“ä½œä¸€ä¸ªæ˜¾è‘—çš„ç‰¹ç‚¹å°±æ˜¯ä¼šè¿›è¡Œmapç«¯çš„æœ¬åœ°èšåˆ**ï¼Œmapç«¯ä¼šå…ˆå¯¹æœ¬åœ°çš„æ•°æ®è¿›è¡Œcombineæ“ä½œï¼Œç„¶åŽå°†æ•°æ®å†™å…¥ç»™ä¸‹ä¸ªstageçš„æ¯ä¸ªtaskåˆ›å»ºçš„æ–‡ä»¶ä¸­ï¼Œä¹Ÿå°±æ˜¯åœ¨mapç«¯ï¼Œå¯¹æ¯ä¸€ä¸ªkeyå¯¹åº”çš„valueï¼Œæ‰§è¡ŒreduceByKeyç®—å­å‡½æ•°ã€‚

reduceByKeyç®—å­çš„æ‰§è¡Œè¿‡ç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClWK5nAJBCGibKRhRoWY4MO9rSkwBhFVq39ib71Batnib28JPMvOhiaPFClg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)reduceByKey ç®—å­æ‰§è¡Œè¿‡ç¨‹

ä½¿ç”¨reduceByKeyå¯¹æ€§èƒ½çš„æå‡å¦‚ä¸‹ï¼š

1. æœ¬åœ°èšåˆåŽï¼Œåœ¨mapç«¯çš„æ•°æ®é‡å˜å°‘ï¼Œå‡å°‘äº†ç£ç›˜IOï¼Œä¹Ÿå‡å°‘äº†å¯¹ç£ç›˜ç©ºé—´çš„å ç”¨ï¼›
2. æœ¬åœ°èšåˆåŽï¼Œä¸‹ä¸€ä¸ªstageæ‹‰å–çš„æ•°æ®é‡å˜å°‘ï¼Œå‡å°‘äº†ç½‘ç»œä¼ è¾“çš„æ•°æ®é‡ï¼›
3. æœ¬åœ°èšåˆåŽï¼Œåœ¨reduceç«¯è¿›è¡Œæ•°æ®ç¼“å­˜çš„å†…å­˜å ç”¨å‡å°‘ï¼›
4. æœ¬åœ°èšåˆåŽï¼Œåœ¨reduceç«¯è¿›è¡Œèšåˆçš„æ•°æ®é‡å‡å°‘ã€‚

åŸºäºŽreduceByKeyçš„æœ¬åœ°èšåˆç‰¹å¾ï¼Œæˆ‘ä»¬åº”è¯¥è€ƒè™‘ä½¿ç”¨reduceByKeyä»£æ›¿å…¶ä»–çš„shuffleç®—å­ï¼Œä¾‹å¦‚groupByKeyã€‚

groupByKeyä¸ŽreduceByKeyçš„è¿è¡ŒåŽŸç†å¦‚ä¸‹å›¾1å’Œå›¾2æ‰€ç¤ºï¼š

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClL71POlOW4vRLbcEsNu7EKB4leWZ1nMTfkGVOspNn77MNegVOuR7yLA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)å›¾1ï¼šgroupByKeyåŽŸç†

![å›¾ç‰‡](https://mmbiz.qpic.cn/sz_mmbiz_png/ZubDbBye0zFsrnOGLdJAJn2Ul3LY8BClxZFwd0rn6rHjxuHEBuiaGMCnVa2LWrvIcibJpKR4VG2ptSKbrobvK1Pg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)å›¾2ï¼šreduceByKeyåŽŸç†

æ ¹æ®ä¸Šå›¾å¯çŸ¥ï¼ŒgroupByKeyä¸ä¼šè¿›è¡Œmapç«¯çš„èšåˆï¼Œè€Œæ˜¯å°†æ‰€æœ‰mapç«¯çš„æ•°æ®shuffleåˆ°reduceç«¯ï¼Œç„¶åŽåœ¨reduceç«¯è¿›è¡Œæ•°æ®çš„èšåˆæ“ä½œã€‚ç”±äºŽreduceByKeyæœ‰mapç«¯èšåˆçš„ç‰¹æ€§ï¼Œä½¿å¾—ç½‘ç»œä¼ è¾“çš„æ•°æ®é‡å‡å°ï¼Œå› æ­¤æ•ˆçŽ‡è¦æ˜Žæ˜¾é«˜äºŽgroupByKeyã€‚

### 9. ä½¿ç”¨æŒä¹…åŒ–+checkpoint

SparkæŒä¹…åŒ–åœ¨å¤§éƒ¨åˆ†æƒ…å†µä¸‹æ˜¯æ²¡æœ‰é—®é¢˜çš„ï¼Œä½†æ˜¯æœ‰æ—¶æ•°æ®å¯èƒ½ä¼šä¸¢å¤±ï¼Œå¦‚æžœæ•°æ®ä¸€æ—¦ä¸¢å¤±ï¼Œå°±éœ€è¦å¯¹ä¸¢å¤±çš„æ•°æ®é‡æ–°è¿›è¡Œè®¡ç®—ï¼Œè®¡ç®—å®ŒåŽå†ç¼“å­˜å’Œä½¿ç”¨ï¼Œä¸ºäº†é¿å…æ•°æ®çš„ä¸¢å¤±ï¼Œå¯ä»¥é€‰æ‹©å¯¹è¿™ä¸ªRDDè¿›è¡Œcheckpointï¼Œä¹Ÿå°±æ˜¯**å°†æ•°æ®æŒä¹…åŒ–ä¸€ä»½åˆ°å®¹é”™çš„æ–‡ä»¶ç³»ç»Ÿä¸Šï¼ˆæ¯”å¦‚HDFSï¼‰**ã€‚

ä¸€ä¸ªRDDç¼“å­˜å¹¶checkpointåŽï¼Œå¦‚æžœä¸€æ—¦å‘çŽ°ç¼“å­˜ä¸¢å¤±ï¼Œå°±ä¼šä¼˜å…ˆæŸ¥çœ‹checkpointæ•°æ®å­˜ä¸å­˜åœ¨ï¼Œå¦‚æžœæœ‰ï¼Œå°±ä¼šä½¿ç”¨checkpointæ•°æ®ï¼Œè€Œä¸ç”¨é‡æ–°è®¡ç®—ã€‚ä¹Ÿå³æ˜¯è¯´ï¼Œcheckpointå¯ä»¥è§†ä¸ºcacheçš„ä¿éšœæœºåˆ¶ï¼Œå¦‚æžœcacheå¤±è´¥ï¼Œå°±ä½¿ç”¨checkpointçš„æ•°æ®ã€‚

ä½¿ç”¨checkpointçš„**ä¼˜ç‚¹åœ¨äºŽæé«˜äº†Sparkä½œä¸šçš„å¯é æ€§ï¼Œä¸€æ—¦ç¼“å­˜å‡ºçŽ°é—®é¢˜ï¼Œä¸éœ€è¦é‡æ–°è®¡ç®—æ•°æ®ï¼Œç¼ºç‚¹åœ¨äºŽï¼Œcheckpointæ—¶éœ€è¦å°†æ•°æ®å†™å…¥HDFSç­‰æ–‡ä»¶ç³»ç»Ÿï¼Œå¯¹æ€§èƒ½çš„æ¶ˆè€—è¾ƒå¤§**ã€‚

æŒä¹…åŒ–è®¾ç½®å¦‚ä¸‹ï¼š

```
sc.setCheckpointDir(â€˜HDFSâ€™)
rdd.cache/persist(memory_and_disk)
rdd.checkpoint
```

### 10. ä½¿ç”¨å¹¿æ’­å˜é‡

é»˜è®¤æƒ…å†µä¸‹ï¼Œtaskä¸­çš„ç®—å­ä¸­å¦‚æžœä½¿ç”¨äº†å¤–éƒ¨çš„å˜é‡ï¼Œæ¯ä¸ªtaskéƒ½ä¼šèŽ·å–ä¸€ä»½å˜é‡çš„å¤æœ¬ï¼Œè¿™å°±é€ æˆäº†å†…å­˜çš„æžå¤§æ¶ˆè€—ã€‚ä¸€æ–¹é¢ï¼Œå¦‚æžœåŽç»­å¯¹RDDè¿›è¡ŒæŒä¹…åŒ–ï¼Œå¯èƒ½å°±æ— æ³•å°†RDDæ•°æ®å­˜å…¥å†…å­˜ï¼Œåªèƒ½å†™å…¥ç£ç›˜ï¼Œç£ç›˜IOå°†ä¼šä¸¥é‡æ¶ˆè€—æ€§èƒ½ï¼›å¦ä¸€æ–¹é¢ï¼Œtaskåœ¨åˆ›å»ºå¯¹è±¡çš„æ—¶å€™ï¼Œä¹Ÿè®¸ä¼šå‘çŽ°å †å†…å­˜æ— æ³•å­˜æ”¾æ–°åˆ›å»ºçš„å¯¹è±¡ï¼Œè¿™å°±ä¼šå¯¼è‡´é¢‘ç¹çš„GCï¼ŒGCä¼šå¯¼è‡´å·¥ä½œçº¿ç¨‹åœæ­¢ï¼Œè¿›è€Œå¯¼è‡´Sparkæš‚åœå·¥ä½œä¸€æ®µæ—¶é—´ï¼Œä¸¥é‡å½±å“Sparkæ€§èƒ½ã€‚

å‡è®¾å½“å‰ä»»åŠ¡é…ç½®äº†20ä¸ªExecutorï¼ŒæŒ‡å®š500ä¸ªtaskï¼Œæœ‰ä¸€ä¸ª20Mçš„å˜é‡è¢«æ‰€æœ‰taskå…±ç”¨ï¼Œæ­¤æ—¶ä¼šåœ¨500ä¸ªtaskä¸­äº§ç”Ÿ500ä¸ªå‰¯æœ¬ï¼Œè€—è´¹é›†ç¾¤10Gçš„å†…å­˜ï¼Œå¦‚æžœä½¿ç”¨äº†å¹¿æ’­å˜é‡ï¼Œ é‚£ä¹ˆæ¯ä¸ªExecutorä¿å­˜ä¸€ä¸ªå‰¯æœ¬ï¼Œä¸€å…±æ¶ˆè€—400Må†…å­˜ï¼Œå†…å­˜æ¶ˆè€—å‡å°‘äº†5å€ã€‚

å¹¿æ’­å˜é‡åœ¨æ¯ä¸ªExecutorä¿å­˜ä¸€ä¸ªå‰¯æœ¬ï¼Œæ­¤Executorçš„æ‰€æœ‰taskå…±ç”¨æ­¤å¹¿æ’­å˜é‡ï¼Œè¿™è®©å˜é‡äº§ç”Ÿçš„å‰¯æœ¬æ•°é‡å¤§å¤§å‡å°‘ã€‚

åœ¨åˆå§‹é˜¶æ®µï¼Œå¹¿æ’­å˜é‡åªåœ¨Driverä¸­æœ‰ä¸€ä»½å‰¯æœ¬ã€‚taskåœ¨è¿è¡Œçš„æ—¶å€™ï¼Œæƒ³è¦ä½¿ç”¨å¹¿æ’­å˜é‡ä¸­çš„æ•°æ®ï¼Œæ­¤æ—¶é¦–å…ˆä¼šåœ¨è‡ªå·±æœ¬åœ°çš„Executorå¯¹åº”çš„BlockManagerä¸­å°è¯•èŽ·å–å˜é‡ï¼Œå¦‚æžœæœ¬åœ°æ²¡æœ‰ï¼ŒBlockManagerå°±ä¼šä»ŽDriveræˆ–è€…å…¶ä»–èŠ‚ç‚¹çš„BlockManagerä¸Šè¿œç¨‹æ‹‰å–å˜é‡çš„å¤æœ¬ï¼Œå¹¶ç”±æœ¬åœ°çš„BlockManagerè¿›è¡Œç®¡ç†ï¼›ä¹‹åŽæ­¤Executorçš„æ‰€æœ‰taskéƒ½ä¼šç›´æŽ¥ä»Žæœ¬åœ°çš„BlockManagerä¸­èŽ·å–å˜é‡ã€‚

å¯¹äºŽå¤šä¸ªTaskå¯èƒ½ä¼šå…±ç”¨çš„æ•°æ®å¯ä»¥å¹¿æ’­åˆ°æ¯ä¸ªExecutorä¸Šï¼š

```
val å¹¿æ’­å˜é‡å= sc.broadcast(ä¼šè¢«å„ä¸ªTaskç”¨åˆ°çš„å˜é‡,å³éœ€è¦å¹¿æ’­çš„å˜é‡)

å¹¿æ’­å˜é‡å.value//èŽ·å–å¹¿æ’­å˜é‡
```

### 11. ä½¿ç”¨Kryoåºåˆ—åŒ–

é»˜è®¤æƒ…å†µä¸‹ï¼ŒSparkä½¿ç”¨Javaçš„åºåˆ—åŒ–æœºåˆ¶ã€‚Javaçš„åºåˆ—åŒ–æœºåˆ¶ä½¿ç”¨æ–¹ä¾¿ï¼Œä¸éœ€è¦é¢å¤–çš„é…ç½®ï¼Œåœ¨ç®—å­ä¸­ä½¿ç”¨çš„å˜é‡å®žçŽ°SerializableæŽ¥å£å³å¯ï¼Œä½†æ˜¯ï¼ŒJavaåºåˆ—åŒ–æœºåˆ¶çš„æ•ˆçŽ‡ä¸é«˜ï¼Œåºåˆ—åŒ–é€Ÿåº¦æ…¢å¹¶ä¸”åºåˆ—åŒ–åŽçš„æ•°æ®æ‰€å ç”¨çš„ç©ºé—´ä¾ç„¶è¾ƒå¤§ã€‚

Sparkå®˜æ–¹å®£ç§°Kryoåºåˆ—åŒ–æœºåˆ¶æ¯”Javaåºåˆ—åŒ–æœºåˆ¶**æ€§èƒ½æé«˜10å€å·¦å³**ï¼ŒSparkä¹‹æ‰€ä»¥æ²¡æœ‰é»˜è®¤ä½¿ç”¨Kryoä½œä¸ºåºåˆ—åŒ–ç±»åº“ï¼Œæ˜¯å› ä¸º**å®ƒä¸æ”¯æŒæ‰€æœ‰å¯¹è±¡çš„åºåˆ—åŒ–**ï¼ŒåŒæ—¶Kryoéœ€è¦ç”¨æˆ·åœ¨ä½¿ç”¨å‰æ³¨å†Œéœ€è¦åºåˆ—åŒ–çš„ç±»åž‹ï¼Œä¸å¤Ÿæ–¹ä¾¿ï¼Œ**ä½†ä»ŽSpark 2.0.0ç‰ˆæœ¬å¼€å§‹ï¼Œç®€å•ç±»åž‹ã€ç®€å•ç±»åž‹æ•°ç»„ã€å­—ç¬¦ä¸²ç±»åž‹çš„Shuffling RDDs å·²ç»é»˜è®¤ä½¿ç”¨Kryoåºåˆ—åŒ–æ–¹å¼äº†**ã€‚

Kryoåºåˆ—åŒ–æ³¨å†Œæ–¹å¼çš„ä»£ç å¦‚ä¸‹ï¼š

```
public class MyKryoRegistrator implements KryoRegistrator{
  @Override
  public void registerClasses(Kryo kryo){
    kryo.register(StartupReportLogs.class);
  }
}
```

é…ç½®Kryoåºåˆ—åŒ–æ–¹å¼çš„ä»£ç å¦‚ä¸‹ï¼š

```
//åˆ›å»ºSparkConfå¯¹è±¡
val conf = new SparkConf().setMaster(â€¦).setAppName(â€¦)
//ä½¿ç”¨Kryoåºåˆ—åŒ–åº“
conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer");  
//åœ¨Kryoåºåˆ—åŒ–åº“ä¸­æ³¨å†Œè‡ªå®šä¹‰çš„ç±»é›†åˆ
conf.set("spark.kryo.registrator", "bigdata.com.MyKryoRegistrator"); 
```

> æœ¬æ–‡æ¡£é¦–å‘äºŽå…¬ä¼—å·ï¼šäº”åˆ†é’Ÿå­¦å¤§æ•°æ®ï¼Œå›žå¤ã€666ã€‘å³å¯èŽ·å¾—å…¨å¥—å¤§æ•°æ®ç¬”é¢è¯•æ•™ç¨‹

## Sparkè°ƒä¼˜ä¹‹Shuffleè°ƒä¼˜

### 1. mapå’Œreduceç«¯ç¼“å†²åŒºå¤§å°

åœ¨Sparkä»»åŠ¡è¿è¡Œè¿‡ç¨‹ä¸­ï¼Œå¦‚æžœshuffleçš„mapç«¯å¤„ç†çš„æ•°æ®é‡æ¯”è¾ƒå¤§ï¼Œä½†æ˜¯mapç«¯ç¼“å†²çš„å¤§å°æ˜¯å›ºå®šçš„ï¼Œå¯èƒ½ä¼šå‡ºçŽ°mapç«¯ç¼“å†²æ•°æ®é¢‘ç¹spillæº¢å†™åˆ°ç£ç›˜æ–‡ä»¶ä¸­çš„æƒ…å†µï¼Œä½¿å¾—æ€§èƒ½éžå¸¸ä½Žä¸‹ï¼Œé€šè¿‡è°ƒèŠ‚mapç«¯ç¼“å†²çš„å¤§å°ï¼Œå¯ä»¥é¿å…é¢‘ç¹çš„ç£ç›˜IOæ“ä½œï¼Œè¿›è€Œæå‡Sparkä»»åŠ¡çš„æ•´ä½“æ€§èƒ½ã€‚

mapç«¯ç¼“å†²çš„é»˜è®¤é…ç½®æ˜¯32KBï¼Œå¦‚æžœæ¯ä¸ªtaskå¤„ç†640KBçš„æ•°æ®ï¼Œé‚£ä¹ˆä¼šå‘ç”Ÿ640/32 = 20æ¬¡æº¢å†™ï¼Œå¦‚æžœæ¯ä¸ªtaskå¤„ç†64000KBçš„æ•°æ®ï¼Œå³ä¼šå‘ç”Ÿ64000/32=2000æ¬¡æº¢å†™ï¼Œè¿™å¯¹äºŽæ€§èƒ½çš„å½±å“æ˜¯éžå¸¸ä¸¥é‡çš„ã€‚

mapç«¯ç¼“å†²çš„é…ç½®æ–¹æ³•ï¼š

```
val conf = new SparkConf()
  .set("spark.shuffle.file.buffer", "64")
```

Spark Shuffleè¿‡ç¨‹ä¸­ï¼Œshuffle reduce taskçš„bufferç¼“å†²åŒºå¤§å°å†³å®šäº†reduce taskæ¯æ¬¡èƒ½å¤Ÿç¼“å†²çš„æ•°æ®é‡ï¼Œä¹Ÿå°±æ˜¯æ¯æ¬¡èƒ½å¤Ÿæ‹‰å–çš„æ•°æ®é‡ï¼Œå¦‚æžœå†…å­˜èµ„æºè¾ƒä¸ºå……è¶³ï¼Œé€‚å½“å¢žåŠ æ‹‰å–æ•°æ®ç¼“å†²åŒºçš„å¤§å°ï¼Œå¯ä»¥å‡å°‘æ‹‰å–æ•°æ®çš„æ¬¡æ•°ï¼Œä¹Ÿå°±å¯ä»¥å‡å°‘ç½‘ç»œä¼ è¾“çš„æ¬¡æ•°ï¼Œè¿›è€Œæå‡æ€§èƒ½ã€‚

reduceç«¯æ•°æ®æ‹‰å–ç¼“å†²åŒºçš„å¤§å°å¯ä»¥é€šè¿‡`spark.reducer.maxSizeInFlight`å‚æ•°è¿›è¡Œè®¾ç½®ï¼Œé»˜è®¤ä¸º48MBã€‚è¯¥å‚æ•°çš„è®¾ç½®æ–¹æ³•å¦‚ä¸‹ï¼š

reduceç«¯æ•°æ®æ‹‰å–ç¼“å†²åŒºé…ç½®ï¼š

```
val conf = new SparkConf()
  .set("spark.reducer.maxSizeInFlight", "96")
```

### 2. reduceç«¯é‡è¯•æ¬¡æ•°å’Œç­‰å¾…æ—¶é—´é—´éš”

Spark Shuffleè¿‡ç¨‹ä¸­ï¼Œreduce taskæ‹‰å–å±žäºŽè‡ªå·±çš„æ•°æ®æ—¶ï¼Œå¦‚æžœå› ä¸ºç½‘ç»œå¼‚å¸¸ç­‰åŽŸå› å¯¼è‡´å¤±è´¥ä¼šè‡ªåŠ¨è¿›è¡Œé‡è¯•ã€‚**å¯¹äºŽé‚£äº›åŒ…å«äº†ç‰¹åˆ«è€—æ—¶çš„shuffleæ“ä½œçš„ä½œä¸šï¼Œå»ºè®®å¢žåŠ é‡è¯•æœ€å¤§æ¬¡æ•°**ï¼ˆæ¯”å¦‚60æ¬¡ï¼‰ï¼Œä»¥é¿å…ç”±äºŽJVMçš„full gcæˆ–è€…ç½‘ç»œä¸ç¨³å®šç­‰å› ç´ å¯¼è‡´çš„æ•°æ®æ‹‰å–å¤±è´¥ã€‚åœ¨å®žè·µä¸­å‘çŽ°ï¼Œå¯¹äºŽé’ˆå¯¹è¶…å¤§æ•°æ®é‡ï¼ˆæ•°åäº¿~ä¸Šç™¾äº¿ï¼‰çš„shuffleè¿‡ç¨‹ï¼Œè°ƒèŠ‚è¯¥å‚æ•°å¯ä»¥å¤§å¹…åº¦æå‡ç¨³å®šæ€§ã€‚

reduceç«¯æ‹‰å–æ•°æ®é‡è¯•æ¬¡æ•°å¯ä»¥é€šè¿‡`spark.shuffle.io.maxRetries`å‚æ•°è¿›è¡Œè®¾ç½®ï¼Œè¯¥å‚æ•°å°±ä»£è¡¨äº†å¯ä»¥é‡è¯•çš„æœ€å¤§æ¬¡æ•°ã€‚å¦‚æžœåœ¨æŒ‡å®šæ¬¡æ•°ä¹‹å†…æ‹‰å–è¿˜æ˜¯æ²¡æœ‰æˆåŠŸï¼Œå°±å¯èƒ½ä¼šå¯¼è‡´ä½œä¸šæ‰§è¡Œå¤±è´¥ï¼Œé»˜è®¤ä¸º3ï¼Œè¯¥å‚æ•°çš„è®¾ç½®æ–¹æ³•å¦‚ä¸‹ï¼š

reduceç«¯æ‹‰å–æ•°æ®é‡è¯•æ¬¡æ•°é…ç½®ï¼š

```
val conf = new SparkConf()
  .set("spark.shuffle.io.maxRetries", "6")
```

Spark Shuffleè¿‡ç¨‹ä¸­ï¼Œreduce taskæ‹‰å–å±žäºŽè‡ªå·±çš„æ•°æ®æ—¶ï¼Œå¦‚æžœå› ä¸ºç½‘ç»œå¼‚å¸¸ç­‰åŽŸå› å¯¼è‡´å¤±è´¥ä¼šè‡ªåŠ¨è¿›è¡Œé‡è¯•ï¼Œåœ¨ä¸€æ¬¡å¤±è´¥åŽï¼Œä¼šç­‰å¾…ä¸€å®šçš„æ—¶é—´é—´éš”å†è¿›è¡Œé‡è¯•ï¼Œ**å¯ä»¥é€šè¿‡åŠ å¤§é—´éš”æ—¶é•¿ï¼ˆæ¯”å¦‚60sï¼‰ï¼Œä»¥å¢žåŠ shuffleæ“ä½œçš„ç¨³å®šæ€§**ã€‚

reduceç«¯æ‹‰å–æ•°æ®ç­‰å¾…é—´éš”å¯ä»¥é€šè¿‡`spark.shuffle.io.retryWait`å‚æ•°è¿›è¡Œè®¾ç½®ï¼Œé»˜è®¤å€¼ä¸º5sï¼Œè¯¥å‚æ•°çš„è®¾ç½®æ–¹æ³•å¦‚ä¸‹ï¼š

reduceç«¯æ‹‰å–æ•°æ®ç­‰å¾…é—´éš”é…ç½®ï¼š

```
val conf = new SparkConf()
  .set("spark.shuffle.io.retryWait", "60s")
```

### 3. bypassæœºåˆ¶å¼€å¯é˜ˆå€¼

å¯¹äºŽSortShuffleManagerï¼Œå¦‚æžœshuffle reduce taskçš„æ•°é‡å°äºŽæŸä¸€é˜ˆå€¼åˆ™shuffle writeè¿‡ç¨‹ä¸­ä¸ä¼šè¿›è¡ŒæŽ’åºæ“ä½œï¼Œè€Œæ˜¯ç›´æŽ¥æŒ‰ç…§æœªç»ä¼˜åŒ–çš„HashShuffleManagerçš„æ–¹å¼åŽ»å†™æ•°æ®ï¼Œä½†æ˜¯æœ€åŽä¼šå°†æ¯ä¸ªtaskäº§ç”Ÿçš„æ‰€æœ‰ä¸´æ—¶ç£ç›˜æ–‡ä»¶éƒ½åˆå¹¶æˆä¸€ä¸ªæ–‡ä»¶ï¼Œå¹¶ä¼šåˆ›å»ºå•ç‹¬çš„ç´¢å¼•æ–‡ä»¶ã€‚

å½“ä½ ä½¿ç”¨SortShuffleManageræ—¶ï¼Œå¦‚æžœçš„ç¡®ä¸éœ€è¦æŽ’åºæ“ä½œï¼Œé‚£ä¹ˆå»ºè®®å°†è¿™ä¸ªå‚æ•°è°ƒå¤§ä¸€äº›ï¼Œå¤§äºŽshuffle read taskçš„æ•°é‡ï¼Œé‚£ä¹ˆæ­¤æ—¶map-sideå°±ä¸ä¼šè¿›è¡ŒæŽ’åºäº†ï¼Œå‡å°‘äº†æŽ’åºçš„æ€§èƒ½å¼€é”€ï¼Œä½†æ˜¯è¿™ç§æ–¹å¼ä¸‹ï¼Œä¾ç„¶ä¼šäº§ç”Ÿå¤§é‡çš„ç£ç›˜æ–‡ä»¶ï¼Œå› æ­¤shuffle writeæ€§èƒ½æœ‰å¾…æé«˜ã€‚

SortShuffleManageræŽ’åºæ“ä½œé˜ˆå€¼çš„è®¾ç½®å¯ä»¥é€šè¿‡`spark.shuffle.sort.bypassMergeThreshold`è¿™ä¸€å‚æ•°è¿›è¡Œè®¾ç½®ï¼Œé»˜è®¤å€¼ä¸º200ï¼Œè¯¥å‚æ•°çš„è®¾ç½®æ–¹æ³•å¦‚ä¸‹ï¼š

reduceç«¯æ‹‰å–æ•°æ®ç­‰å¾…é—´éš”é…ç½®ï¼š

```
val conf = new SparkConf()
  .set("spark.shuffle.sort.bypassMergeThreshold", "400")
```

## åã€æ•…éšœæŽ’é™¤

### 1. é¿å…OOM-out of memory

åœ¨Shuffleè¿‡ç¨‹ï¼Œreduceç«¯taskå¹¶ä¸æ˜¯ç­‰åˆ°mapç«¯taskå°†å…¶æ•°æ®å…¨éƒ¨å†™å…¥ç£ç›˜åŽå†åŽ»æ‹‰å–ï¼Œè€Œæ˜¯mapç«¯å†™ä¸€ç‚¹æ•°æ®ï¼Œreduceç«¯taskå°±ä¼šæ‹‰å–ä¸€å°éƒ¨åˆ†æ•°æ®ï¼Œç„¶åŽç«‹å³è¿›è¡ŒåŽé¢çš„èšåˆã€ç®—å­å‡½æ•°çš„ä½¿ç”¨ç­‰æ“ä½œã€‚

reduceç«¯taskèƒ½å¤Ÿæ‹‰å–å¤šå°‘æ•°æ®ï¼Œç”±reduceæ‹‰å–æ•°æ®çš„ç¼“å†²åŒºbufferæ¥å†³å®šï¼Œå› ä¸º**æ‹‰å–è¿‡æ¥çš„æ•°æ®éƒ½æ˜¯å…ˆæ”¾åœ¨bufferä¸­ï¼Œç„¶åŽå†è¿›è¡ŒåŽç»­çš„å¤„ç†ï¼Œbufferçš„é»˜è®¤å¤§å°ä¸º48MB**ã€‚

reduceç«¯taskä¼šä¸€è¾¹æ‹‰å–ä¸€è¾¹è®¡ç®—ï¼Œä¸ä¸€å®šæ¯æ¬¡éƒ½ä¼šæ‹‰æ»¡48MBçš„æ•°æ®ï¼Œå¯èƒ½å¤§å¤šæ•°æ—¶å€™æ‹‰å–ä¸€éƒ¨åˆ†æ•°æ®å°±å¤„ç†æŽ‰äº†ã€‚

è™½ç„¶è¯´å¢žå¤§reduceç«¯ç¼“å†²åŒºå¤§å°å¯ä»¥å‡å°‘æ‹‰å–æ¬¡æ•°ï¼Œæå‡Shuffleæ€§èƒ½ï¼Œä½†æ˜¯æœ‰æ—¶mapç«¯çš„æ•°æ®é‡éžå¸¸å¤§ï¼Œå†™å‡ºçš„é€Ÿåº¦éžå¸¸å¿«ï¼Œæ­¤æ—¶reduceç«¯çš„æ‰€æœ‰taskåœ¨æ‹‰å–çš„æ—¶å€™ï¼Œæœ‰å¯èƒ½å…¨éƒ¨è¾¾åˆ°è‡ªå·±ç¼“å†²çš„æœ€å¤§æžé™å€¼ï¼Œå³48MBï¼Œæ­¤æ—¶ï¼Œå†åŠ ä¸Šreduceç«¯æ‰§è¡Œçš„èšåˆå‡½æ•°çš„ä»£ç ï¼Œå¯èƒ½ä¼šåˆ›å»ºå¤§é‡çš„å¯¹è±¡ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´å†…å­˜æº¢å‡ºï¼Œå³**OOM**ã€‚

**å¦‚æžœä¸€æ—¦å‡ºçŽ°reduceç«¯å†…å­˜æº¢å‡ºçš„é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥è€ƒè™‘å‡å°reduceç«¯æ‹‰å–æ•°æ®ç¼“å†²åŒºçš„å¤§å°ï¼Œä¾‹å¦‚å‡å°‘ä¸º12MB**ã€‚

åœ¨å®žé™…ç”Ÿäº§çŽ¯å¢ƒä¸­æ˜¯å‡ºçŽ°è¿‡è¿™ç§é—®é¢˜çš„ï¼Œè¿™æ˜¯å…¸åž‹çš„**ä»¥æ€§èƒ½æ¢æ‰§è¡Œçš„åŽŸç†**ã€‚reduceç«¯æ‹‰å–æ•°æ®çš„ç¼“å†²åŒºå‡å°ï¼Œä¸å®¹æ˜“å¯¼è‡´OOMï¼Œä½†æ˜¯ç›¸åº”çš„ï¼Œreudceç«¯çš„æ‹‰å–æ¬¡æ•°å¢žåŠ ï¼Œé€ æˆæ›´å¤šçš„ç½‘ç»œä¼ è¾“å¼€é”€ï¼Œé€ æˆæ€§èƒ½çš„ä¸‹é™ã€‚

> æ³¨æ„ï¼Œè¦ä¿è¯ä»»åŠ¡èƒ½å¤Ÿè¿è¡Œï¼Œå†è€ƒè™‘æ€§èƒ½çš„ä¼˜åŒ–ã€‚

### 2. é¿å…GCå¯¼è‡´çš„shuffleæ–‡ä»¶æ‹‰å–å¤±è´¥

åœ¨Sparkä½œä¸šä¸­ï¼Œæœ‰æ—¶ä¼šå‡ºçŽ°`shuffle file not found`çš„é”™è¯¯ï¼Œè¿™æ˜¯éžå¸¸å¸¸è§çš„ä¸€ä¸ªæŠ¥é”™ï¼Œæœ‰æ—¶å‡ºçŽ°è¿™ç§é”™è¯¯ä»¥åŽï¼Œé€‰æ‹©é‡æ–°æ‰§è¡Œä¸€éï¼Œå°±ä¸å†æŠ¥å‡ºè¿™ç§é”™è¯¯ã€‚

å‡ºçŽ°ä¸Šè¿°é—®é¢˜å¯èƒ½çš„åŽŸå› **æ˜¯Shuffleæ“ä½œä¸­ï¼ŒåŽé¢stageçš„taskæƒ³è¦åŽ»ä¸Šä¸€ä¸ªstageçš„taskæ‰€åœ¨çš„Executoræ‹‰å–æ•°æ®ï¼Œç»“æžœå¯¹æ–¹æ­£åœ¨æ‰§è¡ŒGCï¼Œæ‰§è¡ŒGCä¼šå¯¼è‡´Executorå†…æ‰€æœ‰çš„å·¥ä½œçŽ°åœºå…¨éƒ¨åœæ­¢**ï¼Œæ¯”å¦‚BlockManagerã€åŸºäºŽnettyçš„ç½‘ç»œé€šä¿¡ç­‰ï¼Œè¿™å°±ä¼šå¯¼è‡´åŽé¢çš„taskæ‹‰å–æ•°æ®æ‹‰å–äº†åŠå¤©éƒ½æ²¡æœ‰æ‹‰å–åˆ°ï¼Œå°±ä¼šæŠ¥å‡º`shuffle file not found`çš„é”™è¯¯ï¼Œè€Œç¬¬äºŒæ¬¡å†æ¬¡æ‰§è¡Œå°±ä¸ä¼šå†å‡ºçŽ°è¿™ç§é”™è¯¯ã€‚

å¯ä»¥é€šè¿‡è°ƒæ•´reduceç«¯æ‹‰å–æ•°æ®é‡è¯•æ¬¡æ•°å’Œreduceç«¯æ‹‰å–æ•°æ®æ—¶é—´é—´éš”è¿™ä¸¤ä¸ªå‚æ•°æ¥å¯¹Shuffleæ€§èƒ½è¿›è¡Œè°ƒæ•´ï¼Œå¢žå¤§å‚æ•°å€¼ï¼Œä½¿å¾—reduceç«¯æ‹‰å–æ•°æ®çš„é‡è¯•æ¬¡æ•°å¢žåŠ ï¼Œå¹¶ä¸”æ¯æ¬¡å¤±è´¥åŽç­‰å¾…çš„æ—¶é—´é—´éš”åŠ é•¿ã€‚

JVM GCå¯¼è‡´çš„shuffleæ–‡ä»¶æ‹‰å–å¤±è´¥è°ƒæ•´æ•°æ®é‡è¯•æ¬¡æ•°å’Œreduceç«¯æ‹‰å–æ•°æ®æ—¶é—´é—´éš”ï¼š

```
val conf = new SparkConf()
  .set("spark.shuffle.io.maxRetries", "6")
  .set("spark.shuffle.io.retryWait", "60s")
```

### 3. YARN-CLIENTæ¨¡å¼å¯¼è‡´çš„ç½‘å¡æµé‡æ¿€å¢žé—®é¢˜

åœ¨YARN-clientæ¨¡å¼ä¸‹ï¼ŒDriverå¯åŠ¨åœ¨æœ¬åœ°æœºå™¨ä¸Šï¼Œè€ŒDriverè´Ÿè´£æ‰€æœ‰çš„ä»»åŠ¡è°ƒåº¦ï¼Œéœ€è¦ä¸ŽYARNé›†ç¾¤ä¸Šçš„å¤šä¸ªExecutorè¿›è¡Œé¢‘ç¹çš„é€šä¿¡ã€‚

å‡è®¾æœ‰100ä¸ªExecutorï¼Œ1000ä¸ªtaskï¼Œé‚£ä¹ˆæ¯ä¸ªExecutoråˆ†é…åˆ°10ä¸ªtaskï¼Œä¹‹åŽï¼ŒDriverè¦é¢‘ç¹åœ°è·ŸExecutorä¸Šè¿è¡Œçš„1000ä¸ªtaskè¿›è¡Œé€šä¿¡ï¼Œé€šä¿¡æ•°æ®éžå¸¸å¤šï¼Œå¹¶ä¸”é€šä¿¡å“ç±»ç‰¹åˆ«é«˜ã€‚è¿™å°±å¯¼è‡´æœ‰å¯èƒ½åœ¨Sparkä»»åŠ¡è¿è¡Œè¿‡ç¨‹ä¸­ï¼Œç”±äºŽé¢‘ç¹å¤§é‡çš„ç½‘ç»œé€šè®¯ï¼Œæœ¬åœ°æœºå™¨çš„ç½‘å¡æµé‡ä¼šæ¿€å¢žã€‚

æ³¨æ„ï¼Œ**YARN-clientæ¨¡å¼åªä¼šåœ¨æµ‹è¯•çŽ¯å¢ƒä¸­ä½¿ç”¨ï¼Œè€Œä¹‹æ‰€ä»¥ä½¿ç”¨YARN-clientæ¨¡å¼ï¼Œæ˜¯ç”±äºŽå¯ä»¥çœ‹åˆ°è¯¦ç»†å…¨é¢çš„logä¿¡æ¯**ï¼Œé€šè¿‡æŸ¥çœ‹logï¼Œå¯ä»¥é”å®šç¨‹åºä¸­å­˜åœ¨çš„é—®é¢˜ï¼Œé¿å…åœ¨ç”Ÿäº§çŽ¯å¢ƒä¸‹å‘ç”Ÿæ•…éšœã€‚

**åœ¨ç”Ÿäº§çŽ¯å¢ƒä¸‹ï¼Œä½¿ç”¨çš„ä¸€å®šæ˜¯YARN-clusteræ¨¡å¼ã€‚åœ¨YARN-clusteræ¨¡å¼ä¸‹ï¼Œå°±ä¸ä¼šé€ æˆæœ¬åœ°æœºå™¨ç½‘å¡æµé‡æ¿€å¢žé—®é¢˜**ï¼Œå¦‚æžœYARN-clusteræ¨¡å¼ä¸‹å­˜åœ¨ç½‘ç»œé€šä¿¡çš„é—®é¢˜ï¼Œéœ€è¦è¿ç»´å›¢é˜Ÿè¿›è¡Œè§£å†³ã€‚

### 4. YARN-CLUSTERæ¨¡å¼çš„JVMæ ˆå†…å­˜æº¢å‡ºæ— æ³•æ‰§è¡Œé—®é¢˜

å½“Sparkä½œä¸šä¸­åŒ…å«SparkSQLçš„å†…å®¹æ—¶ï¼Œå¯èƒ½ä¼šç¢°åˆ°YARN-clientæ¨¡å¼ä¸‹å¯ä»¥è¿è¡Œï¼Œä½†æ˜¯YARN-clusteræ¨¡å¼ä¸‹æ— æ³•æäº¤è¿è¡Œï¼ˆæŠ¥å‡ºOOMé”™è¯¯ï¼‰çš„æƒ…å†µã€‚

**YARN-clientæ¨¡å¼ä¸‹**ï¼ŒDriveræ˜¯è¿è¡Œåœ¨æœ¬åœ°æœºå™¨ä¸Šçš„ï¼ŒSparkä½¿ç”¨çš„JVMçš„PermGençš„é…ç½®ï¼Œæ˜¯æœ¬åœ°æœºå™¨ä¸Šçš„spark-classæ–‡ä»¶ï¼ŒJVMæ°¸ä¹…ä»£çš„å¤§å°æ˜¯128MBï¼Œè¿™ä¸ªæ˜¯æ²¡æœ‰é—®é¢˜çš„ï¼Œä½†æ˜¯åœ¨**YARN-clusteræ¨¡å¼ä¸‹**ï¼ŒDriverè¿è¡Œåœ¨YARNé›†ç¾¤çš„æŸä¸ªèŠ‚ç‚¹ä¸Šï¼Œä½¿ç”¨çš„æ˜¯æ²¡æœ‰ç»è¿‡é…ç½®çš„é»˜è®¤è®¾ç½®ï¼ŒPermGenæ°¸ä¹…ä»£å¤§å°ä¸º82MBã€‚

SparkSQLçš„å†…éƒ¨è¦è¿›è¡Œå¾ˆå¤æ‚çš„SQLçš„è¯­ä¹‰è§£æžã€è¯­æ³•æ ‘è½¬æ¢ç­‰ç­‰ï¼Œéžå¸¸å¤æ‚ï¼Œå¦‚æžœsqlè¯­å¥æœ¬èº«å°±éžå¸¸å¤æ‚ï¼Œé‚£ä¹ˆå¾ˆæœ‰å¯èƒ½ä¼šå¯¼è‡´æ€§èƒ½çš„æŸè€—å’Œå†…å­˜çš„å ç”¨ï¼Œç‰¹åˆ«æ˜¯å¯¹PermGençš„å ç”¨ä¼šæ¯”è¾ƒå¤§ã€‚

æ‰€ä»¥ï¼Œæ­¤æ—¶å¦‚æžœPermGenå ç”¨å¥½è¿‡äº†82MBï¼Œä½†æ˜¯åˆå°äºŽ128MBï¼Œå°±ä¼šå‡ºçŽ°YARN-clientæ¨¡å¼ä¸‹å¯ä»¥è¿è¡Œï¼ŒYARN-clusteræ¨¡å¼ä¸‹æ— æ³•è¿è¡Œçš„æƒ…å†µã€‚

**è§£å†³ä¸Šè¿°é—®é¢˜çš„æ–¹æ³•æ˜¯å¢žåŠ PermGen(æ°¸ä¹…ä»£)çš„å®¹é‡**ï¼Œéœ€è¦åœ¨spark-submitè„šæœ¬ä¸­å¯¹ç›¸å…³å‚æ•°è¿›è¡Œè®¾ç½®ï¼Œè®¾ç½®æ–¹æ³•å¦‚ä¸‹ï¼š

```
--conf spark.driver.extraJavaOptions="-XX:PermSize=128M -XX:MaxPermSize=256M"
```

é€šè¿‡ä¸Šè¿°æ–¹æ³•å°±è®¾ç½®äº†Driveræ°¸ä¹…ä»£çš„å¤§å°ï¼Œé»˜è®¤ä¸º128MBï¼Œæœ€å¤§256MBï¼Œè¿™æ ·å°±å¯ä»¥é¿å…ä¸Šé¢æ‰€è¯´çš„é—®é¢˜ã€‚

### 5. é¿å…SparkSQL JVMæ ˆå†…å­˜æº¢å‡º

å½“SparkSQLçš„sqlè¯­å¥æœ‰æˆç™¾ä¸Šåƒçš„orå…³é”®å­—æ—¶ï¼Œå°±å¯èƒ½ä¼šå‡ºçŽ°Driverç«¯çš„JVMæ ˆå†…å­˜æº¢å‡ºã€‚

**JVMæ ˆå†…å­˜æº¢å‡ºåŸºæœ¬ä¸Šå°±æ˜¯ç”±äºŽè°ƒç”¨çš„æ–¹æ³•å±‚çº§è¿‡å¤šï¼Œäº§ç”Ÿäº†å¤§é‡çš„ï¼Œéžå¸¸æ·±çš„ï¼Œè¶…å‡ºäº†JVMæ ˆæ·±åº¦é™åˆ¶çš„é€’å½’**ã€‚ï¼ˆæˆ‘ä»¬çŒœæµ‹SparkSQLæœ‰å¤§é‡orè¯­å¥çš„æ—¶å€™ï¼Œåœ¨è§£æžSQLæ—¶ï¼Œä¾‹å¦‚è½¬æ¢ä¸ºè¯­æ³•æ ‘æˆ–è€…è¿›è¡Œæ‰§è¡Œè®¡åˆ’çš„ç”Ÿæˆçš„æ—¶å€™ï¼Œå¯¹äºŽorçš„å¤„ç†æ˜¯é€’å½’ï¼Œoréžå¸¸å¤šæ—¶ï¼Œä¼šå‘ç”Ÿå¤§é‡çš„é€’å½’ï¼‰

æ­¤æ—¶ï¼Œ**å»ºè®®å°†ä¸€æ¡sqlè¯­å¥æ‹†åˆ†ä¸ºå¤šæ¡sqlè¯­å¥æ¥æ‰§è¡Œï¼Œæ¯æ¡sqlè¯­å¥å°½é‡ä¿è¯100ä¸ªä»¥å†…çš„å­å¥**ã€‚æ ¹æ®å®žé™…çš„ç”Ÿäº§çŽ¯å¢ƒè¯•éªŒï¼Œä¸€æ¡sqlè¯­å¥çš„orå…³é”®å­—æŽ§åˆ¶åœ¨100ä¸ªä»¥å†…ï¼Œé€šå¸¸ä¸ä¼šå¯¼è‡´JVMæ ˆå†…å­˜æº¢å‡ºã€‚

------

> æ›´å¤šå¤§æ•°æ®å¥½æ–‡ï¼Œæ¬¢è¿Žå…³æ³¨å…¬ä¼—å·ã€**äº”åˆ†é’Ÿå­¦å¤§æ•°æ®**ã€‘

## åä¸€ã€Sparkå¤§åŽ‚é¢è¯•çœŸé¢˜

#### 1. é€šå¸¸æ¥è¯´ï¼ŒSparkä¸ŽMapReduceç›¸æ¯”ï¼ŒSparkè¿è¡Œæ•ˆçŽ‡æ›´é«˜ã€‚è¯·è¯´æ˜Žæ•ˆçŽ‡æ›´é«˜æ¥æºäºŽSparkå†…ç½®çš„å“ªäº›æœºåˆ¶ï¼Ÿ

sparkæ˜¯å€Ÿé‰´äº†Mapreduce,å¹¶åœ¨å…¶åŸºç¡€ä¸Šå‘å±•èµ·æ¥çš„ï¼Œç»§æ‰¿äº†å…¶åˆ†å¸ƒå¼è®¡ç®—çš„ä¼˜ç‚¹å¹¶è¿›è¡Œäº†æ”¹è¿›ï¼Œsparkç”Ÿæ€æ›´ä¸ºä¸°å¯Œï¼ŒåŠŸèƒ½æ›´ä¸ºå¼ºå¤§ï¼Œæ€§èƒ½æ›´åŠ é€‚ç”¨èŒƒå›´å¹¿ï¼Œmapreduceæ›´ç®€å•ï¼Œç¨³å®šæ€§å¥½ã€‚ä¸»è¦åŒºåˆ«ï¼š

1. sparkæŠŠè¿ç®—çš„ä¸­é—´æ•°æ®(shuffleé˜¶æ®µäº§ç”Ÿçš„æ•°æ®)å­˜æ”¾åœ¨å†…å­˜ï¼Œè¿­ä»£è®¡ç®—æ•ˆçŽ‡æ›´é«˜ï¼Œmapreduceçš„ä¸­é—´ç»“æžœéœ€è¦è½åœ°ï¼Œä¿å­˜åˆ°ç£ç›˜ï¼›
2. Sparkå®¹é”™æ€§é«˜ï¼Œå®ƒé€šè¿‡å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†RDDæ¥å®žçŽ°é«˜æ•ˆå®¹é”™ï¼ŒRDDæ˜¯ä¸€ç»„åˆ†å¸ƒå¼çš„å­˜å‚¨åœ¨èŠ‚ç‚¹å†…å­˜ä¸­çš„åªè¯»æ€§çš„æ•°æ®é›†ï¼Œè¿™äº›é›†åˆçŸ³å¼¹æ€§çš„ï¼ŒæŸä¸€éƒ¨åˆ†ä¸¢å¤±æˆ–è€…å‡ºé”™ï¼Œå¯ä»¥é€šè¿‡æ•´ä¸ªæ•°æ®é›†çš„è®¡ç®—æµç¨‹çš„è¡€ç¼˜å…³ç³»æ¥å®žçŽ°é‡å»ºï¼Œmapreduceçš„å®¹é”™åªèƒ½é‡æ–°è®¡ç®—ï¼›
3. Sparkæ›´é€šç”¨ï¼Œæä¾›äº†transformationå’Œactionè¿™ä¸¤å¤§ç±»çš„å¤šåŠŸèƒ½apiï¼Œå¦å¤–è¿˜æœ‰æµå¼å¤„ç†sparkstreamingæ¨¡å—ã€å›¾è®¡ç®—ç­‰ç­‰ï¼Œmapreduceåªæä¾›äº†mapå’Œreduceä¸¤ç§æ“ä½œï¼Œæµè®¡ç®—åŠå…¶ä»–çš„æ¨¡å—æ”¯æŒæ¯”è¾ƒç¼ºä¹ï¼›
4. Sparkæ¡†æž¶å’Œç”Ÿæ€æ›´ä¸ºå¤æ‚ï¼Œæœ‰RDDï¼Œè¡€ç¼˜lineageã€æ‰§è¡Œæ—¶çš„æœ‰å‘æ— çŽ¯å›¾DAGï¼Œstageåˆ’åˆ†ç­‰ï¼Œå¾ˆå¤šæ—¶å€™sparkä½œä¸šéƒ½éœ€è¦æ ¹æ®ä¸åŒä¸šåŠ¡åœºæ™¯çš„éœ€è¦è¿›è¡Œè°ƒä¼˜ä»¥è¾¾åˆ°æ€§èƒ½è¦æ±‚ï¼Œmapreduceæ¡†æž¶åŠå…¶ç”Ÿæ€ç›¸å¯¹è¾ƒä¸ºç®€å•ï¼Œå¯¹æ€§èƒ½çš„è¦æ±‚ä¹Ÿç›¸å¯¹è¾ƒå¼±ï¼Œè¿è¡Œè¾ƒä¸ºç¨³å®šï¼Œé€‚åˆé•¿æœŸåŽå°è¿è¡Œï¼›
5. Sparkè®¡ç®—æ¡†æž¶å¯¹å†…å­˜çš„åˆ©ç”¨å’Œè¿è¡Œçš„å¹¶è¡Œåº¦æ¯”mapreduceé«˜ï¼ŒSparkè¿è¡Œå®¹å™¨ä¸ºexecutorï¼Œå†…éƒ¨ThreadPoolä¸­çº¿ç¨‹è¿è¡Œä¸€ä¸ªTaskï¼Œmapreduceåœ¨çº¿ç¨‹å†…éƒ¨è¿è¡Œcontainerï¼Œcontainerå®¹å™¨åˆ†ç±»ä¸ºMapTaskå’ŒReduceTaskã€‚Sparkç¨‹åºè¿è¡Œå¹¶è¡Œåº¦é«˜ï¼›
6. Sparkå¯¹äºŽexecutorçš„ä¼˜åŒ–ï¼Œåœ¨JVMè™šæ‹Ÿæœºçš„åŸºç¡€ä¸Šå¯¹å†…å­˜å¼¹æ€§åˆ©ç”¨ï¼šstorage memoryä¸ŽExecution memoryçš„å¼¹æ€§æ‰©å®¹ï¼Œä½¿å¾—å†…å­˜åˆ©ç”¨æ•ˆçŽ‡æ›´é«˜

#### 2. hadoopå’Œsparkä½¿ç”¨åœºæ™¯ï¼Ÿ

Hadoop/MapReduceå’ŒSparkæœ€é€‚åˆçš„éƒ½æ˜¯åšç¦»çº¿åž‹çš„æ•°æ®åˆ†æžï¼Œä½†Hadoopç‰¹åˆ«é€‚åˆæ˜¯å•æ¬¡åˆ†æžçš„æ•°æ®é‡â€œå¾ˆå¤§â€çš„æƒ…æ™¯ï¼Œè€ŒSparkåˆ™é€‚ç”¨äºŽæ•°æ®é‡ä¸æ˜¯å¾ˆå¤§çš„æƒ…æ™¯ã€‚

1. ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œå¯¹äºŽä¸­å°äº’è”ç½‘å’Œä¼ä¸šçº§çš„å¤§æ•°æ®åº”ç”¨è€Œè¨€ï¼Œå•æ¬¡åˆ†æžçš„æ•°é‡éƒ½ä¸ä¼šâ€œå¾ˆå¤§â€ï¼Œå› æ­¤å¯ä»¥ä¼˜å…ˆè€ƒè™‘ä½¿ç”¨Sparkã€‚
2. ä¸šåŠ¡é€šå¸¸è®¤ä¸ºSparkæ›´é€‚ç”¨äºŽæœºå™¨å­¦ä¹ ä¹‹ç±»çš„â€œè¿­ä»£å¼â€åº”ç”¨ï¼Œ80GBçš„åŽ‹ç¼©æ•°æ®ï¼ˆè§£åŽ‹åŽè¶…è¿‡200GBï¼‰ï¼Œ10ä¸ªèŠ‚ç‚¹çš„é›†ç¾¤è§„æ¨¡ï¼Œè·‘ç±»ä¼¼â€œsum+group-byâ€çš„åº”ç”¨ï¼ŒMapReduceèŠ±äº†5åˆ†é’Ÿï¼Œè€Œsparkåªéœ€è¦2åˆ†é’Ÿã€‚

#### 3. sparkå¦‚ä½•ä¿è¯å®•æœºè¿…é€Ÿæ¢å¤?

1. é€‚å½“å¢žåŠ spark standby master
2. ç¼–å†™shellè„šæœ¬ï¼Œå®šæœŸæ£€æµ‹masterçŠ¶æ€ï¼Œå‡ºçŽ°å®•æœºåŽå¯¹masterè¿›è¡Œé‡å¯æ“ä½œ

#### 4. hadoopå’Œsparkçš„ç›¸åŒç‚¹å’Œä¸åŒç‚¹ï¼Ÿ

**Hadoopåº•å±‚ä½¿ç”¨MapReduceè®¡ç®—æž¶æž„ï¼Œåªæœ‰mapå’Œreduceä¸¤ç§æ“ä½œï¼Œè¡¨è¾¾èƒ½åŠ›æ¯”è¾ƒæ¬ ç¼ºï¼Œè€Œä¸”åœ¨MRè¿‡ç¨‹ä¸­ä¼šé‡å¤çš„è¯»å†™hdfsï¼Œé€ æˆå¤§é‡çš„ç£ç›˜ioè¯»å†™æ“ä½œ**ï¼Œæ‰€ä»¥é€‚åˆé«˜æ—¶å»¶çŽ¯å¢ƒä¸‹æ‰¹å¤„ç†è®¡ç®—çš„åº”ç”¨ï¼›

**Sparkæ˜¯åŸºäºŽå†…å­˜çš„åˆ†å¸ƒå¼è®¡ç®—æž¶æž„ï¼Œæä¾›æ›´åŠ ä¸°å¯Œçš„æ•°æ®é›†æ“ä½œç±»åž‹ï¼Œä¸»è¦åˆ†æˆè½¬åŒ–æ“ä½œå’Œè¡ŒåŠ¨æ“ä½œ**ï¼ŒåŒ…æ‹¬mapã€reduceã€filterã€flatmapã€groupbykeyã€reducebykeyã€unionå’Œjoinç­‰ï¼Œæ•°æ®åˆ†æžæ›´åŠ å¿«é€Ÿï¼Œæ‰€ä»¥é€‚åˆä½Žæ—¶å»¶çŽ¯å¢ƒä¸‹è®¡ç®—çš„åº”ç”¨ï¼›

**sparkä¸Žhadoopæœ€å¤§çš„åŒºåˆ«åœ¨äºŽè¿­ä»£å¼è®¡ç®—æ¨¡åž‹**ã€‚åŸºäºŽmapreduceæ¡†æž¶çš„Hadoopä¸»è¦åˆ†ä¸ºmapå’Œreduceä¸¤ä¸ªé˜¶æ®µï¼Œä¸¤ä¸ªé˜¶æ®µå®Œäº†å°±ç»“æŸäº†ï¼Œæ‰€ä»¥åœ¨ä¸€ä¸ªjobé‡Œé¢èƒ½åšçš„å¤„ç†å¾ˆæœ‰é™ï¼›sparkè®¡ç®—æ¨¡åž‹æ˜¯åŸºäºŽå†…å­˜çš„è¿­ä»£å¼è®¡ç®—æ¨¡åž‹ï¼Œå¯ä»¥åˆ†ä¸ºnä¸ªé˜¶æ®µï¼Œæ ¹æ®ç”¨æˆ·ç¼–å†™çš„RDDç®—å­å’Œç¨‹åºï¼Œåœ¨å¤„ç†å®Œä¸€ä¸ªé˜¶æ®µåŽå¯ä»¥ç»§ç»­å¾€ä¸‹å¤„ç†å¾ˆå¤šä¸ªé˜¶æ®µï¼Œè€Œä¸åªæ˜¯ä¸¤ä¸ªé˜¶æ®µã€‚æ‰€ä»¥sparkç›¸è¾ƒäºŽmapreduceï¼Œè®¡ç®—æ¨¡åž‹æ›´åŠ çµæ´»ï¼Œå¯ä»¥æä¾›æ›´å¼ºå¤§çš„åŠŸèƒ½ã€‚

ä½†æ˜¯sparkä¹Ÿæœ‰åŠ£åŠ¿ï¼Œç”±äºŽsparkåŸºäºŽå†…å­˜è¿›è¡Œè®¡ç®—ï¼Œè™½ç„¶å¼€å‘å®¹æ˜“ï¼Œä½†æ˜¯çœŸæ­£é¢å¯¹å¤§æ•°æ®çš„æ—¶å€™ï¼Œåœ¨æ²¡æœ‰è¿›è¡Œè°ƒä¼˜çš„æƒ…å†µä¸‹ï¼Œå¯èƒ½ä¼šå‡ºçŽ°å„ç§å„æ ·çš„é—®é¢˜ï¼Œæ¯”å¦‚OOMå†…å­˜æº¢å‡ºç­‰æƒ…å†µï¼Œå¯¼è‡´sparkç¨‹åºå¯èƒ½æ— æ³•è¿è¡Œèµ·æ¥ï¼Œè€Œmapreduceè™½ç„¶è¿è¡Œç¼“æ…¢ï¼Œä½†æ˜¯è‡³å°‘å¯ä»¥æ…¢æ…¢è¿è¡Œå®Œã€‚

#### 5. RDDæŒä¹…åŒ–åŽŸç†ï¼Ÿ

sparkéžå¸¸é‡è¦çš„ä¸€ä¸ªåŠŸèƒ½ç‰¹æ€§å°±æ˜¯å¯ä»¥å°†RDDæŒä¹…åŒ–åœ¨å†…å­˜ä¸­ã€‚

è°ƒç”¨cache()å’Œpersist()æ–¹æ³•å³å¯ã€‚cache()å’Œpersist()çš„åŒºåˆ«åœ¨äºŽï¼Œcache()æ˜¯persist()çš„ä¸€ç§ç®€åŒ–æ–¹å¼ï¼Œcache()çš„åº•å±‚å°±æ˜¯è°ƒç”¨persist()çš„æ— å‚ç‰ˆæœ¬persist(MEMORY_ONLY)ï¼Œå°†æ•°æ®æŒä¹…åŒ–åˆ°å†…å­˜ä¸­ã€‚

å¦‚æžœéœ€è¦ä»Žå†…å­˜ä¸­æ¸…é™¤ç¼“å­˜ï¼Œå¯ä»¥ä½¿ç”¨unpersist()æ–¹æ³•ã€‚RDDæŒä¹…åŒ–æ˜¯å¯ä»¥æ‰‹åŠ¨é€‰æ‹©ä¸åŒçš„ç­–ç•¥çš„ã€‚åœ¨è°ƒç”¨persist()æ—¶ä¼ å…¥å¯¹åº”çš„StorageLevelå³å¯ã€‚

#### 6. checkpointæ£€æŸ¥ç‚¹æœºåˆ¶ï¼Ÿ

åº”ç”¨åœºæ™¯ï¼šå½“sparkåº”ç”¨ç¨‹åºç‰¹åˆ«å¤æ‚ï¼Œä»Žåˆå§‹çš„RDDå¼€å§‹åˆ°æœ€åŽæ•´ä¸ªåº”ç”¨ç¨‹åºå®Œæˆæœ‰å¾ˆå¤šçš„æ­¥éª¤ï¼Œè€Œä¸”æ•´ä¸ªåº”ç”¨è¿è¡Œæ—¶é—´ç‰¹åˆ«é•¿ï¼Œè¿™ç§æƒ…å†µä¸‹å°±æ¯”è¾ƒé€‚åˆä½¿ç”¨checkpointåŠŸèƒ½ã€‚

åŽŸå› ï¼šå¯¹äºŽç‰¹åˆ«å¤æ‚çš„Sparkåº”ç”¨ï¼Œä¼šå‡ºçŽ°æŸä¸ªåå¤ä½¿ç”¨çš„RDDï¼Œå³ä½¿ä¹‹å‰æŒä¹…åŒ–è¿‡ä½†ç”±äºŽèŠ‚ç‚¹çš„æ•…éšœå¯¼è‡´æ•°æ®ä¸¢å¤±äº†ï¼Œæ²¡æœ‰å®¹é”™æœºåˆ¶ï¼Œæ‰€ä»¥éœ€è¦é‡æ–°è®¡ç®—ä¸€æ¬¡æ•°æ®ã€‚

Checkpointé¦–å…ˆä¼šè°ƒç”¨SparkContextçš„setCheckPointDIR()æ–¹æ³•ï¼Œè®¾ç½®ä¸€ä¸ªå®¹é”™çš„æ–‡ä»¶ç³»ç»Ÿçš„ç›®å½•ï¼Œæ¯”å¦‚è¯´HDFSï¼›ç„¶åŽå¯¹RDDè°ƒç”¨checkpoint()æ–¹æ³•ã€‚ä¹‹åŽåœ¨RDDæ‰€å¤„çš„jobè¿è¡Œç»“æŸä¹‹åŽï¼Œä¼šå¯åŠ¨ä¸€ä¸ªå•ç‹¬çš„jobï¼Œæ¥å°†checkpointè¿‡çš„RDDæ•°æ®å†™å…¥ä¹‹å‰è®¾ç½®çš„æ–‡ä»¶ç³»ç»Ÿï¼Œè¿›è¡Œé«˜å¯ç”¨ã€å®¹é”™çš„ç±»æŒä¹…åŒ–æ“ä½œã€‚

æ£€æŸ¥ç‚¹æœºåˆ¶æ˜¯æˆ‘ä»¬åœ¨spark  streamingä¸­ç”¨æ¥ä¿éšœå®¹é”™æ€§çš„ä¸»è¦æœºåˆ¶ï¼Œå®ƒå¯ä»¥ä½¿spark streamingé˜¶æ®µæ€§çš„æŠŠåº”ç”¨æ•°æ®å­˜å‚¨åˆ°è¯¸å¦‚HDFSç­‰å¯é å­˜å‚¨ç³»ç»Ÿä¸­ï¼Œä»¥ä¾›æ¢å¤æ—¶ä½¿ç”¨ã€‚å…·ä½“æ¥è¯´åŸºäºŽä»¥ä¸‹ä¸¤ä¸ªç›®çš„æœåŠ¡ï¼š

1. æŽ§åˆ¶å‘ç”Ÿå¤±è´¥æ—¶éœ€è¦é‡ç®—çš„çŠ¶æ€æ•°ã€‚Spark streamingå¯ä»¥é€šè¿‡è½¬åŒ–å›¾çš„è°±ç³»å›¾æ¥é‡ç®—çŠ¶æ€ï¼Œæ£€æŸ¥ç‚¹æœºåˆ¶åˆ™å¯ä»¥æŽ§åˆ¶éœ€è¦åœ¨è½¬åŒ–å›¾ä¸­å›žæº¯å¤šè¿œã€‚
2. æä¾›é©±åŠ¨å™¨ç¨‹åºå®¹é”™ã€‚å¦‚æžœæµè®¡ç®—åº”ç”¨ä¸­çš„é©±åŠ¨å™¨ç¨‹åºå´©æºƒäº†ï¼Œä½ å¯ä»¥é‡å¯é©±åŠ¨å™¨ç¨‹åºå¹¶è®©é©±åŠ¨å™¨ç¨‹åºä»Žæ£€æŸ¥ç‚¹æ¢å¤ï¼Œè¿™æ ·spark streamingå°±å¯ä»¥è¯»å–ä¹‹å‰è¿è¡Œçš„ç¨‹åºå¤„ç†æ•°æ®çš„è¿›åº¦ï¼Œå¹¶ä»Žé‚£é‡Œç»§ç»­ã€‚

#### 7. checkpointå’ŒæŒä¹…åŒ–æœºåˆ¶çš„åŒºåˆ«ï¼Ÿ

æœ€ä¸»è¦çš„åŒºåˆ«åœ¨äºŽæŒä¹…åŒ–åªæ˜¯å°†æ•°æ®ä¿å­˜åœ¨BlockManagerä¸­ï¼Œä½†æ˜¯RDDçš„lineage(è¡€ç¼˜å…³ç³»ï¼Œä¾èµ–å…³ç³»)æ˜¯ä¸å˜çš„ã€‚ä½†æ˜¯checkpointæ‰§è¡Œå®Œä¹‹åŽï¼Œrddå·²ç»æ²¡æœ‰ä¹‹å‰æ‰€è°“çš„ä¾èµ–rddäº†ï¼Œè€Œåªæœ‰ä¸€ä¸ªå¼ºè¡Œä¸ºå…¶è®¾ç½®çš„checkpointRDDï¼Œcheckpointä¹‹åŽrddçš„lineageå°±æ”¹å˜äº†ã€‚

æŒä¹…åŒ–çš„æ•°æ®ä¸¢å¤±çš„å¯èƒ½æ€§æ›´å¤§ï¼Œå› ä¸ºèŠ‚ç‚¹çš„æ•…éšœä¼šå¯¼è‡´ç£ç›˜ã€å†…å­˜çš„æ•°æ®ä¸¢å¤±ã€‚ä½†æ˜¯checkpointçš„æ•°æ®é€šå¸¸æ˜¯ä¿å­˜åœ¨é«˜å¯ç”¨çš„æ–‡ä»¶ç³»ç»Ÿä¸­ï¼Œæ¯”å¦‚HDFSä¸­ï¼Œæ‰€ä»¥æ•°æ®ä¸¢å¤±å¯èƒ½æ€§æ¯”è¾ƒä½Ž

#### 8. RDDæœºåˆ¶ç†è§£å—ï¼Ÿ

rddåˆ†å¸ƒå¼å¼¹æ€§æ•°æ®é›†ï¼Œç®€å•çš„ç†è§£æˆä¸€ç§æ•°æ®ç»“æž„ï¼Œæ˜¯sparkæ¡†æž¶ä¸Šçš„é€šç”¨è´§å¸ã€‚æ‰€æœ‰ç®—å­éƒ½æ˜¯åŸºäºŽrddæ¥æ‰§è¡Œçš„ï¼Œä¸åŒçš„åœºæ™¯ä¼šæœ‰ä¸åŒçš„rddå®žçŽ°ç±»ï¼Œä½†æ˜¯éƒ½å¯ä»¥è¿›è¡Œäº’ç›¸è½¬æ¢ã€‚rddæ‰§è¡Œè¿‡ç¨‹ä¸­ä¼šå½¢æˆdagå›¾ï¼Œç„¶åŽå½¢æˆlineageä¿è¯å®¹é”™æ€§ç­‰ã€‚ä»Žç‰©ç†çš„è§’åº¦æ¥çœ‹rddå­˜å‚¨çš„æ˜¯blockå’Œnodeä¹‹é—´çš„æ˜ å°„ã€‚

RDDæ˜¯sparkæä¾›çš„æ ¸å¿ƒæŠ½è±¡ï¼Œå…¨ç§°ä¸ºå¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†ã€‚

RDDåœ¨é€»è¾‘ä¸Šæ˜¯ä¸€ä¸ªhdfsæ–‡ä»¶ï¼Œåœ¨æŠ½è±¡ä¸Šæ˜¯ä¸€ç§å…ƒç´ é›†åˆï¼ŒåŒ…å«äº†æ•°æ®ã€‚å®ƒæ˜¯è¢«åˆ†åŒºçš„ï¼Œåˆ†ä¸ºå¤šä¸ªåˆ†åŒºï¼Œæ¯ä¸ªåˆ†åŒºåˆ†å¸ƒåœ¨é›†ç¾¤ä¸­çš„ä¸åŒç»“ç‚¹ä¸Šï¼Œä»Žè€Œè®©RDDä¸­çš„æ•°æ®å¯ä»¥è¢«å¹¶è¡Œæ“ä½œï¼ˆåˆ†å¸ƒå¼æ•°æ®é›†ï¼‰

æ¯”å¦‚æœ‰ä¸ªRDDæœ‰90Wæ•°æ®ï¼Œ3ä¸ªpartitionï¼Œåˆ™æ¯ä¸ªåˆ†åŒºä¸Šæœ‰30Wæ•°æ®ã€‚RDDé€šå¸¸é€šè¿‡Hadoopä¸Šçš„æ–‡ä»¶ï¼Œå³HDFSæˆ–è€…HIVEè¡¨æ¥åˆ›å»ºï¼Œè¿˜å¯ä»¥é€šè¿‡åº”ç”¨ç¨‹åºä¸­çš„é›†åˆæ¥åˆ›å»ºï¼›RDDæœ€é‡è¦çš„ç‰¹æ€§å°±æ˜¯å®¹é”™æ€§ï¼Œå¯ä»¥è‡ªåŠ¨ä»ŽèŠ‚ç‚¹å¤±è´¥ä¸­æ¢å¤è¿‡æ¥ã€‚å³å¦‚æžœæŸä¸ªç»“ç‚¹ä¸Šçš„RDD partitionå› ä¸ºèŠ‚ç‚¹æ•…éšœï¼Œå¯¼è‡´æ•°æ®ä¸¢å¤±ï¼Œé‚£ä¹ˆRDDå¯ä»¥é€šè¿‡è‡ªå·±çš„æ•°æ®æ¥æºé‡æ–°è®¡ç®—è¯¥partitionã€‚è¿™ä¸€åˆ‡å¯¹ä½¿ç”¨è€…éƒ½æ˜¯é€æ˜Žçš„ã€‚

RDDçš„æ•°æ®é»˜è®¤å­˜æ”¾åœ¨å†…å­˜ä¸­ï¼Œä½†æ˜¯å½“å†…å­˜èµ„æºä¸è¶³æ—¶ï¼Œsparkä¼šè‡ªåŠ¨å°†RDDæ•°æ®å†™å…¥ç£ç›˜ã€‚æ¯”å¦‚æŸç»“ç‚¹å†…å­˜åªèƒ½å¤„ç†20Wæ•°æ®ï¼Œé‚£ä¹ˆè¿™20Wæ•°æ®å°±ä¼šæ”¾å…¥å†…å­˜ä¸­è®¡ç®—ï¼Œå‰©ä¸‹10Wæ”¾åˆ°ç£ç›˜ä¸­ã€‚RDDçš„å¼¹æ€§ä½“çŽ°åœ¨äºŽRDDä¸Šè‡ªåŠ¨è¿›è¡Œå†…å­˜å’Œç£ç›˜ä¹‹é—´æƒè¡¡å’Œåˆ‡æ¢çš„æœºåˆ¶ã€‚

#### 9. Spark streamingä»¥åŠåŸºæœ¬å·¥ä½œåŽŸç†ï¼Ÿ

Spark  streamingæ˜¯spark  core  APIçš„ä¸€ç§æ‰©å±•ï¼Œå¯ä»¥ç”¨äºŽè¿›è¡Œå¤§è§„æ¨¡ã€é«˜åžåé‡ã€å®¹é”™çš„å®žæ—¶æ•°æ®æµçš„å¤„ç†ã€‚

å®ƒæ”¯æŒä»Žå¤šç§æ•°æ®æºè¯»å–æ•°æ®ï¼Œæ¯”å¦‚Kafkaã€Flumeã€Twitterå’ŒTCP Socketï¼Œå¹¶ä¸”èƒ½å¤Ÿä½¿ç”¨ç®—å­æ¯”å¦‚mapã€reduceã€joinå’Œwindowç­‰æ¥å¤„ç†æ•°æ®ï¼Œå¤„ç†åŽçš„æ•°æ®å¯ä»¥ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿã€æ•°æ®åº“ç­‰å­˜å‚¨ä¸­ã€‚

Spark streamingå†…éƒ¨çš„åŸºæœ¬å·¥ä½œåŽŸç†æ˜¯ï¼šæŽ¥å—å®žæ—¶è¾“å…¥æ•°æ®æµï¼Œç„¶åŽå°†æ•°æ®æ‹†åˆ†æˆbatchï¼Œæ¯”å¦‚æ¯æ”¶é›†ä¸€ç§’çš„æ•°æ®å°è£…æˆä¸€ä¸ªbatchï¼Œç„¶åŽå°†æ¯ä¸ªbatchäº¤ç»™sparkçš„è®¡ç®—å¼•æ“Žè¿›è¡Œå¤„ç†ï¼Œæœ€åŽä¼šç”Ÿäº§å¤„ä¸€ä¸ªç»“æžœæ•°æ®æµï¼Œå…¶ä¸­çš„æ•°æ®ä¹Ÿæ˜¯ä¸€ä¸ªä¸€ä¸ªçš„batchç»„æˆçš„ã€‚

#### 10. DStreamä»¥åŠåŸºæœ¬å·¥ä½œåŽŸç†ï¼Ÿ

DStreamæ˜¯spark  streamingæä¾›çš„ä¸€ç§é«˜çº§æŠ½è±¡ï¼Œä»£è¡¨äº†ä¸€ä¸ªæŒç»­ä¸æ–­çš„æ•°æ®æµã€‚

DStreamå¯ä»¥é€šè¿‡è¾“å…¥æ•°æ®æºæ¥åˆ›å»ºï¼Œæ¯”å¦‚Kafkaã€flumeç­‰ï¼Œä¹Ÿå¯ä»¥é€šè¿‡å…¶ä»–DStreamçš„é«˜é˜¶å‡½æ•°æ¥åˆ›å»ºï¼Œæ¯”å¦‚mapã€reduceã€joinå’Œwindowç­‰ã€‚

DStreamå†…éƒ¨å…¶å®žä¸æ–­äº§ç”ŸRDDï¼Œæ¯ä¸ªRDDåŒ…å«äº†ä¸€ä¸ªæ—¶é—´æ®µçš„æ•°æ®ã€‚

Spark  streamingä¸€å®šæ˜¯æœ‰ä¸€ä¸ªè¾“å…¥çš„DStreamæŽ¥æ”¶æ•°æ®ï¼ŒæŒ‰ç…§æ—¶é—´åˆ’åˆ†æˆä¸€ä¸ªä¸€ä¸ªçš„batchï¼Œå¹¶è½¬åŒ–ä¸ºä¸€ä¸ªRDDï¼ŒRDDçš„æ•°æ®æ˜¯åˆ†æ•£åœ¨å„ä¸ªå­èŠ‚ç‚¹çš„partitionä¸­ã€‚

#### 11. sparkæœ‰å“ªäº›ç»„ä»¶ï¼Ÿ

1. masterï¼šç®¡ç†é›†ç¾¤å’ŒèŠ‚ç‚¹ï¼Œä¸å‚ä¸Žè®¡ç®—ã€‚
2. workerï¼šè®¡ç®—èŠ‚ç‚¹ï¼Œè¿›ç¨‹æœ¬èº«ä¸å‚ä¸Žè®¡ç®—ï¼Œå’Œmasteræ±‡æŠ¥ã€‚
3. Driverï¼šè¿è¡Œç¨‹åºçš„mainæ–¹æ³•ï¼Œåˆ›å»ºspark contextå¯¹è±¡ã€‚
4. spark contextï¼šæŽ§åˆ¶æ•´ä¸ªapplicationçš„ç”Ÿå‘½å‘¨æœŸï¼ŒåŒ…æ‹¬dagshedulerå’Œtask schedulerç­‰ç»„ä»¶ã€‚
5. clientï¼šç”¨æˆ·æäº¤ç¨‹åºçš„å…¥å£ã€‚

#### 12. sparkå·¥ä½œæœºåˆ¶ï¼Ÿ

ç”¨æˆ·åœ¨clientç«¯æäº¤ä½œä¸šåŽï¼Œä¼šç”±Driverè¿è¡Œmainæ–¹æ³•å¹¶åˆ›å»ºspark contextä¸Šä¸‹æ–‡ã€‚æ‰§è¡Œaddç®—å­ï¼Œå½¢æˆdagå›¾è¾“å…¥dagschedulerï¼ŒæŒ‰ç…§addä¹‹é—´çš„ä¾èµ–å…³ç³»åˆ’åˆ†stageè¾“å…¥task schedulerã€‚task schedulerä¼šå°†stageåˆ’åˆ†ä¸ºtask setåˆ†å‘åˆ°å„ä¸ªèŠ‚ç‚¹çš„executorä¸­æ‰§è¡Œã€‚

#### 13. è¯´ä¸‹å®½ä¾èµ–å’Œçª„ä¾èµ–

- å®½ä¾èµ–ï¼š

  æœ¬è´¨å°±æ˜¯shuffleã€‚çˆ¶RDDçš„æ¯ä¸€ä¸ªpartitionä¸­çš„æ•°æ®ï¼Œéƒ½å¯èƒ½ä¼šä¼ è¾“ä¸€éƒ¨åˆ†åˆ°ä¸‹ä¸€ä¸ªå­RDDçš„æ¯ä¸€ä¸ªpartitionä¸­ï¼Œæ­¤æ—¶ä¼šå‡ºçŽ°çˆ¶RDDå’Œå­RDDçš„partitionä¹‹é—´å…·æœ‰äº¤äº’é”™ç»¼å¤æ‚çš„å…³ç³»ï¼Œè¿™ç§æƒ…å†µå°±å«åšä¸¤ä¸ªRDDä¹‹é—´æ˜¯å®½ä¾èµ–ã€‚

- çª„ä¾èµ–ï¼š

  çˆ¶RDDå’Œå­RDDçš„partitionä¹‹é—´çš„å¯¹åº”å…³ç³»æ˜¯ä¸€å¯¹ä¸€çš„ã€‚

#### 14. Sparkä¸»å¤‡åˆ‡æ¢æœºåˆ¶åŽŸç†çŸ¥é“å—ï¼Ÿ

Masterå®žé™…ä¸Šå¯ä»¥é…ç½®ä¸¤ä¸ªï¼ŒSparkåŽŸç”Ÿçš„standaloneæ¨¡å¼æ˜¯æ”¯æŒMasterä¸»å¤‡åˆ‡æ¢çš„ã€‚å½“Active MasterèŠ‚ç‚¹æŒ‚æŽ‰ä»¥åŽï¼Œæˆ‘ä»¬å¯ä»¥å°†Standby Masteråˆ‡æ¢ä¸ºActive Masterã€‚

Spark Masterä¸»å¤‡åˆ‡æ¢å¯ä»¥åŸºäºŽä¸¤ç§æœºåˆ¶ï¼Œä¸€ç§æ˜¯åŸºäºŽæ–‡ä»¶ç³»ç»Ÿçš„ï¼Œä¸€ç§æ˜¯åŸºäºŽZooKeeperçš„ã€‚

åŸºäºŽæ–‡ä»¶ç³»ç»Ÿçš„ä¸»å¤‡åˆ‡æ¢æœºåˆ¶ï¼Œéœ€è¦åœ¨Active MasteræŒ‚æŽ‰ä¹‹åŽæ‰‹åŠ¨åˆ‡æ¢åˆ°Standby Masterä¸Šï¼›

è€ŒåŸºäºŽZookeeperçš„ä¸»å¤‡åˆ‡æ¢æœºåˆ¶ï¼Œå¯ä»¥å®žçŽ°è‡ªåŠ¨åˆ‡æ¢Masterã€‚

#### 15. sparkè§£å†³äº†hadoopçš„å“ªäº›é—®é¢˜ï¼Ÿ

1. **MR**ï¼šæŠ½è±¡å±‚æ¬¡ä½Žï¼Œéœ€è¦ä½¿ç”¨æ‰‹å·¥ä»£ç æ¥å®Œæˆç¨‹åºç¼–å†™ï¼Œä½¿ç”¨ä¸Šéš¾ä»¥ä¸Šæ‰‹ï¼›

   **Spark**ï¼šSparké‡‡ç”¨RDDè®¡ç®—æ¨¡åž‹ï¼Œç®€å•å®¹æ˜“ä¸Šæ‰‹ã€‚

2. **MR**ï¼šåªæä¾›mapå’Œreduceä¸¤ä¸ªæ“ä½œï¼Œè¡¨è¾¾èƒ½åŠ›æ¬ ç¼ºï¼›

   **Spark**ï¼šSparké‡‡ç”¨æ›´åŠ ä¸°å¯Œçš„ç®—å­æ¨¡åž‹ï¼ŒåŒ…æ‹¬mapã€flatmapã€groupbykeyã€reducebykeyç­‰ï¼›

3. **MR**ï¼šä¸€ä¸ªjobåªèƒ½åŒ…å«mapå’Œreduceä¸¤ä¸ªé˜¶æ®µï¼Œå¤æ‚çš„ä»»åŠ¡éœ€è¦åŒ…å«å¾ˆå¤šä¸ªjobï¼Œè¿™äº›jobä¹‹é—´çš„ç®¡ç†ä»¥æ¥éœ€è¦å¼€å‘è€…è‡ªå·±è¿›è¡Œç®¡ç†ï¼›

   **Spark**ï¼šSparkä¸­ä¸€ä¸ªjobå¯ä»¥åŒ…å«å¤šä¸ªè½¬æ¢æ“ä½œï¼Œåœ¨è°ƒåº¦æ—¶å¯ä»¥ç”Ÿæˆå¤šä¸ªstageï¼Œè€Œä¸”å¦‚æžœå¤šä¸ªmapæ“ä½œçš„åˆ†åŒºä¸å˜ï¼Œæ˜¯å¯ä»¥æ”¾åœ¨åŒä¸€ä¸ªtaské‡Œé¢åŽ»æ‰§è¡Œï¼›

4. **MR**ï¼šä¸­é—´ç»“æžœå­˜æ”¾åœ¨hdfsä¸­ï¼›

   **Spark**ï¼šSparkçš„ä¸­é—´ç»“æžœä¸€èˆ¬å­˜åœ¨å†…å­˜ä¸­ï¼Œåªæœ‰å½“å†…å­˜ä¸å¤Ÿäº†ï¼Œæ‰ä¼šå­˜å…¥æœ¬åœ°ç£ç›˜ï¼Œè€Œä¸æ˜¯hdfsï¼›

5. **MR**ï¼šåªæœ‰ç­‰åˆ°æ‰€æœ‰çš„map taskæ‰§è¡Œå®Œæ¯•åŽæ‰èƒ½æ‰§è¡Œreduce taskï¼›

   **Spark**ï¼šSparkä¸­åˆ†åŒºç›¸åŒçš„è½¬æ¢æž„æˆæµæ°´çº¿åœ¨ä¸€ä¸ªtaskä¸­æ‰§è¡Œï¼Œåˆ†åŒºä¸åŒçš„éœ€è¦è¿›è¡Œshuffleæ“ä½œï¼Œè¢«åˆ’åˆ†æˆä¸åŒçš„stageéœ€è¦ç­‰å¾…å‰é¢çš„stageæ‰§è¡Œå®Œæ‰èƒ½æ‰§è¡Œã€‚

6. **MR**ï¼šåªé€‚åˆbatchæ‰¹å¤„ç†ï¼Œæ—¶å»¶é«˜ï¼Œå¯¹äºŽäº¤äº’å¼å¤„ç†å’Œå®žæ—¶å¤„ç†æ”¯æŒä¸å¤Ÿï¼›

   **Spark**ï¼šSpark streamingå¯ä»¥å°†æµæ‹†æˆæ—¶é—´é—´éš”çš„batchè¿›è¡Œå¤„ç†ï¼Œå®žæ—¶è®¡ç®—ã€‚

#### 16. æ•°æ®å€¾æ–œçš„äº§ç”Ÿå’Œè§£å†³åŠžæ³•ï¼Ÿ

æ•°æ®å€¾æ–œä»¥ä¸ºç€æŸä¸€ä¸ªæˆ–è€…æŸå‡ ä¸ªpartitionçš„æ•°æ®ç‰¹åˆ«å¤§ï¼Œå¯¼è‡´è¿™å‡ ä¸ªpartitionä¸Šçš„è®¡ç®—éœ€è¦è€—è´¹ç›¸å½“é•¿çš„æ—¶é—´ã€‚

åœ¨sparkä¸­åŒä¸€ä¸ªåº”ç”¨ç¨‹åºåˆ’åˆ†æˆå¤šä¸ªstageï¼Œè¿™äº›stageä¹‹é—´æ˜¯ä¸²è¡Œæ‰§è¡Œçš„ï¼Œè€Œä¸€ä¸ªstageé‡Œé¢çš„å¤šä¸ªtaskæ˜¯å¯ä»¥å¹¶è¡Œæ‰§è¡Œï¼Œtaskæ•°ç›®ç”±partitionæ•°ç›®å†³å®šï¼Œå¦‚æžœä¸€ä¸ªpartitionçš„æ•°ç›®ç‰¹åˆ«å¤§ï¼Œé‚£ä¹ˆå¯¼è‡´è¿™ä¸ªtaskæ‰§è¡Œæ—¶é—´å¾ˆé•¿ï¼Œå¯¼è‡´æŽ¥ä¸‹æ¥çš„stageæ— æ³•æ‰§è¡Œï¼Œä»Žè€Œå¯¼è‡´æ•´ä¸ªjobæ‰§è¡Œå˜æ…¢ã€‚

é¿å…æ•°æ®å€¾æ–œï¼Œä¸€èˆ¬æ˜¯è¦é€‰ç”¨åˆé€‚çš„keyï¼Œæˆ–è€…è‡ªå·±å®šä¹‰ç›¸å…³çš„partitionerï¼Œé€šè¿‡åŠ ç›æˆ–è€…å“ˆå¸Œå€¼æ¥æ‹†åˆ†è¿™äº›keyï¼Œä»Žè€Œå°†è¿™äº›æ•°æ®åˆ†æ•£åˆ°ä¸åŒçš„partitionåŽ»æ‰§è¡Œã€‚

å¦‚ä¸‹ç®—å­ä¼šå¯¼è‡´shuffleæ“ä½œï¼Œæ˜¯å¯¼è‡´æ•°æ®å€¾æ–œå¯èƒ½å‘ç”Ÿçš„å…³é”®ç‚¹æ‰€åœ¨ï¼šgroupByKeyï¼›reduceByKeyï¼›aggregaByKeyï¼›joinï¼›cogroupï¼›

#### 17. ä½ ç”¨sparksqlå¤„ç†çš„æ—¶å€™ï¼Œ å¤„ç†è¿‡ç¨‹ä¸­ç”¨çš„dataframeè¿˜æ˜¯ç›´æŽ¥å†™çš„sqlï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ

è¿™ä¸ªé—®é¢˜çš„å®—æ—¨æ˜¯é—®ä½ spark sql ä¸­dataframeå’Œsqlçš„åŒºåˆ«ï¼Œä»Žæ‰§è¡ŒåŽŸç†ã€æ“ä½œæ–¹ä¾¿ç¨‹åº¦å’Œè‡ªå®šä¹‰ç¨‹åº¦æ¥åˆ†æž è¿™ä¸ªé—®é¢˜ã€‚

#### 18. RDDä¸­reduceBykeyä¸ŽgroupByKeyå“ªä¸ªæ€§èƒ½å¥½ï¼Œä¸ºä»€ä¹ˆ

**reduceByKey**ï¼šreduceByKeyä¼šåœ¨ç»“æžœå‘é€è‡³reducerä¹‹å‰ä¼šå¯¹æ¯ä¸ªmapperåœ¨æœ¬åœ°è¿›è¡Œmergeï¼Œæœ‰ç‚¹ç±»ä¼¼äºŽåœ¨MapReduceä¸­çš„combinerã€‚è¿™æ ·åšçš„å¥½å¤„åœ¨äºŽï¼Œåœ¨mapç«¯è¿›è¡Œä¸€æ¬¡reduceä¹‹åŽï¼Œæ•°æ®é‡ä¼šå¤§å¹…åº¦å‡å°ï¼Œä»Žè€Œå‡å°ä¼ è¾“ï¼Œä¿è¯reduceç«¯èƒ½å¤Ÿæ›´å¿«çš„è¿›è¡Œç»“æžœè®¡ç®—ã€‚

**groupByKey**ï¼šgroupByKeyä¼šå¯¹æ¯ä¸€ä¸ªRDDä¸­çš„valueå€¼è¿›è¡Œèšåˆå½¢æˆä¸€ä¸ªåºåˆ—(Iterator)ï¼Œæ­¤æ“ä½œå‘ç”Ÿåœ¨reduceç«¯ï¼Œæ‰€ä»¥åŠ¿å¿…ä¼šå°†æ‰€æœ‰çš„æ•°æ®é€šè¿‡ç½‘ç»œè¿›è¡Œä¼ è¾“ï¼Œé€ æˆä¸å¿…è¦çš„æµªè´¹ã€‚åŒæ—¶å¦‚æžœæ•°æ®é‡ååˆ†å¤§ï¼Œå¯èƒ½è¿˜ä¼šé€ æˆOutOfMemoryErrorã€‚

æ‰€ä»¥åœ¨è¿›è¡Œå¤§é‡æ•°æ®çš„reduceæ“ä½œæ—¶å€™å»ºè®®ä½¿ç”¨reduceByKeyã€‚ä¸ä»…å¯ä»¥æé«˜é€Ÿåº¦ï¼Œè¿˜å¯ä»¥é˜²æ­¢ä½¿ç”¨groupByKeyé€ æˆçš„å†…å­˜æº¢å‡ºé—®é¢˜ã€‚

#### 19. Spark master HAä¸»ä»Žåˆ‡æ¢è¿‡ç¨‹ä¸ä¼šå½±å“åˆ°é›†ç¾¤å·²æœ‰ä½œä¸šçš„è¿è¡Œï¼Œä¸ºä»€ä¹ˆ

ä¸ä¼šçš„ã€‚

å› ä¸ºç¨‹åºåœ¨è¿è¡Œä¹‹å‰ï¼Œå·²ç»ç”³è¯·è¿‡èµ„æºäº†ï¼Œdriverå’ŒExecutorsé€šè®¯ï¼Œä¸éœ€è¦å’Œmasterè¿›è¡Œé€šè®¯çš„ã€‚

#### 20. spark masterä½¿ç”¨zookeeperè¿›è¡Œhaï¼Œæœ‰å“ªäº›æºæ•°æ®ä¿å­˜åˆ°Zookeeperé‡Œé¢

sparké€šè¿‡è¿™ä¸ªå‚æ•°spark.deploy.zookeeper.diræŒ‡å®šmasterå…ƒæ•°æ®åœ¨zookeeperä¸­ä¿å­˜çš„ä½ç½®ï¼ŒåŒ…æ‹¬Workerï¼ŒDriverå’ŒApplicationä»¥åŠExecutorsã€‚standbyèŠ‚ç‚¹è¦ä»Žzkä¸­ï¼ŒèŽ·å¾—å…ƒæ•°æ®ä¿¡æ¯ï¼Œæ¢å¤é›†ç¾¤è¿è¡ŒçŠ¶æ€ï¼Œæ‰èƒ½å¯¹å¤–ç»§ç»­æä¾›æœåŠ¡ï¼Œä½œä¸šæäº¤èµ„æºç”³è¯·ç­‰ï¼Œåœ¨æ¢å¤å‰æ˜¯ä¸èƒ½æŽ¥å—è¯·æ±‚çš„ã€‚

> æ³¨ï¼šMasteråˆ‡æ¢éœ€è¦æ³¨æ„2ç‚¹ï¼š
> 1ã€åœ¨Masteråˆ‡æ¢çš„è¿‡ç¨‹ä¸­ï¼Œæ‰€æœ‰çš„å·²ç»åœ¨è¿è¡Œçš„ç¨‹åºçš†æ­£å¸¸è¿è¡Œï¼å› ä¸ºSpark Applicationåœ¨è¿è¡Œå‰å°±å·²ç»é€šè¿‡Cluster ManagerèŽ·å¾—äº†è®¡ç®—èµ„æºï¼Œæ‰€ä»¥åœ¨è¿è¡Œæ—¶Jobæœ¬èº«çš„ è°ƒåº¦å’Œå¤„ç†å’ŒMasteræ˜¯æ²¡æœ‰ä»»ä½•å…³ç³»ã€‚
> 2ã€åœ¨Masterçš„åˆ‡æ¢è¿‡ç¨‹ä¸­å”¯ä¸€çš„å½±å“æ˜¯ä¸èƒ½æäº¤æ–°çš„Jobï¼šä¸€æ–¹é¢ä¸èƒ½å¤Ÿæäº¤æ–°çš„åº”ç”¨ç¨‹åºç»™é›†ç¾¤ï¼Œ å› ä¸ºåªæœ‰Active Masteræ‰èƒ½æŽ¥å—æ–°çš„ç¨‹åºçš„æäº¤è¯·æ±‚ï¼›å¦å¤–ä¸€æ–¹é¢ï¼Œå·²ç»è¿è¡Œçš„ç¨‹åºä¸­ä¹Ÿä¸èƒ½å¤Ÿå›  Actionæ“ä½œè§¦å‘æ–°çš„Jobçš„æäº¤è¯·æ±‚ã€‚



--end--

```
æ‰«æä¸‹æ–¹äºŒç»´ç 
æ·»åŠ å¥½å‹ï¼Œå¤‡æ³¨ã€äº¤æµã€‘å¯ç§èŠäº¤æµï¼Œä¹Ÿå¯è¿›èµ„æºä¸°å¯Œå­¦ä¹ ç¾¤
æ›´æ–‡ä¸æ˜“ï¼Œç‚¹ä¸ªâ€œåœ¨çœ‹â€æ”¯æŒä¸€ä¸‹ðŸ‘‡
```

é˜…è¯»åŽŸæ–‡

é˜…è¯» 1600

åˆ†äº«æ”¶è—

èµž10

åœ¨çœ‹9



åˆ†äº«æ­¤å†…å®¹çš„äººè¿˜å–œæ¬¢

æ•°æ®ä»“åº“å’Œæ•°æ®é›†å¸‚å»ºæ¨¡ä½“ç³»åŒ–æ€»ç»“





é˜…è¯» 508

å¤§æ•°æ®å­¦ä¹ ä¸Žåˆ†äº«

ä¸å–œæ¬¢

ä¸çœ‹çš„åŽŸå› 

- å†…å®¹è´¨é‡ä½Ž
-  

- ä¸çœ‹æ­¤å…¬ä¼—å·



å¤§è§„æ¨¡ä¸šåŠ¡æŠ€æœ¯æž¶æž„è®¾è®¡ä¸Žæˆ˜æœ¯ï¼ˆæž¶æž„å¸ˆå¿…çœ‹ï¼‰





é˜…è¯» 2032

DevOpsæŠ€æœ¯æ ˆ

ä¸å–œæ¬¢

ä¸çœ‹çš„åŽŸå› 

- å†…å®¹è´¨é‡ä½Ž
-  

- ä¸çœ‹æ­¤å…¬ä¼—å·



ä¸€å¼ å›¾ï¼Œè¯¦è§£å¤§æ•°æ®æŠ€æœ¯æž¶æž„





é˜…è¯» 548

æž¶æž„å¸ˆç¤¾åŒº

ä¸å–œæ¬¢

ä¸çœ‹çš„åŽŸå› 

- å†…å®¹è´¨é‡ä½Ž
-  

- ä¸çœ‹æ­¤å…¬ä¼—å·



ï¼š

ï¼Œ

ã€‚

è§†é¢‘

å°ç¨‹åº

èµž

ï¼Œè½»ç‚¹ä¸¤ä¸‹å–æ¶ˆèµž

åœ¨çœ‹

ï¼Œè½»ç‚¹ä¸¤ä¸‹å–æ¶ˆåœ¨çœ‹





oss-cn-beijing

[vscodepic](https://oss.console.aliyun.com/bucket/oss-c

AccessKey ID     **LTAI5t6Nh1faCp7vgJ8nZv4K**

AccessKey Secret    **immWi3ACavE8gXSTUbuTYttNXRNHmK**

